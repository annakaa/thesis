@inproceedings{seide_transcription,
  title={Conversational speech transcription using context-dependent deep neural networks},
  author={Frank Seide and Gang Li and Dong Yu},
  booktitle={INTERSPEECH},
  year={2011}
}


@inproceedings{kruspe_phonerec2,
  author = {A. M. Kruspe},
  title = {{Bootstrapping a system for phoneme recognition and keyword spotting in unaccompanied singing}},
  booktitle = {17th International Conference on Music Information Retrieval (ISMIR)},
  year = 2016,
  address={New York, NY, USA}
}

@inproceedings{shazam,
  author = {Wang, A. L.},
  booktitle = {Proceedings of the 4th International Conference on Music Information Retrieval (ISMIR)},
  pages = {7--13},
  title = {{An industrial-strength audio search algorithm}},
  year = 2003,
  address = {Baltimore, MD, USA}
}



@Article {intro_voicesearch,
author       = {Ye-Yi Wang and Dong Yu and Yun-Cheng Ju and Alex Acero},
journal      = {IEEE Signal Processing Magazine (Special Issue on Spoken Language Technology)},
month        = {May},
publisher    = {Institute of Electrical and Electronics Engineers, Inc.},
title        = {An Introduction to Voice Search},
year         = {2008},
}

@article{yang_qbh,
 author = {Wang, Chung-Che and Jang, Jyh-Shing Roger},
 title = {Improving Query-by-singing/Humming by Combining Melody and Lyric Information},
 journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
 issue_date = {April 2015},
 volume = {23},
 number = {4},
 month = apr,
 year = {2015},
 issn = {2329-9290},
 pages = {798--806}
 } 

@inproceedings{meinard_lyrics,
  author    = {Meinard M{\"{u}}ller and Frank Kurth and David Damm and Christian Fremerey and Michael Clausen},
  title     = {Lyrics-Based Audio Retrieval and Multimodal Navigation in Music Collections},
  booktitle = {Research and Advanced Technology for Digital Libraries, 11th European
               Conference, {ECDL} 2007},
  pages     = {112--123},
  year      = {2007},
  address = {Budapest, Hungary}
}


@INPROCEEDINGS{nist_trec,
    author = {John S. Garofolo and Cedric G. P. Auzanne and Ellen M. Voorhees},
    title = {{The TREC Spoken Document Retrieval Track: A Success Story}},
    booktitle = {in Text Retrieval Conference (TREC) 8},
    year = {2000},
    pages = {16--19}
}


@techreport{nist_cavg,
  author = {},
  number = {},
  institution = {NIST},
  title = {{The 2015 NIST Language Recognition Evaluation Plan (LRE15)}},
  year = 2015
}



@inproceedings{kruspe_lid2,
  author    = {Anna M. Kruspe},
  title     = {Improving Singing Language Identification through i-Vector Extraction},
  booktitle = {Proceedings of the 17th International Conference on Digital Audio
               Effects (DAFx-14)},
  pages     = {227--233},
  year      = {2014},
  address={Erlangen, Germany}
}

@article {lid_li_ma_lee,
	title = {{A Vector Space Modeling Approach to Spoken Language Identification}},
	journal = {IEEE Transactions on Audio, Speech \& Language Processing},
	volume = {15},
	year = {2007},
	pages = {271{\textendash}284},
	doi = {10.1109/TASL.2006.876860},
	author = {Haizhou Li and Bin Ma and Chin-Hui Lee}
}

@inproceedings{peche,
  author    = {Marius Peche and Marelie H. Davel and     Etienne Barnard},
  title     = {Phonotactic spoken language identification with limited training data},
  booktitle = {{INTERSPEECH} },
  pages     = {1537--1540},
  year      = {2007},
  address = {Antwerp, Belgium}
}

@inproceedings{lid_matejka,
  title={Phonotactic language identification using high quality phoneme recognition},
  author={Pavel Matejka and Petr Schwarz and Jan Cernocky and Pavel Chytil},
  booktitle={INTERSPEECH},
  year={2005}
}

@inproceedings{lid_li_ma,
 author = {Li, Haizhou and Ma, Bin},
 title = {A Phonotactic Language Model for Spoken Language Identification},
 booktitle = {Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics},
 series = {ACL '05},
 year = {2005},
 location = {Ann Arbor, Michigan},
 pages = {515--522}
}

@phdthesis{phdthesis:berkling_phd,
    author = {Kay Margarethe Berkling},
    title = {Automatic Language Identification with Sequences of Language-Independent Phoneme Clusters},
    year = {1996},
    school={Oregon Graduate Institute of Science \& Technology}
}

@book{book:jurafsky,
 author    = {D. Jurafsky and J. H. Martin},
 title     = {{Speech and language processing: An introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition}},
 publisher = {Prentice Hall},
 adress		 = {},
 year      = {2009},
 edition   = {}
 }

@phdthesis{phdthesis:thambiratnam,
	author = {A. J. K. Thambiratnam},
	title = {Acoustic keyword spotting in speech with applications to data mining},
	school = {Queensland University of Technology},
	year = {2005}
}

@phdthesis{phdthesis:jeffreysmith,
	author = {J. C. Smith},
	title = {Correlation analyses of encoded music performance},
	school = {Stanford University},
	year = {2013}
}

@inproceedings{dzhambazov_ismir,
	title={Searching Lyrical Phrases in A-Capella Turkish Makam Recordings},
	author={G. Dzhambazov and S. Sent\"{u}rk and X. Serra},
	booktitle={Proceedings of the 16th International Conference on Music Information Retrieval (ISMIR)},
	address={Malaga, Spain},
	year={2015}
}

@INPROCEEDINGS{aurora,
    author = {H.-G. Hirsch and D. Pearce},
    title = {The {Aurora} Experimental Framework for the Performance Evaluation of Speech Recognition Systems under Noisy Conditions},
    booktitle = {ISCA ITRW ASR},
    year = {2000},
    pages = {29--32}
}

@article{ntimit,
    author = {Jankowski, C. and Kalyanswamy, A. and Basson, S. and Spitz, J.},
    journal = {ICASSP},
    pages = {109--112},
    title = {{NTIMIT: A} Phonetically Balanced, Continuous Speech Telephone Bandwidth Speech Database},
    year = {1990}
}

@ARTICLE{flanagan,
    author = {J. L. Flanagan, R. M. Golden},
    title = {Phase vocoder},
    journal = {Bell System Technical Journal},
    year = {1966},
    month=NOV,
    pages = {1493--1509}
}

@ARTICLE{dolson,
    author = {M. Dolson},
    title = {The phase vocoder: A tutorial},
    journal = {Computer Music Journal},
    year = {1986},
    pages = {14--27},
    volume=10,
    number=4
}


@ARTICLE{mohamed,
    author = {A.-R. Mohamed and G. E. Dahl and G. Hinton},
    title = {Acoustic modeling using deep belief networks},
    journal = {IEEE Trans. Audio, Speech, Lang. Process},
    year = {2012},
    pages = {14--22}
}


@article{hunt,
  author = {Hunt, M. J.},
  journal = {Speech Communication},
  number = 4,
  pages = {329-336},
  title = {Figures of merit for assessing connected-word recognisers.},
  volume = 9,
  year = 1990
}


@INPROCEEDINGS{loscos,
    author = {A. Loscos and P. Cano and J. Bonada},
    title = {Low-Delay Singing Voice Alignment to Text},
    booktitle = {Proceedings of the ICMC},
    year = {1999}
}

@inproceedings{mesaros_alignment,
  author = {Mesaros, A. and Virtanen, T.},
  booktitle = {DaFX-08},
  title = {Automatic alignment of music audio and lyrics},
  year = 2008,
  address={Espoo, Finland}
}

@inproceedings{mesaros1,
  author = {Mesaros, A. and Virtanen, T.},
  booktitle = {ICASSP},
  pages = {2146-2149},
  publisher = {IEEE},
  title = {Recognition of phonemes and words in singing.},
  year = 2010
}

@article{mesaros2,
  author = {Mesaros, A. and Virtanen, T.},
  journal = {EURASIP J. Audio, Speech and Music Processing},
  title = {Automatic Recognition of Lyrics in Singing.},
  volume = 2010,
  year = 2010
}

@inproceedings{fujihara_phonemes,
  author = {Fujihara, H. and Goto, M. and Okuno, H. G.},
  booktitle = {WASPAA},
  pages = {17-20},
  publisher = {IEEE},
  title = {A novel framework for recognizing phonemes of singing voice in polyphonic music.},
  year = 2009
}


@INPROCEEDINGS{gruhne,
author = {M. Gruhne and K. Schmidt and C. Dittmar},
title = {Phoneme Recognition on Popular Music},
booktitle = {Proceedings of the 8th International Conference on Music Information Retrieval (ISMIR)},
year = {2007},
month = {September},
address = {Vienna, Austria},
}

@inbook{szepannek,
  author={G. Szepannek and M. Gruhne and B. Bischl and S. Krey and T. Harczos and F. Klefenz and C. Dittmar and C. Weihs},
  year= 2010, 
  chapter={Perceptually Based Phoneme Recognition in Popular Music}, 
  editor = {}, 
  title= {Classification as a tool for research}, 
  publisher= {Springer},
  address= {Heidelberg} 
}

@inbook{sundberg,
  author={J. Sundberg},
  year= 2012, 
  chapter={6. Perception of singing}, 
  editor = {D. Deutsch}, 
  title= {The Psychology of Music}, 
  publisher= {Academic Press},
  edition = 3
}

@inbook{goto_alignment,
  author={H. Fujihara and M. Goto},
  year= 2012, 
  chapter={Lyrics-to-audio alignment and its applications}, 
  editor = {M. M\"uller and M. Goto and M. Schedl}, 
  title= {Multimodal Music Processing}, 
  publisher= {Dagstuhl Follow-Ups}
}

@misc{ellis_pvoc,
  author = {D. P. W. Ellis},
  year = {2002},
  title = {A Phase Vocoder in {M}atlab},
  note = {Web resource, Last checked: 04/29/15},
  url = {http://www.ee.columbia.edu/~dpwe/resources/matlab/dtw/}
}

@misc{ellis_dtw,
  author = {D. P. W. Ellis},
  year = {2003},
  title = {{Dynamic Time Warp (DTW) in Matlab}},
  note = {Web resource, Last checked: 03/30/16},
  url = {http://www.ee.columbia.edu/~dpwe/resources/matlab/pvoc/}
}

@misc{autotunetoy,
  author = {C. Arft},
  year = {2010},
  title = {{AutoTune Toy}},
  note = {Web resource, Last checked: 4/29/15},
  url = {http://www.mathworks.com/matlabcentral/fileexchange/26337-autotune-toy}
}


@inproceedings{pylkkonen,
  author = {J. {Pylkkonen} and M. Kurimo},
  booktitle = {{EUROSPEECH}},
  pages = {},
  publisher = {},
  title = {Duration modeling techniques for continuous speech recognition},
  year = 2003
}

@inproceedings{juang,
  author = {Juang, B. H. and Rabiner, L. R. and Levinson, S. E. and Sondhi, M. M.},
  booktitle = {{IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP)}},
  pages = {},
  publisher = {},
  title = {Recent Developments in the Application of Hidden Markov Models to Speaker-Independent Isolated Word Recognition},
  year = 1985
}

@inproceedings{levinson,
  author = {S. E. Levinson},
  booktitle = {{IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP)}},
  pages = {},
  publisher = {},
  title = {Continuously variable duration hidden Markov models for speech analysis},
  year = 1986
}


@inproceedings{huang,
  author = {S. Huang and D. Karakos and G. A. Coppersmith and K. W. Church and S. M. Siniscalchi},
  booktitle = {{IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)}},
  pages = {342-347},
  publisher = {IEEE},
  title = {Bootstrapping a spoken language identification system using unsupervised integrated sensing and processing decision trees.},
  year = 2011
}



@inproceedings{szoeke2,
  author = {I. Szoeke and P. Schwarz and P. Matejka and L. Burget and M. Karafiat and M. Fapso and J. Cernocky},
  booktitle = {Interspeech},
  pages = {633-636},
  publisher = {ISCA},
  title = {Comparison of keyword spotting approaches for informal continuous speech.},
  year = 2005
}



@article{yu_kobayashi,
  author = {S.-Z. Yu and H. Kobayashi},
  journal = {IEEE Trans. on Signal Processing},
  month = May,
  number = 5,
  title = {{Practical implementation of an efficient forward-backward algorithm for an explicit-duration Hidden Markov Model}},
  volume = 54,
  year = 2006
}

@inproceedings{yoma,
  author = {N. B. Yoma and F. McInnes and M. Jack},
  title = {{Weighted Viterbi algorithm and state duration modeling for speech recognition in noise}},
  booktitle = {IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP)},
  year = 1998,
  address={}
}


@article{burshtein,
  author = {D. Burshtein},
  journal = {IEEE Trans. ASSP},
  month = May,
  number = 3,
  title = {{Robust parametric modeling of durations in Hidden Markov Models}},
  volume = 4,
  year = 1996
}


@inproceedings{ferguson,
  author = {J. D. Ferguson},
  title = {{Variable duration models for speech}},
  booktitle = {Proc. Symp. Applications Hidden Markov Models Text Speech},
  year = 1980,
  address={Princeton, NJ}
}


@inproceedings{kruspe_kws2,
	author = {A. M. Kruspe},
	title = {{Keyword spotting in a-capella singing with duration-modeled HMMs}},
	booktitle = {EUSIPCO},
	year = 2015,
	address={Nice, France}
}



@inproceedings{kruspe_kws1,
  author = {A. M. Kruspe},
  title = {{Keyword spotting in a-capella singing}},
  booktitle = {15th International Conference on Music Information Retrieval (ISMIR)},
  year = 2014,
  address={Taipei, Taiwan}
}



@inproceedings{kruspe_phonerec,
	author = {A. M. Kruspe},
	title = {{Training phoneme models for singing with "songified" speech data}},
	booktitle = {15th International Conference on Music Information Retrieval (ISMIR)},
	year = 2015,
	address={Malaga, Spain}
}

@inproceedings{anna_lid,
  author = {A. M. Kruspe and J. Abesser and C. Dittmar},
  title = {{A GMM approach to singing language identification}},
  booktitle = {53rd AES Conference on Semantic Audio},
  year = 2014,
  address={London, UK}
}

@article{mandal_kumar,
  author = {A. Mandal and K. R. P. Kumar and P. Mitra},
  journal = {International Journal of Speech Technology},
  month = Jun,
  number = 2,
  title = {Recent developments in spoken term detection: a survey},
  volume = 17,
  year = 2014,
  pages={183--198}
}

@inproceedings{syncglobal,
  author = {H. Grossmann and A. Kruspe and J. Abesser and H. Lukashevich},
  booktitle = {International Congress on Computer Science Information Systems and Technologies (CSIST)},
  title = {Towards Cross-Modal Search and Synchronization of Music and Video},
  year = 2011,
  address={Minsk, Belarus}
}

@inproceedings{ppm,
  added-at = {2013-01-25T00:00:00.000+0100},
  author = {K. Kintzley and A. Jansen and K. Church and H. Hermansky},
  booktitle = {INTERSPEECH},
  publisher = {ISCA},
  title = {Inverting the Point Process Model for Fast Phonetic Keyword Search.},
  year = 2012
}


@inproceedings{szoeke,
  author = {I. Szoe\9Ake and P. Schwarz and P. Matejka and L. Burget and M. Karafi\87at and J. Cernocky},
  booktitle = {TSD},
  editor = {V. Matousek and P. Mautner and T. Pavelka},
  pages = {302--309},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  title = {Phoneme Based Acoustics Keyword Spotting in Informal Continuous Speech.},
  volume = 3658,
  year = 2005
}


@techreport{jansen,
  author = {A. Jansen and P. Niyogi},
  number = {},
  institution = {Department of Computer Science, University of Chicago},
  title = {An experimental evaluation of keyword-filler hidden Markov models},
  year = 2009
}

@inproceedings{fujihara,
author = {H. Fujihara and M. Goto},
booktitle = {IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)},
address={Las Vegas, NV, USA},
pages = {69--72},
title = {{Three techniques for improving automatic synchronization between music and lyrics: Fricative detection, filler model, and novel feature vectors for vocal activity detection}},
year = {2008}
}

@article{mesaros,
  author = {A. Mesaros and T. Virtanen},
  journal = {EURASIP Journal on Audio, Speech, and Music Processing},
  month = Jan,
  number = 4,
  title = {Automatic recognition of lyrics in singing},
  volume = 2010,
  year = 2010
}

@inbook{kws_overview,
  author={A. Moyal and V. Aharonson and E. Tetariy and M. Gishri},
  year= 2013, 
  chapter={2: Keyword spotting methods}, 
  editor = {}, 
  title= {Phonetic Search Methods for Large Speech Databases}, 
  publisher= {Springer},
  address= {} 
}

@techreport{timit,
  author = {{J. S. Garofolo et al.}},
  number = {},
  institution = {Linguistic Data Consortium, Philadelphia},
  title = {{TIMIT Acoustic-Phonetic Continuous Speech Corpus}},
  year = 1993
}

@article{hermansky90,
  author = {Hermansky, H.},
  journal = {J. Acoust. Soc. Am.},
  month = Apr,
  number = 4,
  pages = {1738-52},
  title = {Perceptual Linear Predictive ({PLP}) Analysis of Speech},
  volume = 57,
  year = 1990
}

@misc{quicknet,
    author = {Johnson, D. },
    citeulike-article-id = {11150212},
    howpublished = {http://www.icsi.berkeley.edu/Speech/qn.html},
    posted-at = {2012-08-28 22:36:54},
    priority = {2},
    title = {{ICSI} Quicknet Software Package},
    year = {2004}
}

@INPROCEEDINGS{reynolds00,
    author = {D. A. Reynolds and T. F. Quatieri and R. B. Dunn},
    title = {Speaker verification using Adapted Gaussian mixture models},
    booktitle = {Digital Signal Processing},
    year = {2000},
    pages = {2000}
}

@inproceedings{Charbuillet2011_,
author = {Charbuillet, C. and Tardieu, D. and Peeters, G.},
journal = {Conference on Digital Audio Effects (DAFx)},
number = {1},
pages = {1--4},
title = {{GMM Supervector for content based music similarity}},
year = {2011}
}

@techreport{nist2003lre,
  author = {A. Martin and M. Pryzbocki},
  number = {},
  institution = {Linguistic Data Consortium, Philadelphia},
  title = {{2003 NIST Language Recognition Evaluation}},
  year = 2006
}

@techreport{ogimultilang,
  author = {R. Cole and Y. Muthusamy},
  number = {},
  institution = {Linguistic Data Consortium, Philadelphia},
  title = {{OGI Multilanguage Corpus}},
  year = 1994
}

@inproceedings{Martinez2011,
author = {Martinez, D. and Plchot, O. and Burget, L.},
booktitle={Interspeech},
address={Florence, Italy},
month = {August},
pages = {861--864},
title = {{Language Recognition in iVectors Space}},
year = {2011}
}


@article{Dehak2011_,
author = {Dehak, N. and Kenny, P. J. and Dehak, R. and Dumouchel, P. and Ouellet, P.},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
month = may,
number = {4},
pages = {788--798},
title = {{Front-End Factor Analysis for Speaker Verification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5545402},
volume = {19},
year = {2011}
}

@inproceedings{kruspe_aes,
  author = {A. Kruspe and J. Abesser and C. Dittmar},
  booktitle = {AES 53},
  pages = {},
  title = {{A GMM approach to singing language identification}},
  year = 2014,
  address = {London, UK}
}


@inproceedings{brno,
  author = {{Martinez Gonzalez}, D. and Plchot, O. and Burget, L. and Glembek, O. and Matejka, P.},
  booktitle = {INTERSPEECH},
  pages = {861--864},
  title = {Language Recognition in iVectors Space.},
  year = 2011,
  address = {Florence, Italy}
}



@INPROCEEDINGS{matejka,
    author = {P. Matejka and I. Szoeke and P. Schwarz and J. Cernocky},
    title = {Automatic language identification using phoneme and automatically derived unit strings},
    booktitle = {Proceedings of 7th International Conference on Text, Speech, and Dialogue (TSD)},
    year = {2004},
    pages = {147--154},
    address = {Brno, Czech Republic}
}

@INPROCEEDINGS{yan_barnard,
    author = {Y. Yan and E. Barnard},
    title = {Experiments For An Approach To Language Identification With Conversational Telephone Speech},
    booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
    year = {1996},
    pages = {789--792},
    address = {Atlanta, GA, USA}
}

@INPROCEEDINGS{prosody,
    author = {C.-Y. Lin and H.-C. Wang},
    title = {Language identification using pitch contour information},
    booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
    year = {2005},
    pages = {601--604},
    publisher = {},
    address = {Philadelphia, PA, USA}
}

@inproceedings{dustor,
  author = {A. Dustor and P. Szwarc},
  title = {Spoken language identification based on GMM models},
  booktitle = {International conference on signals and electronic systems(ICSES)},
  year = {2010},
  address = {Gliwice, Poland},
  pages = {105--108}
  }


@inproceedings{torres_,
  author = {P. A. Torres-Carrasquillo and E. Singer and M. A. Kohler and R. J. Greene and D. A. Reynolds and {J. R. Deller, Jr.}},
  title= {Approaches to language identification using Gaussian mixture models and shifted delta cepstral features},
  booktitle = {International Conference on Spoken Language Processing (ICSLP)},
  year = {2002},
  pages = {89--92},
  address = {Denver, CO, USA}
  }

@inproceedings{singer_,
  author = {E. Singer and P. A. Torres-Carrasquillo and T. P. Gleason and W. M. Campbell and D. A. Reynolds},
  title = {Acoustic, phonetic, and discriminative approaches to automatic language identification},
  booktitle = {Proceedings of Eurospeech},
  year = {2003},
  pages = {1345--1348},
  address = {Geneva, Switzerland}
  }
  
@ARTICLE{campbell_,
    author = {W. M. Campbell and J. P. Campbell and D. A. Reynolds and E. Singer and P. A. Torres-Carrasquillo},
    title = {Support vector machines for speaker and language recognition},
    journal = {Computer Speech and Language},
    year = {2006},
    volume = {20},
    pages = {210--229}
}
  
@inproceedings{bielefeld_,
  author = {B. Bielefeld},
  title = {Language identification using shifted delta cepstrum},
  booktitle = {Fourteenth annual speech research symposium},
  year = {1994},
  address = {Baltimore, MD, USA}
  }
  
@inproceedings{allen_,
  author = {F. Allen and E. Ambikairajah and J. Epps},
  title= {Language identification using Warping and the Shifted Delta Cepstrum},
  booktitle = {2005 IEEE 7th Workshop on Multimedia Signal Processing},
  year = {2006},
  pages = {1--4},
  address = {Shanghai, China}
  }
  
@inproceedings{chandrasekhar_,
  author = {V. Chandraskehar and M. E. Sargin and D. A. Ross},
  title = {Automatic language identification in music videos with low level audio and visual features},
  booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  year = {2011},
  pages = {5724--5727},
  address = {Prague, Czech Republic}
  }

@INPROCEEDINGS{kruspe11,
    author = {A. Kruspe and H. Lukashevich and J. Abesser and H. Grossmann and C. Dittmar},
    title = {Automatic classification of musical pieces into global cultural areas},
    booktitle = {Proceedings of Audio Engineering Society 42nd Conference},
    year = {2011},
    address = {Ilmenau, Germany},
    pages = {44--53}
}

@INPROCEEDINGS{source_separation,
    author = {M. Vinyes and J. Bonada and A. Loscos},
    title = {Demixing Commercial Music Productions via Human-Assisted Time-Frequency Masking},
    booktitle = {Proceedings of Audio Engineering Society 120th Convention},
    year = {2006}
}
@INPROCEEDINGS{glass_ceiling,
    author = {{J.-J.} Aucouturier and F. Pachet},
    title = {Improving timbre similarity: How high is the sky?},
    booktitle = {Journal of Negative Results in Speech and Audio Sciences},
    year = {2004},
    volume = {1}
}

@INPROCEEDINGS{pedro,
    author = {C. Dittmar and P. Mercado and H. Grossmann and E. Cano},
    title = {Towards lyrics spotting in the {SyncGlobal} project},
    booktitle = {3rd International Workshop on Cognitive Information Processing (CIP)},
    year = {2012},
    pages = {}
}

@INPROCEEDINGS{jens_,
    author = {J. K. Hansen},
    title = {Recognition of Phonemes in A-cappella Recordings using Temporal Patterns and Mel Frequency Cepstral Coefficients},
    booktitle = {9th Sound and Music Computing Conference (SMC)},
    year = {2012},
    pages = {494--499},
    address = {Copenhagen, Denmark}
}

@techreport{rasta_plp_,
  author = {Hermansky, H. and Morgan, N. and Bayya, A. and Kohn, P.},
  number = {TR-91-069},
  institution = {ICSI},
  title = {{RASTA-PLP} Speech Analysis},
  year = 1991
}

@INPROCEEDINGS{traps1,
    author = {H. Hermansky and S. Sharma},
    title = {Traps -- Classifiers Of Temporal Patterns},
    booktitle = {Proceedings of the 5th International Conference on Spoken Language Processing (ICSLP)},
    year = {1998},
    pages = {1003--1006},
    publisher = {},
    address = {Sydney, Australia}
}

@INPROCEEDINGS{traps2,
    author = {H. Hermansky and S. Sharma},
    title = {{Temporal patterns (TRAPS) in ASR of noisy speech}},
    booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
    year = {1999},
    pages = {289--292},
    publisher = {},
    address = {Phoenix, AZ, USA}
}


@inproceedings{jhu_lid1,
	author = {Huang, S. and Karakos, D. and Church, K. and Coppersmith, G.}, 
	title = {Bootstrapping a Spoken Language Identification System Using Unsupervised Integrated Sensing and Processing Decision Trees},
	booktitle = {IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)},
	year = {2011}
 }
 
 @inproceedings{jhu_lid2,
	author = {D. Farris and C. White and Khudanpur, S.},
	title = {Sample Selection for Automatic Language Identification},
	booktitle = {Proceedings of the IEEE International Conference on Acoustics Speech and Signal Processing},
	pages = {4225-4228},
	year = {2008}
	}
 
 @inproceedings{jhu_ks1,
	author = {Jansen, Aren and P. Niyogi},
	title = {Robust Keyword Spotting with Rapidly Adapting Point Process Models}, 
	booktitle = {Proc. of Interspeech},
	year = {2009}
 }
 
 @inproceedings{jhu_ks2,
	author = {J. Pinto and A. Lovitt and Hermansky, Hynek},
	title = {Exploiting Phoneme Similarities in Hybrid {HMM-ANN} Keyword Spotting},
	booktitle = {Proceedings of the International Conference on Spoken Language Processing},
	year = {2007}
 }

@inproceedings{jhu_ks3,
	author = {P. Fousek and Hermansky, Hynek},
	title = {Towards {ASR} Based on Hierarchical Posterior-Based Keyword Recognition},
	booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing},
	year = {2006}
 }
 
 @article{zissman,
  author = {M. A. Zissman},
  title = {Comparison of four approaches to automatic language identification of telephone speech},
  journal = {IEEE Transactions on Speech and Audio Processing},
  month = JAN,
  year = {1996},
  volume = {4},
  number = {1},
  pages = {31--44},
  editor = {},
  series = {},
}

 @article{zissman_berkling,
  author = {M. A. Zissman and K. Berkling},
  title = {Automatic Language Identification},
  journal = {Speech Communication},
  year = {2001},
  volume = {35},
  number = {1-2},
  pages = {},
  editor = {},
  series = {},
}
 
  @article{muthusamy_,
  author = {Y. K. Muthusamy and E. Barnard and R. A. Cole},
  title = {Reviewing automatic language identification},
  journal = {IEEE Signal Procesing Magazine},
  month = OCT,
  year = {1994},
  volume = {11},
  number = {4},
  pages = {33 -- 41},
  editor = {},
  series = {},
}

 @inproceedings{schwenninger_,
  title = {Language Identification in Vocal Music},
  author = {J. Schwenninger and R. Brueckner and D. Willett and M. E. Hennecke},
  year = {2006},
  pages = {377--379},
  booktitle = {7th International Conference on Music Information Retrieval (ISMIR)},
  address = {Victoria, Canada}
}
 
 @inproceedings{mehrabani_,
  title = {Language identification for singing},
  author = {M. Mehrabani and J. H. L. Hansen},
  year = {2011},
  pages = {4408--4411},
  booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  address = {Prague, Czech Republic}
}

@inproceedings{tsai_wang_,
  author    = {W.-H. Tsai and H.-M. Wang},
  title     = {Towards Automatic Identification Of Singing Language In Popular Music Recordings},
  booktitle = {5th International Conference on Music Information Retrieval (ISMIR)},
  year      = {2004},
  pages = {568--576},
  address = {Barcelona, Spain}
}

@inproceedings{bridle,
  author    = {J. S. Bridle},
  title     = {An efficient elastic-template method for detecting given words in running speech},
  booktitle = {Brit. Acoust. Soc. Meeting},
  year      = {1973},
  pages = {1 -- 4}
}

@inproceedings{rose_paul,
  author    = {R. C. Rose and D. B. Paul},
  title     = {A hidden markov model based keyword recognition system},
  booktitle = {Proc. of the IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP)},
  year      = {1990},
  pages = {129 -- 132}
}
 
 @book{book:weltmusik,
 author    = {Simon Broughton and Kim Burton and Mark Ellingham and David Muddyman and Richard Trillo (Hrsg.)},
 title     = {Weltmusik},
 series    = {Rough Guide},
 publisher = {J. B. Metzler},
 adress		 = {Stuttgart, Weimar},
 year      = {2000},
 edition   = {}
 }

@book{diplomarbeit,
 author    = {Anna Marie Kruspe},
 title     = {Automatic classification of musical pieces into global cultural areas},
 series    = {Diploma thesis},
 publisher = {TU Ilmenau},
 year      = {2011},
 edition   = {}
 }
 
@article{article:comparative_analysis,
  author = {Emilia G\F3mez and Perfecto Herrera},
  title = {Comparative Analysis of Music Recordings from Western and Non-Western traditions by Automatic Tonal Feature Extraction},
  journal = {Empirical Musicology Review},
  year = {2008},
  volume = {3},
  number = {3},
  pages = {140-156},
  editor = {},
  series = {},
}

@article{article:computational_ethnomusicology,
  author = {George Tzanetakis and Ajay Kapur and W. Andrew Schloss and Matthew Wright},
  title = {Computational Ethnomusicology},
  journal = {Journal of Interdisciplinary Music Studies},
  year = {2007},
  volume = {1},
  number = {2},
  pages = {1-24}
}



@inproceedings{inproceedings:multi_domain_labeling,
author = {Hanna Lukashevich and Jakob Abe\DFer and Christian Dittmar and Holger Grossmann},
title = {From Multi-Labeling to Multi-Domain-Labeling: A novel two-dimensional approach to music genre classification},
year	= {2009},
pages = {},
booktitle = {10th International Society for Music Information Retrieval Conference (ISMIR)},
adress = {}
}

@inproceedings{inproceedings:cultural_style,
author = {Yuxiang Liu and Qiaoliang Xiang and Ye Wang and Lianhong Cai},
title = {Cultural style based music classification of audio signals},
year	= {2009},
pages = {},
booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing},
adress = {Taipei, Taiwan}
}


@inproceedings{inproceedings:low_level_song_level,
author = {Mart\EDn Haro and Perfecto Herrera},
title = {From Low-Level to Song-level percussion descriptors of polyphonic music},
year	= {2009},
pages = {},
booktitle = {10th International Society for Music Information Retrieval Conference (ISMIR)},
adress = {}
}
 
@inproceedings{inproceedings:music_and_geography,
author = {Emilia G\F3mez and Mart\EDn Haro and Perfecto Herrera},
title = {Music and Geography: Content description of musical audio from different parts of the world},
year	= {2009},
pages = {},
booktitle = {10th International Society for Music Information Retrieval Conference (ISMIR)},
adress = {}
}
 
%unused
@inproceedings{inproceedings:raag,
author = {Parag Chordia and Alex Rae},
title = {Real-Time Raag Recognition for Interactive Music},
year	= {2008},
pages = {},
booktitle = {International Conference on New Interfaces for Musical Expression (NIME)},
adress = {Genova, Italy}
}

%unused
@inproceedings{inproceedings:folk_music_hmm,
author = {Wei Chai and Barry Vercoe},
title = {Folk Music Classification Using Hidden Markov Models},
year	= {2001},
pages = {},
booktitle = {IC-AI},
adress = {Las Vegas, NV, USA}
}

@techreport{techreport:cuidado,
	author = {Geoffroy Peeters},
	institution = {CUIDADO I.S.T. project},
	title = {A large set of audio features for sound description (similarity and classification) in the CUIDADO project},
	year = {2004},
	publisher = {IRCAM}
}

%unused
@inproceedings{inproceedings:folk_neural_networks,
author = {Petri Toiviainen and Tuomas Eerola},
title = {A method for comparative analysis of folk music based on musical feature extraction and neural networks},
year	= {2001},
pages = {41-45},
booktitle = {Proceedings of  the 7th International Symposium on Systematic and Comparative Musicology and 3rd International Conference on Cognitive Musicology},
adress = {Jyv\E4skyl\E4, Finland}
}

@article{article:genre_classification,
  author = {George Tzanetakis and Perry Cook},
  title = {Musical Genre Classification of Audio Signals},
  journal = {IEEE Transactions on Speech and Audio Processing},
  year = {2002},
  volume = {10},
  number = {5},
  pages = {293-302}
}


@article{article:genre_survey,
  author = {Nicolas Scaringella and Giorgio Zoia and Daniel Mlynek},
  title = {Automatic genre classification of music content: a survey},
  journal = {IEEE Signal Processing Magazine},
  year = {2006},
  volume = {23},
  number = {2},
  pages = {133-141}
}

@incollection{incollection:mir,
	author = {J. Stephen Downie},
	title = {Music information retrieval},
	chapter = {7},
	booktitle = { Annual Review of Information Science and Technology 37},
	editor = {Blaise Cronin},
	year = {2003},
	pages = {295-340}
}

@inproceedings{inproceedings:ras,
author = {Eric Humphrey},
title = {Automatic Characterization of Digital Music for Rhythmic Auditory Stimulation},
year	= {2010},
pages = {},
booktitle = {11th International Society for Music Information Retrieval Conference (ISMIR)},
adress = {}
}

@inproceedings{inproceedings:western_taxonomy,
author = {Fran\E7ois Pachet and Daniel Cazaly},
title = {A Taxonomy of Musical Genres},
year	= {2000},
pages = {},
booktitle = {Content-Based Multimedia Information Access Conference (RIAO)},
adress = {Paris}
}

@book{book:garland,
author    = {Bruno Nettl and Ruth M. Stone and James Porter and Timothy Rice (Hrsg.)},
title     = {The Garland Encyclopedia of World Music},
publisher = {Garland},
adress		 = {},
year      = {1998},
edition   = {},
note   = "\url{http://glnd.alexanderstreet.com/}"
}

@misc{website:national_geographic,
 author	  = {Online-Quelle: National Geographic Society},
 title		= {Home: National Geographic World Music},
 note		  = "\url{http://worldmusic.nationalgeographic.com/}",
 year		  = {Letzte Pr\FCfung:}
 }
 
@misc{website:allmusic,
 author	  = {Online-Quelle: Rovi Corporation},
 title		= {allmusic},
 note		  = "\url{http://www.allmusic.com/}",
 year		  = {Letzte Pr\FCfung:}
 } 
 
 @book{book:clash_of_civilizations,
author    = {Samuel P. Huntington},
title     = {The Clash of Civilizations and the Remaking of World Order},
publisher = {Simon \& Schuster},
adress		 = {},
year      = {1996},
edition   = {}
}

@phdthesis{phdthesis:roughness,
  author = {Panteleimon Nestor Vassilakis},
  title = {Perceptual and Physical Properties of Amplitude Fluctuation and their Musical Significance},
  school = {University of California},
  year = {2001},
  address = {Los Angeles, CA, USA}
}

@article{article:roughness,
  author = {Pantelis N. Vassilakis},
  title = {Auditory Roughness as a Means of Musical Expression},
  journal = {Perspectives in Systematic Musicology},
  series = {Selected Reports in Ethnomusicology},
  year = {2005},
  volume = {12},
  pages = {119-144}
}


@inproceedings{inproceedings:h2a,
author = {Tim Pohle and Peter Knees and Klaus Seyerlehner and Gerhard Widmer},
title = {A High-Level Audio Feature for Music Retrieval and Sorting},
month = SEP,
year	= {2010},
pages = {},
booktitle = {13th Int. Conference on Digital Audio Effects (DAFx-10)},
adress = {Graz, Austria}
}

@article{article:chorus_section,
  author = {Masataka Goto},
  title = {A Chorus Section Detection Method for Musical Audio Signals and its Application to a Music Listening Station},
  journal = {IEEE Transactions on Audio, Speech and Language Processing},
  year = {2006},
  volume = {14},
  number = {5},
  month = SEP,
  pages = {1783-1794}
}

@book{book:helmholtz,
author    = {Hermann Helmholtz},
title     = {On the Sensations of Tone},
publisher = {Dover},
adress		 = {},
year      = {1954},
edition   = {}
}

@inproceedings{inproceedings:turkish_arel,
author = {Ali C. Gedik and Baris Bozkurt},
title = {Automatic Classification of Turkish Traditional Art Music Recordings by Arel Theory},
month = JUL,
year	= {2008},
pages = {},
booktitle = { Proceedings of the Conference on Interdisciplinary Musicology},
adress = {}
}

@phdthesis{phdthesis:tonal_description,
  author = {Emilia G\F3mez},
  title = {Tonal Description of Music Audio Signals},
  school = {Pompeu Fabra University},
  year = {2006},
  address = {Barcelona, Spain}
}

@book{book:atlas_musik1,
author    = {Ulrich Michels},
title     = {dtv-Atlas zur Musik},
publisher = {Deutscher Taschenbuch-Verlag, B\E4renreiter-Verlag},
adress		 = {},
year      = {1980},
month		 = SEP,
edition   = {5},
volume = {1}
}

@article{article:vibrato_measurements,
author = {E. Prame},
title = {Measurements of the vibrato rate of ten singers},
journal = {STL-QPSR},
year = {1992},
volume = {33},
number = {4},
pages = {73-86}
}

@inproceedings{inproceedings:vibrato_questions,
author = {Renee Timmers and Peter Desain},
title = {Vibrato: Questions and Answers from Musicians and Science},
month = {},
year	= {2000},
pages = {},
booktitle = { Proceedings of 6th ICMP},
adress = {}
}

@inproceedings{inproceedings:singing_voice_detection,
author = {Lise Regnier and Geoffroy Peeters},
title = {Singing voice detection in music tracks using direct voice vibrato detection},
month = {},
year	= {2009},
pages = {1685-1688},
booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
adress = {}
}

 @book{book:wohltemperierte_gehirn,
 author    = {Robert Jourdain},
 title     = {Das wohltemperierte Gehirn. Wie Musik im Kopf entsteht und wirkt},
 series    = {},
 publisher = {Spektrum Akademischer Verlag},
 adress		 = {Heidelberg, Berlin},
 year      = {2001},
 edition   = {}
 }

@phdthesis{phdthesis:automatic_transcription,
  author = {W. Andrew Schloss},
  title = {On the automatic transcription of percussive music - from acoustic signal to high-level analysis},
  school = {Stanford University, Department of Music},
  year = {1985},
  address = {Stanford, CA, USA}
}


@article{article:pulse_salience,
author = {Richard Parncutt},
title = {A Perceptual Model of Pulse Salience and Metrical Accent in Musical Rhythms},
journal = {Music Perception},
year = {1994},
volume = {11},
number = {4},
pages = {409-464}
}

@article{article:birth_of_the_blues,
author = {J. M. Gibson},
title = {The birth of the blues: How physics underlies music},
journal = {Reports on Progress in Physics},
year = {2009},
volume = {72},
number = {7},
pages = {}
}

@article{article:roughness_critical_bands,
author = {R. Plomp and W. J. M. Levelt},
title = {Tonal Consonance and Critical Bandwidth},
journal = {Journal of the Acoustical Society of America},
year = {1965},
volume = {38},
number = {},
pages = {548-560}
}

@article{article:patterns_in_music,
author = {Joseph A. Barone},
title = {Patterns in Music},
journal = {Expedition},
year = {1960},
volume = {3},
number = {1},
pages = {29-31}
}

@article{article:scales_biological_rationale,
author = {Kamraan Z. Gill and Dale Purves},
title = {A Biological Rationale for Musical Scales},
journal = {PLoS ONE},
month	= DEC,
year = {2009},
volume = {4},
number = {12},
pages = {}
}

@article{article:children_implicit_harmony,
author = {E. Glenn Schellenberg and Emmanuel Bigand and Benedicte Poulin-Charronnat and C\E9cilia Garnier and Catherine Stevens},
title = {Children's implicit knowledge of harmony in Western music},
journal = {Developmental Science},
year = {2005},
volume = {8},
number = {6},
pages = {551-566}
}

@inproceedings{inproceedings:analysis_meter,
author = {A. P. Klapuri and A. J. Eronen and J. T. Astola},
title = {Analysis of the meter of acoustic musical signals},
month = {},
year	= {2004},
pages = {342-355},
booktitle = {IEEE Trans. Speech and Audio Processing},
adress = {}
}

@article{article:mood_detection,
author = {Lie Lu and Dan Liu and Hong-Jiang Zhang},
title = {Automatic Mood Detection and Tracking of Music Audio Signals},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
year = {2006},
month = JAN,
volume = {14},
number = {1},
pages = {5-18}
}

@article{article:scheirer,
author = {Eric D. Scheirer},
title = {Tempo and beat analysis of acoustic musical signals},
journal = {J. Acoust. Soc. Am.},
year = {1998},
month = JAN,
volume = {103},
number = {1},
pages = {588-601}
}

@inproceedings{inproceedings:cyclic_beat_spectrum,
author = {Frank Kurth and Thorsten Gehrmann and Meinard M\FCller},
title = {The Cyclic Beat Spectrum: Tempo-Related Audio Features for Time-Scale Invariant Audio Identification},
month = {},
year	= {2006},
pages = {35-40},
booktitle = {Proc. of the 7th Int. Conf. on MIR},
adress = {}
}

@article{article:quatieri,
author = {Robert J. McAulay and Thomas F. Quatieri},
title = {Speech Analysis/Synthesis Based on a Sinusoidal Representation},
journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
year = {1986},
month = AUG,
volume = {34},
number = {4},
pages = {744-754}
}

@inproceedings{inproceedings:dressler,
author = {Karin Dressler},
title = {Sinusoidal Extraction using an efficient implementation of a multi-resolution {FFT}},
month = SEP,
year	= {2006},
pages = {247-252},
booktitle = {Proc. of the 9th Int. Conference on Digital Audio Effects (DAFx-06)},
adress = {}
}

 @book{book:sethares,
 author    = {William A. Sethares},
 title     = {Tuning, Timbre, Spectrum, Scale},
 publisher = {Springer Verlag},
 adress		 = {London},
 year      = {1998},
 edition   = {}
 }

@article{article:pressnitzer,
author = {Daniel Pressnitzer and Stephen McAdams},
title = {Two phase effects in roughness perception},
journal = {J. Acoust. Soc. Am.},
year = {1999},
month = MAY,
volume = {105},
number = {5},
pages = {2773-2782}
}

@inproceedings{inproceedings:separation_harmonic_percussive,
author = {Nobutaka Ono and Kenichi Miyamoto and Jonathan Le Roux and Hirokazu Kameoka and Shigeki Sagayama},
title = {Separation of a monaural audio signal into harmonic/percussive components by complementary diffusion on spectrogram},
month = AUG,
year	= {2008},
pages = {},
booktitle = {Proceedings of the 16th European Signal Processing Conference (EUSIPCO)},
adress = {}
}

@techreport{techreport:practical_svm,
  author = {Chih-Wei Hsu and Chih-Chung Chang and Chih-Jen Lin},
  institution = {Department of Computer Science and Information Engineering, National Taiwan University, Taipei 106, Taiwan},
  title = {A Practical Guide to Support Vector Classification},
  year = 2010,
}

@article{article:intro_svm,
  author = {Dustin Boswell},
  title = {Introduction to Support Vector Machines},
  year = 2002,
}

@misc{misc:libsvm,
    author = {Chih-Chung Chang and Chih-Jen Lin},
    title = {{LIBSVM} - A Library for Support Vector Machines},
    year = {2001},
    URL = {http://www.csie.ntu.edu.tw/\~cjlin/libsvm/}
 }

@article{article:svm_tutorial,
  author = {Christopher J. C. Burges},
  title = {A tutorial on support vector machines for pattern recognition},
  journal = {Data mining and knowledge discovery},
  number = 2,
  pages = {121-167},
  volume = 2,
  year = 1998,
}

@inproceedings{irmfsp,
author = {G. Peeters and X. Rodet},
title = {Hierarchical Gaussian Tree with Inertia Ratio Maximization for the classification of large musical instrument databases},
month = SEP,
year	= {2003},
pages = {1-6},
booktitle = {Proc. of the 6th Int. Conference on Digital Audio Effects (DAFX-03)},
adress = {London, UK}
}

@inproceedings{inproceedings:fs_vs_fst,
author = {Hanna Lukashevich},
title = {Feature Selection vs. Feature Space Transformation in Automatic Music Genre Classification Tasks},
month = MAY,
year	= {2009},
pages = {},
booktitle = {Proceedings of the 126th AES Convention},
adress = {Munich, Germany}
}

@article{article:representing_musical_genre,
  author = {Jean-Julian Aucouturier and Fran\E7ois Pachet},
  title = {Representing Musical Genre: A State of the Art},
  number = 1,
  pages = {83-93},
  journal = {Journal of New Music Research},
  volume = 32,
  year = 2003,
}

@inproceedings{inproceedings:epcp,
author = {Kyogu Lee},
title = {Automatic Chord Recognition from Audio Using Enhanced Pitch Class Profile},
month = {},
year	= {2006},
pages = {},
booktitle = {Proc. Intl. Computer Music Conf. (ICMC)},
adress = {New Orleans, USA}
}

@inproceedings{inproceedings:genre_multilinear,
author = {Ioannis Panagakis and Emmanouil Benetos and Constantine Kotropoulos},
title = {Music Genre Classification: A Multilinear Approach},
month = {},
year	= {2008},
pages = {583-588},
booktitle = {9th International Society for Music Information Retrieval Conference (ISMIR)},
adress = {}
}

@inproceedings{inproceedings:modulation_spectral_contrast_feature,
author = {Chang-Hsing Lee and Jau-Ling Shih and Kun-Ming Yu and Jung-Mau Su},
title = {Automatic Music Genre Classification using Modulation Spectral Contrast Feature},
month = {},
year	= {2007},
pages = {204-207},
booktitle = {Proc. of the IEEE Intl. Conf. on Multimedia and Expo (ICME)},
adress = {}
}

@inproceedings{inproceedings:genre_high_level,
author = {Cory McKay and Ichiro Fujinaga},
title = {Automatic Genre Classification using large High-Level musical feature sets},
month = {},
year	= {2004},
pages = {},
booktitle = {Proc. of the Intl. Conf. on Music Information Retrieval (ISMIR)},
adress = {}
}

@inproceedings{inproceedings:genre_bass,
author = {Jakob Abe\DFer and Hanna Lukashevich and Christian Dittmar and Gerald Schuller},
title = {Genre classification using bass-related high-level features and playing styles},
month = {},
year	= {2009},
pages = {},
booktitle = {Proc. of the Intl. Conf. on Music Information Retrieval (ISMIR)},
adress = {Kobe, Japan}
}

@inproceedings{inproceedings:musical_memory,
author = {Polina Proutskova},
title = {Musical Memory of the World - Data infrastructure in ethnomusicological archives},
month = {},
year	= {2007},
pages = {},
booktitle = {Proc. of the Intl. Conf. on Music Information Retrieval (ISMIR)},
adress = {}
}
@inproceedings{inproceedings:latin_genre,
author = {Thomas V\F6lkel and Jakob Abe\DFer and Christan Dittmar and Holger Gro\DFmann},
title = {Automatic Genre Classification of Latin American Music using Characteristic Rhythmic Patterns},
month = {},
year	= {2010},
pages = {},
booktitle = {Proceedings of the Audio Mostly Conference},
adress = {Pitea, Sweden}
}

@inproceedings{inproceedings:global_access,
author = {Olmo Cornelis and Dirk Moelants and Marc Leman},
title = {Global Access to Ethnic Music: The next big Challenge?},
month = {},
year	= {2009},
pages = {},
booktitle = {Proc. of the Intl. Conf. on Music Information Retrieval (ISMIR)},
adress = {Kobe, Japan}
}

@inproceedings{inproceedings:latin_ensemble_classifiers,
author = {Carlos N. {Silla Jr.} and Celso A. A. Kaestner and Alessandro L. Koerich},
title = {Automatic music genre classification using ensemble of classifiers},
month = OCT,
year	= {2007},
pages = {1687-1692},
booktitle = {Proceedings of the IEEE International Conference on Systems, Man and Cybernetics},
adress = {Montr\E9al, Canada}
}

@inproceedings{inproceedings:rhythmic_turkish,
author = {Andre Holzapfel and Yannis Stylianou},
title = {Rhythmic Similarity in traditional Turkish music},
month = {},
year	= {2009},
pages = {},
booktitle = {Proc. of the Intl. Conf. on Music Information Retrieval (ISMIR)},
adress = {Kobe, Japan}
}

@inproceedings{inproceedings:african_tone_scales,
author = {Dirk Moelants and Olmo Cornelis and Marc Leman},
title = {Exploring African Tone Scales},
month = {},
year	= {2009},
pages = {},
booktitle = {Proc. of the Intl. Conf. on Music Information Retrieval (ISMIR)},
adress = {Kobe, Japan}
}

@article{article:mir_for_nonwestern_music,
  author = {Thomas Lidy and Carlos N. {Silla Jr.} and Olmo Cornelis and Fabien Gouyon and Andreas Rauber and Celso A. A. Kaestner and Alessandro L. Koerich},
  title = {On the suitability of state-of-the-art music information retrieval methods for analyzing, categorizing and accessing non-Western and ethnic music collections},
  journal = {Signal Processing},
  number = 90,
  pages = {1032-1048},
  volume = {},
  year = 2010,
}

 @book{book:information_retrieval,
 author    = {Christopher D. Manning and Prabhakar Raghavan and Hinrich Sch\FCtze},
 title     = {An Introduction to Information Retrieval},
 publisher = {Cambridge University Press},
 adress		 = {Cambridge, England},
 year      = {2009},
 edition   = {},
 chapter	= {8: Evaluation in information retrieval}
 }

@techreport{techreport:kefir,
	author = {Kay Wolter},
	institution = {Fraunhofer IDMT},
	title = {{kefir Guide}},
	year = {2010},
	publisher = {}
}

@article{article:sachs_hornbostel,
  author = {Erich M. von Hornbostel and Curt Sachs},
  title = {{Systematik der Musikinstrumente - Ein Versuch}},
  journal = {Zeitschrift f\FCr Ethnologie},
  number = {4-5},
  pages = {553-590},
  volume = 46,
  year = 1914,
}



@inproceedings{onset_abdallah,
author = {Abdallah, S A and Plumbley, M},
booktitle = {4th International Symposium on Independent Component Analysis and blind Signal separation (ICA)},
title = {{Probability as metadata: Event detection in music using ICA as a conditional density model}}
}
@article{Abdel-hamid2012,
author = {Abdel-hamid, Ossama and Jiang, Hui and Penn, Gerald},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Abdel-hamid, Jiang, Penn - 2012 - APPLYING CONVOLUTIONAL NEURAL NETWORKS CONCEPTS TO HYBRID NN-HMM MODEL FOR SPEECH RECOGNITION.pdf:pdf},
isbn = {9781467300469},
journal = {Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on},
pages = {4277--4280},
title = {{APPLYING CONVOLUTIONAL NEURAL NETWORKS CONCEPTS TO HYBRID NN-HMM MODEL FOR SPEECH RECOGNITION}},
year = {2012}
}
@inproceedings{inproceedings:genre_bass,
author = {Abe{\ss}er, Jakob and Lukashevich, Hanna and Dittmar, Christian and Schuller, Gerald},
booktitle = {Proc. of the Intl. Conf. on Music Information Retrieval (ISMIR)},
title = {{Genre classification using bass-related high-level features and playing styles}},
year = {2009}
}
@article{Adda-Decker2003,
author = {Adda-Decker, M and Antoine, Fabien and Vasilescu, Ioana},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Adda-Decker, Antoine, Vasilescu - 2003 - Phonetic knowledge, phonotactics and perceptual validation for automatic language identificatio.pdf:pdf},
journal = {{\ldots} Congress of Phonetic {\ldots}},
pages = {1--4},
title = {{Phonetic knowledge, phonotactics and perceptual validation for automatic language identification}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.195.1264},
year = {2003}
}
@article{Adda-Decker2001,
author = {Adda-Decker, Martine},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Adda-Decker - 2001 - Automatic language identification.pdf:pdf},
journal = {Speech Communication},
title = {{Automatic language identification}},
url = {http://www.sciencedirect.com/science/article/pii/S0167639300000996},
year = {2001}
}
@article{Akbacak2013,
author = {Akbacak, Murat and Burget, Lukas},
file = {:Users/anna/Thesis/Papers/akbacak{\_}icassp2013{\_}0008267.pdf:pdf},
isbn = {9781479903566},
journal = {Acoustics, Speech and {\ldots}},
pages = {8267--8271},
title = {{Rich system combination for keyword spotting in noisy and acoustically heterogenous audio streams}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6639277},
year = {2013}
}
@inproceedings{allen,
address = {Shanghai, China},
author = {Allen, F and Ambikairajah, E and Epps, J},
booktitle = {2005 IEEE 7th Workshop on Multimedia Signal Processing},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Allen, Ambikairajah, Epps - 2006 - Language identification using Warping and the Shifted Delta Cepstrum.pdf:pdf},
pages = {1--4},
title = {{Language identification using Warping and the Shifted Delta Cepstrum}},
year = {2006}
}
@article{Amir2001,
author = {Amir, Amon and Efrat, Alon and Srinivasan, S},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Amir, Efrat, Srinivasan - 2001 - Advances in phonetic word spotting.pdf:pdf},
journal = {{\ldots} of the tenth international conference on {\ldots}},
pages = {580--582},
title = {{Advances in phonetic word spotting}},
url = {http://dl.acm.org/citation.cfm?id=502697},
year = {2001}
}
@article{Andrews2006,
author = {Andrews, Edna and Hausburg, Taylor},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Andrews, Hausburg - 2006 - Revisiting Spoken and Musical Phonemic Production and Comprehension.pdf:pdf},
number = {1952},
title = {{Revisiting Spoken and Musical Phonemic Production and Comprehension}},
year = {2006}
}
@article{Anguera2013,
author = {Anguera, Xavier},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Anguera - 2013 - Information Retrieval-based Dynamic Time Warping(2).pdf:pdf},
journal = {Proc. Interspeech, Lyon, France},
title = {{Information Retrieval-based Dynamic Time Warping}},
url = {http://www.xavieranguera.com/papers/interspeech2013.pdf},
year = {2013}
}
@inproceedings{events_atrey,
address = {Toulouse, France},
author = {Atrey, Pradeep K and Maddage, Namunu C and Kankanhalli, Mohan S},
booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
pages = {813--816},
title = {{Audio based event detection for multimedia surveillance}},
year = {2006}
}
@inproceedings{glass_ceiling,
author = {Aucouturier, J.-J. and Pachet, F},
booktitle = {Journal of Negative Results in Speech and Audio Sciences},
title = {{Improving timbre similarity: How high is the sky?}},
volume = {1},
year = {2004}
}
@article{article:representing_musical_genre,
author = {Aucouturier, Jean-Julian and Pachet, Fran�ois},
journal = {Journal of New Music Research},
number = {1},
pages = {83--93},
title = {{Representing Musical Genre: A State of the Art}},
volume = {32},
year = {2003}
}
@article{Bahdanau2014,
abstract = {Neural machine translation is a recently proposed approach to machine transla-tion. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neu-ral machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architec-ture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
doi = {10.1146/annurev.neuro.26.041002.131047},
eprint = {1409.0473},
file = {:Users/anna/Work/Interspeech16/whitepaper/bahdanau{\_}endtoend{\_}16.pdf:pdf},
isbn = {0147-006X (Print)},
issn = {0147-006X},
journal = {Iclr 2015},
keywords = {Neural machine translation is a recently proposed,Unlike the traditional statistical machine transla,a source sentence into a fixed-length vector from,and propose to extend this by allowing a model to,bottleneck in improving the performance of this ba,for parts of a source sentence that are relevant t,having to form these parts as a hard segment expli,machine translation often belong to a family of en,maximize the translation performance. The models p,phrase-based system on the task of English-to-Fren,qualitative analysis reveals that the (soft-)align,the neural machine,translation aims at building a single neural netwo,translation. In this paper,we achieve a translation performance comparable to,we conjecture that the use of a fixed-length vecto,well with our intuition,without},
pages = {1--15},
pmid = {14527267},
title = {{Neural Machine Translation By Jointly Learning To Align and Translate}},
url = {http://arxiv.org/abs/1409.0473v3},
year = {2014}
}
@article{Bahdanau2015,
abstract = {Many of the current state-of-the-art Large Vocabulary Con-tinuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs). Most of these systems contain separate components that deal with the acoustic modelling, language modelling and se-quence decoding. We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Net-work (RNN) that performs sequence prediction directly at the character level. Alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the RNN. For each predicted character, the attention mechanism scans the input sequence and chooses relevant frames. We propose two methods to speed up this operation: limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames, thereby reducing source sequence length. Integrating an n-gram language model into the decoding process yields recognition accuracies similar to other HMM-free RNN-based approaches.},
archivePrefix = {arXiv},
arxivId = {1508.04395},
author = {Bahdanau, Dzmitry and Chorowski, Jan and Serdyuk, Dmitriy and Brakel, Phil{\'{e}}mon and Bengio, Yoshua},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1508.04395},
file = {:Users/anna/Work/Interspeech16/whitepaper/bahdanau{\_}attention{\_}16.pdf:pdf},
isbn = {9780874216561},
issn = {13514180},
journal = {arXiv},
keywords = {ASR,Index Terms— neural networks,LVCSR,attention,speech recognition},
pages = {4945--4949},
pmid = {15991970},
title = {{End-To-End Attention-Based Large Vocabulary Speech Recognition}},
year = {2015}
}
@article{Bai2015,
author = {Bai, Linxue and Jan{\v{c}}ovi{\v{c}}, Peter and Russell, Martin and Weber, Philip},
file = {:Users/anna/Work/Interspeech16/whitepaper/bai{\_}bottleneck{\_}15.pdf:pdf},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Bottleneck features,Continuous-state hidden Markov model,Formants,Low-dimensional features,Modelling speech dynamics,Neural networks,TIMIT},
number = {July},
pages = {583--587},
title = {{Analysis of a low-dimensional bottleneck neural network representation of speech for modelling speech dynamics}},
volume = {2015-Janua},
year = {2015}
}
@article{Bar-yosef2012,
author = {Bar-yosef, Yossi and Aloni-lavi, Ruth and Opher, Irit and Lotner, Noam and Tetariy, Ella and Silber-varod, Vered and Moyal, Ami and Aviv, Tel},
file = {:Users/anna/Thesis/Papers/Cross-Language Phonetic-Search for Keyword Spotting{\_}paper.pdf:pdf},
journal = {{\ldots} of 2012 speech {\ldots}},
keywords = {-spotting,phonetic-search,under-resourced languages},
title = {{Cross-language phonetic search for keyword spotting}},
url = {http://events.eventact.com/afeka/afeka/Cross-Language Phonetic-Search for Keyword Spotting{\_}paper.pdf},
year = {2012}
}
@article{article:patterns_in_music,
author = {Barone, Joseph A},
journal = {Expedition},
number = {1},
pages = {29--31},
title = {{Patterns in Music}},
volume = {3},
year = {1960}
}
@article{Bell,
author = {Bell, Peter and Renals, Steve},
file = {:Users/anna/Work/Interspeech16/whitepaper/bell{\_}multitask{\_}15.pdf:pdf},
keywords = {deep neural network,regularization},
title = {{Regularization Of Context-dependent Deep Neural Networks With Context-independent Multi-task Training}}
}
@article{onset_bello,
author = {Bello, J P and Daudet, L and Abdullah, S and Duxbury, C and Davies, M and Sandler, M},
journal = {IEEE Transactions on Speech and Audio Processing},
month = {sep},
number = {5},
pages = {1035--1047},
title = {{A Tutorial on Onset Detection in Music Signals}},
volume = {13},
year = {2005}
}
@article{Bengio2009,
author = {Bengio, Y.},
doi = {10.1561/2200000006},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Bengio - 2009 - Learning Deep Architectures for AI.pdf:pdf},
issn = {1935-8237},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
number = {1},
pages = {1--127},
title = {{Learning Deep Architectures for AI}},
url = {http://www.nowpublishers.com/article/Details/MAL-006},
volume = {2},
year = {2009}
}
@article{Bengio1994,
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Bengio, Yoshua},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
file = {:Users/anna/Work/Interspeech16/whitepaper/bengi{\_}gradient{\_}94.pdf:pdf},
isbn = {1045-9227 VO - 5},
issn = {19410093},
journal = {IEEE Transactions on Neural Networks},
number = {2},
pmid = {18267787},
title = {{Learning long-term dependencies with gradient descent is difficult}},
volume = {5},
year = {1994}
}
@article{Bengio2007,
author = {Bengio, Yoshua and Lamblin, Pascal},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Bengio, Lamblin - 2007 - Greedy layer-wise training of deep networks.pdf:pdf},
journal = {Advances in neural {\ldots}},
number = {1},
title = {{Greedy layer-wise training of deep networks}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=Tbn1l9P1220C{\&}oi=fnd{\&}pg=PA153{\&}dq=Greedy+Layer-Wise+Training+of+Deep+Networks{\&}ots=V3n6Enkl42{\&}sig=AIp409-ERtrnDjuyT5ZZCxXpF3I},
year = {2007}
}
@article{BenZeghiba2009,
author = {BenZeghiba, MF and Gauvain, JL and Lamel, Lori},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/BenZeghiba, Gauvain, Lamel - 2009 - Language score calibration using adapted Gaussian back-end.pdf:pdf},
journal = {INTERSPEECH},
keywords = {[Electronic Manuscript]},
title = {{Language score calibration using adapted Gaussian back-end.}},
url = {http://www.quaero.org/media/files/bibliographie/limsi{\_}qpr6{\_}benzeghiba.pdf},
year = {2009}
}
@phdthesis{Berkling1997,
author = {Berkling, Kay Margarethe},
booktitle = {Dissertation Abstracts International, B: Sciences and Engineering},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Berkling - 1997 - Automatic Language Identification with Sequences of Language-Independent Phoneme Clusters.pdf:pdf},
issn = {04194217},
number = {3},
title = {{Automatic Language Identification with Sequences of Language-Independent Phoneme Clusters}},
volume = {58},
year = {1997}
}
@inproceedings{bielefeld,
address = {Baltimore, MD, USA},
author = {Bielefeld, B},
booktitle = {Fourteenth annual speech research symposium},
title = {{Language identification using shifted delta cepstrum}},
year = {1994}
}
@article{Bock2015,
author = {B{\"{o}}ck, Sebastian and Krebs, Florian and Widmer, Gerhard},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/B{\"{o}}ck, Krebs, Widmer - 2015 - Accurate Tempo Estimation based on Recurrent Neural Networks and Resonating Comb Filters.pdf:pdf},
journal = {Proceedings of the 16th International Society for Music Information Retrieval Conference},
pages = {625--631},
title = {{Accurate Tempo Estimation based on Recurrent Neural Networks and Resonating Comb Filters}},
year = {2015}
}
@article{Bock2016,
abstract = {In this paper we present a novel method for jointly extract-ing beats and downbeats from audio signals. A recurrent neural network operating directly on magnitude spectro-grams is used to model the metrical structure of the audio signals at multiple levels and provides an output feature that clearly distinguishes between beats and downbeats. A dynamic Bayesian network is then used to model bars of variable length and align the predicted beat and down-beat positions to the global best solution. We find that the proposed model achieves state-of-the-art performance on a wide range of different musical genres and styles.},
author = {B{\"{o}}ck, Sebastian and Krebs, Florian and Widmer, Gerhard},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/B{\"{o}}ck, Krebs, Widmer - 2016 - Joint Beat and Downbeat Tracking with Recurrent Neural Networks.pdf:pdf},
journal = {Proceedings of the 17th International Society for Music Information Retrieval Conference},
pages = {255--261},
title = {{Joint Beat and Downbeat Tracking with Recurrent Neural Networks}},
year = {2016}
}
@inproceedings{Bonafonte1993,
address = {Berlin, Germany},
author = {Bonafonte, A and Ros, X and Marino, J.B.},
booktitle = {3rd European Conference on Speech Communication and Technology (EUROSPEECH)},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Bonafonte, Ros, Marino - 1993 - An efficient algorithm to find the best state sequence in HSMM.pdf:pdf},
pages = {1547--1550},
title = {{An efficient algorithm to find the best state sequence in HSMM}},
year = {1993}
}
@article{article:intro_svm,
author = {Boswell, Dustin},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Boswell - 2002 - Introduction to Support Vector Machines.pdf:pdf},
title = {{Introduction to Support Vector Machines}},
year = {2002}
}
@inproceedings{bridle,
author = {Bridle, J S},
booktitle = {Brit. Acoust. Soc. Meeting},
pages = {1--4},
title = {{An efficient elastic-template method for detecting given words in running speech}},
year = {1973}
}
@book{book:weltmusik,
author = {Broughton, Simon and Burton, Kim and Ellingham, Mark and Muddyman, David and (Hrsg.), Richard Trillo},
publisher = {J. B. Metzler},
series = {Rough Guide},
title = {{Weltmusik}},
year = {2000}
}
@article{Brummer2009,
author = {Br{\"{u}}mmer, N},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Br{\"{u}}mmer - 2009 - The EM algorithm and Minimum Divergence.pdf:pdf},
pages = {1--7},
title = {{The EM algorithm and Minimum Divergence}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:The+EM+algorithm+and+Minimum+Divergence{\#}8},
volume = {1},
year = {2009}
}
@article{Bucilua2006,
abstract = {Often the best performing supervised learning models are ensembles of hundreds or thousands of base-level classifiers. Unfortunately, the space required to store this many classifiers, and the time required to execute them at run-time, prohibits their use in applications where test sets are large (e.g. Google), where storage space is at a premium (e.g. PDAs), and where computational power is limited (e.g. hea-ring aids). We present a method for "compressing" large, complex ensembles into smaller, faster models, usually without significant loss in performance.},
author = {Bucilǎ, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru},
doi = {10.1145/1150402.1150464},
file = {:Users/anna/Work/Interspeech16/whitepaper/bucila{\_}compression{\_}06.pdf:pdf},
isbn = {1595933395},
journal = {Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '06},
keywords = {model compression,supervised learning},
pages = {535},
title = {{Model compression}},
url = {http://portal.acm.org/citation.cfm?doid=1150402.1150464},
year = {2006}
}
@article{article:svm_tutorial,
author = {Burges, Christopher J C},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Burges - 1998 - A tutorial on support vector machines for pattern recognition.pdf:pdf},
journal = {Data mining and knowledge discovery},
number = {2},
pages = {121--167},
title = {{A tutorial on support vector machines for pattern recognition}},
volume = {2},
year = {1998}
}
@article{Burshtein1996,
author = {Burshtein, D.},
doi = {10.1109/89.496221},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Burshtein - 1996 - Robust parametric modeling of durations in hidden Markov models.pdf:pdf},
issn = {10636676},
journal = {IEEE Transactions on Speech and Audio Processing},
month = {may},
number = {3},
pages = {240--242},
title = {{Robust parametric modeling of durations in hidden Markov models}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=496221},
volume = {4},
year = {1996}
}
@article{Buzo2013,
author = {Buzo, Andi and Cucu, Horia and Safta, Mihai and Burileanu, Corneliu},
doi = {10.1109/SpeD.2013.6682655},
file = {:Users/anna/Thesis/Papers/2013{\_}sped{\_}std{\_}v3.pdf:pdf},
isbn = {978-1-4799-1065-6},
journal = {2013 7th Conference on Speech Technology and Human - Computer Dialogue (SpeD)},
keywords = {an attractive solution that,but it is also,leads to a full,model,multilingual acoustic,speech,spoken term detection,under-resourced languages},
month = {oct},
pages = {1--6},
publisher = {Ieee},
title = {{Multilingual query by example spoken term detection for under-resourced languages}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6682655},
year = {2013}
}
@article{campbell,
author = {Campbell, W M and Campbell, J P and Reynolds, D A and Singer, E and Torres-Carrasquillo, P A},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Campbell et al. - 2006 - Support vector machines for speaker and language recognition.pdf:pdf},
journal = {Computer Speech and Language},
pages = {210--229},
title = {{Support vector machines for speaker and language recognition}},
volume = {20},
year = {2006}
}
@article{Can2011,
author = {Can, D and Saraclar, M},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Can, Saraclar - 2011 - Lattice indexing for spoken term detection.pdf:pdf},
journal = {Audio, Speech, and Language Processing, {\ldots}},
number = {8},
pages = {2338--2347},
title = {{Lattice indexing for spoken term detection}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5752829},
volume = {19},
year = {2011}
}
@book{Carbonell2004,
author = {Carbonell, J G and Siekmann, J},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Carbonell, Siekmann - 2004 - Text, speech and dialogue.pdf:pdf},
isbn = {3540230491},
title = {{Text, speech and dialogue}},
year = {2004}
}
@article{Caruana1997,
abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents newresults forMTLwith k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learningworks, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.},
author = {Caruana, Rich},
doi = {10.1023/A:1007379606734},
file = {:Users/anna/Work/Interspeech16/whitepaper/caruana{\_}multitask{\_}97.pdf:pdf},
isbn = {1461375274},
issn = {1573-0565},
journal = {Machine Learning},
keywords = {backpropagation,generalization,inductive transfer,k-nearest neighbor,kernel,multitask learning,parallel transfer,regression,supervised learning},
number = {1},
pages = {41--75},
pmid = {20421687},
title = {{Multitask Learning}},
volume = {28},
year = {1997}
}
@inproceedings{Caseiro2002,
author = {Caseiro, D. and Meinedo, H. and Serralheiro, A. and Trancoso, I. and Neto, J.},
booktitle = {Proceedings of the second international conference on Human Language Technology Research},
file = {:Users/anna/Thesis/Papers/10.1.1.18.2176.ps:ps},
pages = {196},
title = {{Spoken book alignment using WFSTs}},
url = {http://portal.acm.org/citation.cfm?id=1289189.1289195},
year = {2002}
}
@article{Casey2008,
author = {Casey, Michael A. and Veltkamp, Remco and Goto, Masataka and Leman, Marc and Rhodes, Christopher and Slaney, Malcolm},
doi = {10.1109/JPROC.2008.916370},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Casey et al. - 2008 - Content-Based Music Information Retrieval Current Directions and Future Challenges.pdf:pdf},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
month = {apr},
number = {4},
pages = {668--696},
title = {{Content-Based Music Information Retrieval: Current Directions and Future Challenges}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4472077},
volume = {96},
year = {2008}
}
@inproceedings{inproceedings:folk_music_hmm,
author = {Chai, Wei and Vercoe, Barry},
booktitle = {IC-AI},
title = {{Folk Music Classification Using Hidden Markov Models}},
year = {2001}
}
@article{Chan2015,
abstract = {We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits characters as outputs. The network produces character sequences without making any independence assumptions between the characters. This is the key improvement of LAS over previous end-to-end CTC models. On a subset of the Google voice search task, LAS achieves a word error rate (WER) of 14.1{\%} without a dictionary or a language model, and 10.3{\%} with language model rescoring over the top 32 beams. By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0{\%}.},
archivePrefix = {arXiv},
arxivId = {1508.01211},
author = {Chan, William and Jaitly, Navdeep and Le, Quoc V. and Vinyals, Oriol},
doi = {10.1109/72.279181},
eprint = {1508.01211},
file = {:Users/anna/Work/Interspeech16/whitepaper/chan{\_}attention{\_}15.pdf:pdf},
isbn = {1045-9227 VO - 5},
issn = {19410093},
journal = {arXiv preprint},
pages = {1--16},
pmid = {18267787},
title = {{Listen, attend and spell}},
url = {http://arxiv.org/abs/1508.01211},
year = {2015}
}
@inproceedings{chandrasekhar,
address = {Prague, Czech Republic},
author = {Chandraskehar, V and Sargin, M E and Ross, D A},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Chandraskehar, Sargin, Ross - 2011 - Automatic language identification in music videos with low level audio and visual features.pdf:pdf},
pages = {5724--5727},
title = {{Automatic language identification in music videos with low level audio and visual features}},
year = {2011}
}
@misc{misc:libsvm,
author = {Chang, Chih-Chung and Lin, Chih-Jen},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Chang, Lin - 2001 - {\{}LIBSVM{\}} - A Library for Support Vector Machines.pdf:pdf},
title = {{{\{}LIBSVM{\}} - A Library for Support Vector Machines}},
url = {http://www.csie.ntu.edu.tw/{\%}5C{~}cjlin/libsvm/},
year = {2001}
}
@article{Charbuillet2011,
author = {Charbuillet, C and Tardieu, D and Peeters, G},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Charbuillet, Tardieu, Peeters - 2011 - GMM Supervector for content based music similarity.pdf:pdf},
journal = {{\ldots} Conference on Digital {\ldots}},
number = {1},
pages = {1--4},
title = {{GMM Supervector for content based music similarity}},
url = {http://recherche.ircam.fr/pub/dafx11/Papers/98{\_}e.pdf},
year = {2011}
}
@inproceedings{Chen2016a,
author = {Chen, Hongjie and Leung, Cheung-chi and Xie, Lei and Ma, Bin and Li, Haizhou},
booktitle = {Interspeech},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2016 - Unsupervised Bottleneck Features for Low-Resource Query-by-Example Spoken Term Detection.pdf:pdf},
keywords = {[Electronic Manuscript]},
pages = {923--927},
title = {{Unsupervised Bottleneck Features for Low-Resource Query-by-Example Spoken Term Detection}},
year = {2016}
}
@inproceedings{Chen2016,
author = {Chen, Zhehuai and Deng, Wei and Xu, Tao and Yu, Kai},
booktitle = {Interspeech},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2016 - Phone Synchronous Decoding with CTC Lattice.pdf:pdf},
keywords = {[Electronic Manuscript]},
pages = {1923--1927},
title = {{Phone Synchronous Decoding with CTC Lattice}},
year = {2016}
}
@inproceedings{Chen2015,
author = {Chen, Zhuo and Watanabe, Shinji and Erdogan, Hakan and Hershey, John R},
booktitle = {Interspeech},
file = {:Users/anna/Work/Interspeech16/whitepaper/chen{\_}multitask{\_}15.pdf:pdf},
keywords = {[Electronic Manuscript]},
pages = {3274--3278},
title = {{Speech enhancement and recognition using multi-task learning of long short-term memory recurrent neural networks}},
year = {2015}
}
@article{Cho2014a,
abstract = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
archivePrefix = {arXiv},
arxivId = {arXiv:1409.1259v2},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
eprint = {arXiv:1409.1259v2},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Cho et al. - 2014 - On the Properties of Neural Machine Translation Encoder–Decoder Approaches.pdf:pdf},
isbn = {9781937284961},
journal = {Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation},
pages = {103--111},
title = {{On the Properties of Neural Machine Translation: Encoder–Decoder Approaches}},
url = {http://arxiv.org/pdf/1409.1259v2.pdf{\%}5Cnhttp://arxiv.org/abs/1409.1259},
year = {2014}
}
@article{Cho2014,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder--Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder--Decoder as an additional feature in the existing linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
doi = {10.3115/v1/D14-1179},
eprint = {1406.1078},
file = {:Users/anna/Work/Interspeech16/whitepaper/cho{\_}endtoend{\_}14.pdf:pdf},
isbn = {9781937284961},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
keywords = {decoder,for statistical machine translation,rning phrase representations using,rnn encoder},
pages = {1724--1734},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
url = {http://arxiv.org/abs/1406.1078},
year = {2014}
}
@inproceedings{inproceedings:raag,
author = {Chordia, Parag and Rae, Alex},
booktitle = {International Conference on New Interfaces for Musical Expression (NIME)},
title = {{Real-Time Raag Recognition for Interactive Music}},
year = {2008}
}
@article{Chorowski2014,
abstract = {We replace the Hidden Markov Model (HMM) which is traditionally used in in continuous speech recognition with a bi-directional recurrent neural network encoder coupled to a recurrent neural network decoder that directly emits a stream of phonemes. The alignment between the input and output sequences is established using an attention mechanism: the decoder emits each symbol based on a context created with a subset of input symbols elected by the attention mechanism. We report initial results demonstrating that this new approach achieves phoneme error rates that are comparable to the state-of-the-art HMM-based decoders, on the TIMIT dataset.},
archivePrefix = {arXiv},
arxivId = {1412.1602},
author = {Chorowski, Jan and Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {1412.1602},
file = {:Users/anna/Work/Interspeech16/whitepaper/chorowski{\_}attention{\_}14.pdf:pdf},
journal = {Deep Learning and Representation Learning Workshop, NIPS 2014},
pages = {1--10},
title = {{End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results}},
url = {http://arxiv.org/abs/1412.1602},
year = {2014}
}
@article{Chorowski2015,
abstract = {Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis and image caption generation. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation reaches a competitive 18.6$\backslash${\%} phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18$\backslash${\%} PER in single utterances and 20$\backslash${\%} in 10-times longer (repeated) utterances. Finally, we propose a change to the attention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6$\backslash${\%} level.},
archivePrefix = {arXiv},
arxivId = {1506.07503},
author = {Chorowski, Jan and Bahdanau, Dzmitry and Serdyuk, Dmitriy and Cho, Kyunghyun and Bengio, Yoshua},
doi = {10.1016/j.asr.2015.02.035},
eprint = {1506.07503},
file = {:Users/anna/Work/Interspeech16/whitepaper/chorowski{\_}attention{\_}15.pdf:pdf},
issn = {18791948},
journal = {Advances in Neural Information Processing Systems 28},
pages = {577--585},
title = {{Attention-Based Models for Speech Recognition}},
url = {http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf},
year = {2015}
}
@article{Chrupaa2015,
abstract = {We propose Imaginet, a model of learning visually grounded representations of language from coupled textual and visual input. The model consists of two Gated Recurrent Unit networks with shared word embeddings, and uses a multi-task objective by receiving a textual description of a scene and trying to concurrently predict its visual representation and the next word in the sentence. Like humans, it acquires meaning representations for individual words from descriptions of visual scenes. Moreover, it learns to effectively use sequential structure in semantic interpretation of multi-word phrases.},
archivePrefix = {arXiv},
arxivId = {1506.03694},
author = {Chrupa{\l}a, Grzegorz and K{\'{a}}d{\'{a}}r, {\'{A}}kos and Alishahi, Afra},
eprint = {1506.03694},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Chrupa{\l}a, K{\'{a}}d{\'{a}}r, Alishahi - 2015 - Learning language through pictures.pdf:pdf},
journal = {arXiv:1506.03694 [cs]},
title = {{Learning language through pictures}},
url = {http://arxiv.org/abs/1506.03694},
year = {2015}
}
@article{Chung2014,
abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
archivePrefix = {arXiv},
arxivId = {1412.3555v1},
author = {Chung, Junyoung and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {1412.3555v1},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.pdf:pdf},
journal = {arXiv},
pages = {1--9},
title = {{Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}},
year = {2014}
}
@inproceedings{events_clavel,
address = {Amsterdam, The Netherlands},
author = {Clavel, Chlo{\'{e}} and Ehrette, Thibaut and Richard, Ga{\"{e}}l},
booktitle = {International Conference on Multimedia and Expo (ICME)},
pages = {1306--1309},
publisher = {IEEE},
title = {{Events Detection for an Audio-Based Surveillance System.}},
year = {2005}
}
@inproceedings{inproceedings:global_access,
author = {Cornelis, Olmo and Moelants, Dirk and Leman, Marc},
booktitle = {Proc. of the Intl. Conf. on Music Information Retrieval (ISMIR)},
title = {{Global Access to Ethnic Music: The next big Challenge?}},
year = {2009}
}
@misc{website:allmusic,
annote = {$\backslash$url{\{}http://www.allmusic.com/{\}}},
author = {Corporation, Online-Quelle: Rovi},
title = {allmusic}
}
@article{shot_detection,
author = {Cotsaces, C and Nikolaidis, N and Pitas, I},
journal = {IEEE Signal Processing Magazine},
number = {2},
pages = {28--37},
title = {{Shot detection and condensed representation -- A review}},
volume = {23},
year = {2006}
}
@article{Cumani2012,
author = {Cumani, S},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Cumani - 2012 - Speaker and Language Recognition Techniques.pdf:pdf},
title = {{Speaker and Language Recognition Techniques}},
url = {http://porto.polito.it/2496928/},
year = {2012}
}
@phdthesis{Dehak2009,
author = {Dehak, Najim},
title = {{Discriminative and generative approaches for long-and short-term speaker characteristics modeling: application to speaker verification}},
url = {http://dl.acm.org/citation.cfm?id=1751362},
year = {2009}
}
@article{Dehak2011,
author = {Dehak, Najim and Kenny, Patrick J and Dehak, Reda and Dumouchel, Pierre and Ouellet, Pierre},
doi = {10.1109/TASL.2010.2064307},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Dehak et al. - 2011 - Front-End Factor Analysis for Speaker Verification.pdf:pdf},
issn = {1558-7916},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
month = {may},
number = {4},
pages = {788--798},
title = {{Front-End Factor Analysis for Speaker Verification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5545402},
volume = {19},
year = {2011}
}
@article{DeMenthon2000,
author = {DeMenthon, D},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/DeMenthon - 2000 - Hidden markov models for images.pdf:pdf},
journal = {Int. Conf. on Pattern {\ldots}},
number = {September},
title = {{Hidden markov models for images}},
url = {http://lampsrv02.umiacs.umd.edu/pubs/Papers/ICPR00-HMM/ICPR00-HMM.pdf},
year = {2000}
}
@article{Dempster1977,
author = {Dempster, AP and Laird, NM and Rubin, DB},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Dempster, Laird, Rubin - 1977 - Maximum likelihood from incomplete data via the EM algorithm.pdf:pdf},
journal = {Journal of the Royal statistical {\ldots}},
number = {1},
pages = {1--38},
title = {{Maximum likelihood from incomplete data via the EM algorithm}},
url = {http://www.academia.edu/download/31110297/dempster{\_}EM{\_}1977.pdf},
volume = {39},
year = {1977}
}
@article{Deng2016,
author = {Deng, Junqi and Kwok, Yu-kwong},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Deng, Kwok - 2016 - A hybrid Gaussian-HMM-Deep-Learning approach for automatic chord estimation with very large vocabulary.pdf:pdf},
journal = {Proc. 17th International Society for Music Information Retrieval Conference},
pages = {812--818},
title = {{A hybrid Gaussian-HMM-Deep-Learning approach for automatic chord estimation with very large vocabulary}},
year = {2016}
}
@article{Denton2015,
abstract = {In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach (Goodfellow et al.). Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40{\%} of the time, compared to 10{\%} for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset.},
archivePrefix = {arXiv},
arxivId = {1506.05751},
author = {Denton, Emily and Chintala, Soumith and Szlam, Arthur and Fergus, Rob},
eprint = {1506.05751},
file = {:Users/anna/Work/Interspeech16/whitepaper/denton{\_}adversarial{\_}15.pdf:pdf},
issn = {10495258},
journal = {Arxiv},
pages = {1--10},
title = {{Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks}},
url = {http://arxiv.org/abs/1506.05751},
year = {2015}
}
@inproceedings{soundtracks,
address = {Copenhagen, Denmark},
author = {Dhandhania, V and Abesser, J and Kruspe, A and Grossmann, H},
booktitle = {Proceedings of the 9th Sound and Music Computing Conference (SMC)},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Dhandhania et al. - 2012 - Automatic And Manual Annotation Of Time-Varying Perceptual Properties In Movie Soundtracks.pdf:pdf},
pages = {461--466},
title = {{Automatic And Manual Annotation Of Time-Varying Perceptual Properties In Movie Soundtracks}},
year = {2012}
}
@inproceedings{Dielemann2014,
author = {Dielemann, Sander and Schrauwer, Benjamin},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
file = {:Users/anna/Downloads/06854950.pdf:pdf},
isbn = {9781479928934},
pages = {6964--6968},
title = {{End-to-end learning for music audio}},
year = {2014}
}
@inproceedings{pedro,
author = {Dittmar, Christian and Mercado, Pedro and Gro�mann, Holger and Cano, Estefan�a},
booktitle = {3rd International Workshop on Cognitive Information Processing (CIP)},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Dittmar et al. - 2012 - Towards lyrics spotting in the {\{}SyncGlobal{\}} project.pdf:pdf},
title = {{Towards lyrics spotting in the {\{}SyncGlobal{\}} project}},
year = {2012}
}
@inproceedings{Do2016,
author = {Do, Quoc Truong and Sakti, Sakriani and Neubig, Graham and Nakamura, Satoshi},
booktitle = {Interspeech},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Do et al. - 2016 - Transferring Emphasis in Speech Translation Using Hard-Attentional Neural Network Models.pdf:pdf},
keywords = {[Electronic Manuscript]},
pages = {2533--2537},
title = {{Transferring Emphasis in Speech Translation Using Hard-Attentional Neural Network Models}},
year = {2016}
}
@inproceedings{Dorfer,
abstract = {This paper addresses the matching of short music audio snippets to the corresponding pixel location in images of sheet music. A system is presented that simultaneously learns to read notes, listens to music and matches the currently played music to its corresponding notes in the sheet. It consists of an end-to-end multi-modal convolu-tional neural network that takes as input images of sheet music and spectrograms of the respective audio snippets. It learns to predict, for a given unseen audio snippet (cov-ering approximately one bar of music), the corresponding position in the respective score line. Our results suggest that with the use of (deep) neural networks – which have proven to be powerful image processing models – working with sheet music becomes feasible and a promising future research direction.},
author = {Dorfer, Matthias and Arzt, Andreas and Widmer, Gerhard},
booktitle = {Proceedings of the 17th International Society for Music Information Retrieval Conference},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Dorfer, Arzt, Widmer - Unknown - Towards Score Following in Sheet Music Images.pdf:pdf},
number = {Section 3},
title = {{Towards Score Following in Sheet Music Images}},
year = {2016}
}
@incollection{incollection:mir,
author = {Downie, J Stephen},
booktitle = {Annual Review of Information Science and Technology 37},
chapter = {7},
editor = {Cronin, Blaise},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Downie - 2003 - Music information retrieval.pdf:pdf},
pages = {295--340},
title = {{Music information retrieval}},
year = {2003}
}
@misc{Dressler,
author = {Dressler, Karin},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Dressler - Unknown - An auditory streaming approach for melody extraction from polyphonic music.pdf:pdf},
title = {{An auditory streaming approach for melody extraction from polyphonic music}}
}
@inproceedings{inproceedings:dressler,
author = {Dressler, Karin},
booktitle = {Proc. of the 9th Int. Conference on Digital Audio Effects (DAFx-06)},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Dressler - 2006 - Sinusoidal Extraction using an efficient implementation of a multi-resolution {\{}FFT{\}}(2).pdf:pdf},
month = {sep},
pages = {247--252},
title = {{Sinusoidal Extraction using an efficient implementation of a multi-resolution {\{}FFT{\}}}},
year = {2006}
}
@inproceedings{dustor,
address = {Gliwice, Poland},
author = {Dustor, A and Szwarc, P},
booktitle = {International conference on signals and electronic systems(ICSES)},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Dustor, Szwarc - 2010 - Spoken language identification based on GMM models.pdf:pdf},
pages = {105--108},
title = {{Spoken language identification based on GMM models}},
year = {2010}
}
@article{Dzhambazov,
author = {Dzhambazov, Georgi and Sertan, S},
file = {:Users/anna/Downloads/249{\_}Paper.pdf:pdf},
keywords = {16,a user is,in kws,in speech,interest,interested to find at,is spoken,key-,kws,most of the work,on searching for keywords,presenting a topic of,relevant keyword,spotting,which time position a},
title = {{SEARCHING LYRICAL PHRASES IN A-CAPELLA TURKISH MAKAM RECORDINGS}}
}
@article{Einar2012,
author = {Einar, {\AA}smund and Tokheim, Haugland},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Einar, Tokheim - 2012 - iVector Based Language Recognition.pdf:pdf},
number = {June},
title = {{iVector Based Language Recognition}},
year = {2012}
}
@article{Enriquez2012,
author = {Enr{\'{i}}quez, LF D'haro and Glembek, Ondřej and Plchot, Oldřich},
file = {:Users/anna/Thesis/Papers/Phonotactic{\_}Interspeech2012.pdf:pdf},
pages = {4--7},
title = {{Phonotactic language recognition using i-vectors and phoneme posteriogram counts}},
url = {http://oa.upm.es/20403/1/INVE{\_}MEM{\_}2012{\_}134401.pdf},
year = {2012}
}
@article{Evgrafova2012,
author = {Evgrafova, Karina and Evdokimova, Vera},
doi = {10.3233/978-1-61499-133-5-42},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Evgrafova, Evdokimova - 2012 - Perception of Russian Vowels in Singing.pdf:pdf},
isbn = {9781614991335},
journal = {Baltic HLT},
keywords = {acoustics,aural-perception,sung vowels,vowel intelligibility},
pages = {42--49},
title = {{Perception of Russian Vowels in Singing.}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=ve{\_}AAQAAQBAJ{\&}oi=fnd{\&}pg=PA42{\&}dq=Perception+of+Russian+Vowels+in+Singing{\&}ots=GaavmXQnU5{\&}sig=vORhGuTJFGq{\_}L0fBxvhYO3PTBDM},
year = {2012}
}
@inproceedings{jhu_lid2,
author = {Farris, Dave and White, Chris and Khudanpur, Sanjeev},
booktitle = {Proceedings of the IEEE International Conference on Acoustics Speech and Signal Processing},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Farris, White, Khudanpur - 2008 - Sample Selection for Automatic Language Identification.pdf:pdf},
pages = {4225--4228},
title = {{Sample Selection for Automatic Language Identification}},
year = {2008}
}
@article{Ferrer,
author = {Ferrer, ASMAL and Kajarekar, Sachin and Shriberg, CRNSE},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Ferrer, Kajarekar, Shriberg - Unknown - Improving Language Recognition with Multilingual Phone Recognition and Speaker Adaptation Transf.pdf:pdf},
journal = {researchgate.net},
title = {{Improving Language Recognition with Multilingual Phone Recognition and Speaker Adaptation Transforms}},
url = {http://www.researchgate.net/publication/228957500{\_}Improving{\_}Language{\_}Recognition{\_}with{\_}Multilingual{\_}Phone{\_}Recognition{\_}and{\_}Speaker{\_}Adaptation{\_}Transforms/file/9c96051671c695b8a2.pdf}
}
@inproceedings{Fingerhut2004,
address = {Oslo, Norway},
author = {Fingerhut, Michael},
booktitle = {IAML-IASA Congress},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Fingerhut - 2004 - Music Information Retrieval , or how to search for ( and maybe find ) music and do away with incipits.pdf:pdf},
pages = {1--19},
title = {{Music Information Retrieval , or how to search for ( and maybe find ) music and do away with incipits}},
year = {2004}
}
@article{Fischer2013,
author = {Fischer, Andreas and Frinken, Volkmar and Bunke, Horst and Suen, Ching Y.},
doi = {10.1109/ICDAR.2013.107},
file = {:Users/anna/Thesis/Papers/FFB2013-2.pdf:pdf},
isbn = {978-0-7695-4999-6},
journal = {2013 12th International Conference on Document Analysis and Recognition},
keywords = {alternative to locate specific,document images,high,method for keyword spotting,models that showed a,ously proposed a learning-based,search terms within scanned,spotting is a promising,using character hidden markov,we have previ-},
month = {aug},
pages = {506--510},
publisher = {Ieee},
title = {{Improving HMM-Based Keyword Spotting with Character Language Models}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6628672},
year = {2013}
}
@inproceedings{jhu_ks3,
author = {Fousek, P and Hermansky, Hynek},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Fousek, Hermansky - 2006 - Towards {\{}ASR{\}} Based on Hierarchical Posterior-Based Keyword Recognition(2).pdf:pdf},
title = {{Towards {\{}ASR{\}} Based on Hierarchical Posterior-Based Keyword Recognition}},
year = {2006}
}
@article{Fujihara,
author = {Fujihara, Hiromasa and Goto, Masataka},
file = {:Users/anna/Downloads/3.pdf:pdf},
isbn = {9783939897378},
journal = {Multimodal Music Processing},
keywords = {alignment,and phrases lyrics,karaoke,lyrics-based,multifunctional music player},
pages = {23--36},
title = {{Lyrics-to-Audio Alignment and its Application}},
url = {http://drops.dagstuhl.de/opus/volltexte/2012/3464/},
volume = {3}
}
@article{Fujihara2009,
author = {Fujihara, Hiromasa and Goto, Masataka and Okuno, Hiroshi G.},
doi = {10.1109/ASPAA.2009.5346497},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Fujihara, Goto, Okuno - 2009 - A novel framework for recognizing phonemes of singing voice in polyphonic music.pdf:pdf},
isbn = {978-1-4244-3678-1},
journal = {2009 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics},
month = {oct},
pages = {17--20},
publisher = {Ieee},
title = {{A novel framework for recognizing phonemes of singing voice in polyphonic music}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5346497},
year = {2009}
}
@article{Gada2012,
author = {Gada, Jigar},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Gada - 2012 - G UIDELINES FOR USING S PHINX.pdf:pdf},
title = {{G UIDELINES FOR USING S PHINX}},
year = {2012}
}
@article{Gaikwad2010,
author = {Gaikwad, SK},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Gaikwad - 2010 - A review on speech recognition technique.pdf:pdf},
journal = {International Journal of {\ldots}},
keywords = {A Review on Speech Recognition Technique},
number = {3},
title = {{A review on speech recognition technique}},
url = {http://core.kmi.open.ac.uk/download/pdf/865450},
volume = {10},
year = {2010}
}
@article{Ganapathy,
abstract = {The language identification (LID) task in the Robust Automatic Transcription of Speech (RATS) program is challenging due to the noisy nature of the audio data collected over highly de-graded radio communication channels as well as the use of short duration speech segments for testing. In this paper, we report the recent advances made in the RATS LID task by using bottle-neck features from a convolutional neural network (CNN). The CNN, which is trained with labelled data from one of target lan-guages, generates bottleneck features which are used in a Gaus-sian mixture model (GMM)-ivector LID system. The CNN bot-tleneck features provide substantial complimentary information to the conventional acoustic features even on languages not seen in its training. Using these bottleneck features in conjunction with acoustic features, we obtain significant improvements (av-erage relative improvements of 25{\%} in terms of equal error rate (EER) compared to the corresponding acoustic system) for the LID task. Furthermore, these improvements are consistent for various choices of acoustic features as well as speech segment durations.},
author = {Ganapathy, Sriram and Han, Kyu and Thomas, Samuel and Omar, Mohamed and Segbroeck, Maarten Van and Narayanan, Shrikanth S},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Ganapathy et al. - Unknown - Robust Language Identification Using Convolutional Neural Network Features.pdf:pdf},
keywords = {Bottleneck Fea-tures,Convolutional Neural Networks,Index Terms,Language Identification},
title = {{Robust Language Identification Using Convolutional Neural Network Features}}
}
@inproceedings{onset_gao,
address = {Amsterdam, The Netherlands},
author = {Gao, Sheng and Zhu, Yongwei},
booktitle = {International Conference on Multimedia and Expo (ICME)},
pages = {334--337},
publisher = {IEEE},
title = {{A HMM-Embedded Unsupervised Learning to Musical Event Detection.}},
year = {2005}
}
@inproceedings{Garcia-Romero2016,
author = {Garcia-Romero, Daniel and McCree, Alan},
booktitle = {Interspeech},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Garcia-Romero, McCree - 2016 - Stacked long-term TDNN for Spoken Language Recognition.pdf:pdf},
keywords = {[Electronic Manuscript]},
pages = {3226--3230},
title = {{Stacked long-term TDNN for Spoken Language Recognition}},
year = {2016}
}
@article{Garimella2013,
author = {Garimella, Sri and Hermansky, Hynek},
doi = {10.1109/TNNLS.2012.2236652},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Garimella, Hermansky - 2013 - Factor Analysis of Auto-Associative Neural Networks With Application in Speaker Verification.pdf:pdf},
issn = {2162-237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
month = {apr},
number = {4},
pages = {522--528},
title = {{Factor Analysis of Auto-Associative Neural Networks With Application in Speaker Verification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6414643 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6414643},
volume = {24},
year = {2013}
}
@article{Garofolo1999,
abstract = {This paper describes work within the NIST Text REtrieval Conference$\backslash$n(TREC) over the last three years in designing and implementing evaluations$\backslash$nof Spoken Document Retrieval (SDR) technology within a broadcast$\backslash$nnews domain. SDR involves the search and retrieval of excerpts from$\backslash$nspoken audio recordings using a combination of automatic speech recognition$\backslash$nand information retrieval technologies. The TREC SDR Track has provided$\backslash$nan infrastructure for the development and evaluation of SDR technology$\backslash$nand a common forum for the exchange of knowledge between the speech$\backslash$nrecognition and information retrieval research communities. The SDR$\backslash$nTrack can be declared a success in that it has provided objective,$\backslash$ndemonstrable proof that this technology can be successfully applied$\backslash$nto realistic audio collections using a combination of existing technologies$\backslash$nand that it can be objectively evaluated. The design and implementation$\backslash$nof each of the SDR evaluations are presented and the results are$\backslash$nsummarized. Plans for the 2000 TREC SDR Track are presented and thoughts$\backslash$nabout how the track might evolve are discussed.},
author = {Garofolo, John S and Auzanne, Cedric G P and Voorhees, Ellen M},
doi = {10.1234/12345678},
file = {:Users/anna/Downloads/trec8-sdr-overview.pdf:pdf},
journal = {Proc. {\{}TREC-8{\}}},
keywords = {speech inf{\_}retrieval},
number = {500-246},
pages = {109--130},
title = {{The {\{}TREC{\}} Spoken Document Retrieval Track: A Success Story}},
url = {http://trec.nist.gov/pubs.html},
volume = {8940},
year = {1999}
}
@article{Gauvain,
author = {Gauvain, J L and Lamel, L},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Gauvain, Lamel - Unknown - LANGUAGE-INDEPENDENT ACOUSTIC MODELS.pdf:pdf},
title = {{LANGUAGE-INDEPENDENT ACOUSTIC MODELS}}
}
@article{Gauvain2004,
author = {Gauvain, JL and Messaoudi, A and Schwenk, H},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Gauvain, Messaoudi, Schwenk - 2004 - Language recognition using phone latt ices.pdf:pdf},
journal = {INTERSPEECH},
number = {2},
pages = {2--5},
title = {{Language recognition using phone latt ices}},
url = {ftp://m170.limsi.fr/public/TuA2301o.1{\_}p1283.pdf},
year = {2004}
}
@inproceedings{inproceedings:turkish_arel,
author = {Gedik, Ali C and Bozkurt, Baris},
booktitle = {Proceedings of the Conference on Interdisciplinary Musicology},
month = {jul},
title = {{Automatic Classification of Turkish Traditional Art Music Recordings by Arel Theory}},
year = {2008}
}
@inproceedings{Geng2016,
author = {Geng, Wang and Wang, Wenfu and Zhao, Yuanyuan and Cai, Xinyuan and Xu, Bo},
booktitle = {Interspeech},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Geng et al. - 2016 - End-to-end Language Identification using Attention-based Recurrent Neural Networks.pdf:pdf},
keywords = {[Electronic Manuscript]},
pages = {2944--2948},
title = {{End-to-end Language Identification using Attention-based Recurrent Neural Networks}},
year = {2016}
}
@techreport{d_case,
author = {Giannoulis, D and Benetos, E and Stowell, D and Rossignol, M and Lagrange, M and Plumbley, M},
institution = {IEEE AASP Challenge},
title = {{Detection and Classification of Acoustic Scenes and Events}},
year = {2012}
}
@article{article:birth_of_the_blues,
author = {Gibson, J M},
journal = {Reports on Progress in Physics},
number = {7},
title = {{The birth of the blues: How physics underlies music}},
volume = {72},
year = {2009}
}
@article{article:scales_biological_rationale,
author = {Gill, Kamraan Z and Purves, Dale},
journal = {PLoS ONE},
month = {dec},
number = {12},
title = {{A Biological Rationale for Musical Scales}},
volume = {4},
year = {2009}
}
@article{Glembek2011,
author = {Glembek, O and Burget, L},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Glembek, Burget - 2011 - Simplification and optimization of I-Vector extraction.pdf:pdf},
isbn = {9781457705397},
journal = {{\ldots} , Speech and Signal {\ldots}},
pages = {4516--4519},
title = {{Simplification and optimization of I-Vector extraction}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5947358},
year = {2011}
}
@phdthesis{phdthesis:tonal_description,
address = {Barcelona, Spain},
author = {Gomez, Emilia},
school = {Pompeu Fabra University},
title = {{Tonal Description of Music Audio Signals}},
year = {2006}
}
@inproceedings{inproceedings:music_and_geography,
author = {Gomez, Emilia and Haro, Martin and Herrera, Perfecto},
booktitle = {10th International Society for Music Information Retrieval Conference (ISMIR)},
title = {{Music and Geography: Content description of musical audio from different parts of the world}},
year = {2009}
}
@article{article:comparative_analysis,
author = {Gomez, Emilia and Herrera, Perfecto},
journal = {Empirical Musicology Review},
number = {3},
pages = {140--156},
title = {{Comparative Analysis of Music Recordings from Western and Non-Western traditions by Automatic Tonal Feature Extraction}},
volume = {3},
year = {2008}
}
@incollection{Goodfellow2016a,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
chapter = {20},
file = {:Users/anna/Downloads/DeepLearning Book/generative{\_}models.pdf:pdf},
title = {{Deep Generative Models}},
year = {2016}
}
@incollection{Goodfellow2016b,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
chapter = {12},
doi = {10.1063/1.4882025},
file = {:Users/anna/Downloads/DeepLearning Book/applications.pdf:pdf},
isbn = {1584882409},
title = {{Applications}},
volume = {234106},
year = {2016}
}
@incollection{Goodfellow2016c,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
chapter = {10},
file = {:Users/anna/Downloads/DeepLearning Book/rnn.pdf:pdf},
title = {{Sequence Modeling: Recurrent and recursive nets}},
volume = {113},
year = {2016}
}
@incollection{Goodfellow2016d,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
file = {:Users/anna/Downloads/DeepLearning Book/notation.pdf:pdf},
title = {{Notation}},
year = {2016}
}
@incollection{Goodfellow2016e,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
chapter = {5},
file = {:Users/anna/Downloads/DeepLearning Book/ml.pdf:pdf},
title = {{Machine Learning Basics}},
year = {2016}
}
@incollection{Goodfellow2016f,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
file = {:Users/anna/Downloads/DeepLearning Book/index-.pdf:pdf},
title = {{Index}},
year = {2016}
}
@incollection{Goodfellow2016g,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
chapter = {Par II},
file = {:Users/anna/Downloads/DeepLearning Book/part{\_}practical.pdf:pdf},
number = {09},
pages = {1--3},
title = {{Deep Networks: Modern Practices}},
year = {2016}
}
@book{Goodfellow2016,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
file = {:Users/anna/Thesis/Papers/DeepLearning Book/DeepLearning.pdf:pdf},
title = {{Deep Learning}},
year = {2016}
}
@incollection{Goodfellow2016h,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
chapter = {19},
file = {:Users/anna/Downloads/DeepLearning Book/inference.pdf:pdf},
pages = {631--653},
title = {{Approximate Inference}},
year = {2016}
}
@incollection{Goodfellow2016i,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
chapter = {8},
file = {:Users/anna/Downloads/DeepLearning Book/optimization.pdf:pdf},
pages = {274--329},
title = {{Optimization for Training Deep Models}},
year = {2016}
}
@incollection{Goodfellow2016j,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
chapter = {17},
file = {:Users/anna/Downloads/DeepLearning Book/monte{\_}carlo.pdf:pdf},
title = {{Monte Carlo Methods}},
year = {2016}
}
@incollection{Goodfellow2016k,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
chapter = {3},
file = {:Users/anna/Downloads/DeepLearning Book/prob.pdf:pdf},
pages = {1--27},
title = {{Probability and Information Theory}},
year = {2016}
}
@incollection{Goodfellow2016l,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
chapter = {15},
file = {:Users/anna/Downloads/DeepLearning Book/representation.pdf:pdf},
title = {{Representation learning}},
year = {2016}
}
@incollection{Goodfellow2016m,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
chapter = {Part I},
file = {:Users/anna/Downloads/DeepLearning Book/part{\_}basics.pdf:pdf},
pages = {29--30},
title = {{Applied Math and Machine Learning Basics}},
year = {2016}
}
@incollection{Goodfellow2016n,
archivePrefix = {arXiv},
arxivId = {1211.4246},
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
doi = {abs/1211.4246},
eprint = {1211.4246},
file = {:Users/anna/Downloads/DeepLearning Book/bib.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
pages = {3563--3593},
title = {{Bibilography}},
url = {http://arxiv.org/abs/1211.4246},
volume = {15},
year = {2016}
}
@incollection{Goodfellow2016o,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
chapter = {1},
file = {:Users/anna/Downloads/DeepLearning Book/intro.pdf:pdf},
pages = {1--28},
title = {{Introduction}},
year = {2016}
}
@incollection{Goodfellow2016p,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
file = {:Users/anna/Downloads/DeepLearning Book/optimization.html:html},
title = {optimization},
year = {2016}
}
@incollection{Goodfellow2016q,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
chapter = {9},
file = {:Users/anna/Downloads/DeepLearning Book/convnets.pdf:pdf},
pages = {331--373},
title = {{Convolutional Networks}},
year = {2016}
}
@incollection{Goodfellow2016r,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
chapter = {2},
doi = {10.1016/B978-0-12-391420-0.09987-X},
file = {:Users/anna/Downloads/DeepLearning Book/linear{\_}algebra.pdf:pdf},
isbn = {9780123914200},
pages = {31--52},
title = {{Linear Algebra}},
year = {2016}
}
@incollection{Goodfellow2016s,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
chapter = {18},
file = {:Users/anna/Downloads/DeepLearning Book/partition.pdf:pdf},
title = {{Confronting the partition function}},
year = {2016}
}
@incollection{Goodfellow2016t,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
chapter = {16},
file = {:Users/anna/Downloads/DeepLearning Book/graphical{\_}models.pdf:pdf},
pages = {558--589},
title = {{Structured Probabilistic Models for Deep Learning}},
year = {2016}
}
@incollection{Goodfellow2016u,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
chapter = {13},
file = {:Users/anna/Downloads/DeepLearning Book/linear{\_}factors.pdf:pdf},
pages = {489--501},
title = {{Linear Factor Models}},
volume = {13},
year = {2016}
}
@incollection{Goodfellow2016v,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
file = {:Users/anna/Downloads/DeepLearning Book/acknowledgements.pdf:pdf},
isbn = {0780376633},
pages = {680--683},
title = {{Acknowledgments}},
year = {2016}
}
@incollection{Goodfellow2016w,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
chapter = {4},
file = {:Users/anna/Downloads/DeepLearning Book/numerical.pdf:pdf},
pages = {76--91},
title = {{Numerical Computation}},
volume = {d},
year = {2016}
}
@incollection{Goodfellow2016x,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
chapter = {11},
file = {:Users/anna/Downloads/DeepLearning Book/guidelines.pdf:pdf},
pages = {424--445},
title = {{Practical Methodology}},
year = {2016}
}
@incollection{Goodfellow2016y,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
chapter = {14},
file = {:Users/anna/Downloads/DeepLearning Book/autoencoders.pdf:pdf},
pages = {504--527},
title = {{Autoencoders}},
year = {2016}
}
@incollection{Goodfellow2016z,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
file = {:Users/anna/Downloads/DeepLearning Book/TOC.pdf:pdf},
isbn = {9780321712714},
pages = {2--4},
title = {{Contents}},
year = {2016}
}
@incollection{Goodfellow2016ba,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Deep Learning},
chapter = {Part III},
file = {:Users/anna/Downloads/DeepLearning Book/part{\_}research.pdf:pdf},
pages = {489--491},
title = {{Deep Learning Research}},
year = {2016}
}
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.2661v1},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
eprint = {arXiv:1406.2661v1},
file = {:Users/anna/Work/Interspeech16/whitepaper/goodfellow{\_}adversarial{\_}14.pdf:pdf},
isbn = {1406.2661},
issn = {10495258},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {1--9},
title = {{Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1406.2661},
year = {2014}
}
@article{Gosztolya2011,
author = {Gosztolya, G and T{\'{o}}th, L},
file = {:Users/anna/Thesis/Papers/2011-sami.pdf:pdf},
isbn = {9781424474288},
journal = {{\ldots} and Informatics (SAMI), 2011 IEEE 9th {\ldots}},
keywords = {a list of hits,a speech,an std method returns,and a,e,each of which contain,i,in an audio database,signal index,starting and ending times,the point of occurrence,the set of recordings,the term found,to as,which will be referred},
number = {1},
title = {{Spoken term detection based on the most probable phoneme sequence}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5738856},
year = {2011}
}
@article{article:chorus_section,
author = {Goto, Masataka},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
month = {sep},
number = {5},
pages = {1783--1794},
title = {{A Chorus Section Detection Method for Musical Audio Signals and its Application to a Music Listening Station}},
volume = {14},
year = {2006}
}
@article{Graves2012,
abstract = {后边实在是太数学了，没看。。。RNN解决序列转化的问题，语音识别问题。与以前的RNN不同，经过改进无输入输出的限制问题。},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.3711v1},
author = {Graves, Alex},
eprint = {arXiv:1211.3711v1},
file = {:Users/anna/Work/Interspeech16/whitepaper/graves{\_}endtoend{\_}12.pdf:pdf},
isbn = {2000201075},
journal = {arXiv preprint arXiv:1211.3711},
title = {{Sequence transduction with recurrent neural networks}},
url = {http://arxiv.org/abs/1211.3711},
year = {2012}
}
@phdthesis{Graves2008,
author = {Graves, Alex},
file = {:Users/anna/Downloads/Supervised{\_}Sequence{\_}Labeling.pdf:pdf},
isbn = {2000201075},
title = {{Supervised Sequence Labelling with Recurrent Neural Networks}},
year = {2008}
}
@article{Graves2006,
abstract = {Many real-world sequence learning tasks re- quire the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their out- puts into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label un- segmented sequences directly, thereby solv- ing both problems. An experiment on the TIMIT speech corpus demonstrates its ad- vantages over both a baseline HMM and a hybrid HMM-RNN.},
author = {Graves, Alex and Fernandez, Santiago and Gomez, Faustino and Schmidhuber, J{\"{u}}rgen},
doi = {10.1145/1143844.1143891},
file = {:Users/anna/Work/Interspeech16/whitepaper/graves{\_}ctc{\_}06.pdf:pdf},
isbn = {1595933832},
issn = {10987576},
journal = {Proceedings of the 23rd international conference on Machine Learning},
pages = {369--376},
title = {{Connectionist Temporal Classification : Labelling Unsegmented Sequence Data with Recurrent Neural Networks}},
year = {2006}
}
@article{Graves2014,
abstract = {This paper presents a speech recognition system that directly transcribes audio data with text, without requiring an intermediate phonetic representation. The system is based on a combination of the deep bidirectional LSTM recurrent neural network architecture and the Connectionist Temporal Classification objective function. A modification to the objective function is introduced that trains the network to minimise the expectation of an arbitrary transcription loss function. This allows a direct optimisation of the word error rate, even in the absence of a lexicon or language model. The system achieves a word error rate of 27.3{\%} on the Wall Street Journal corpus with no prior linguistic information, 21.9{\%} with only a lexicon of allowedwords, and 8.2{\%} with a trigram language model. Combining the network with a baseline system further reduces the error rate to 6.7{\%}.},
archivePrefix = {arXiv},
arxivId = {1512.02595},
author = {Graves, Alex and Jaitly, Navdeep},
doi = {10.1145/1143844.1143891},
eprint = {1512.02595},
file = {:Users/anna/Downloads/graves14.pdf:pdf},
isbn = {9781479972913},
issn = {10987576},
journal = {JMLR Workshop and Conference Proceedings},
number = {1},
pages = {1764--1772},
pmid = {1000285842},
title = {{Towards End-To-End Speech Recognition with Recurrent Neural Networks}},
url = {http://jmlr.org/proceedings/papers/v32/graves14.pdf{\%}5Cnhttp://www.jmlr.org/proceedings/papers/v32/graves14.html},
volume = {32},
year = {2014}
}
@article{Graves2013a,
abstract = {Deep Bidirectional LSTM (DBLSTM) recurrent neural networks have recently been shown to give state-of-the-art performance on the TIMIT speech database. However, the results in that work relied on recurrent-neural-network-specific objective functions, which are difficult to integrate with existing large vocabulary speech recognition systems. This paper investigates the use of DBLSTM as an acoustic model in a standard neural network-HMM hybrid system. We find that a DBLSTM-HMM hybrid gives equally good results on TIMIT as the previous work. It also outperforms both GMM and deep network benchmarks on a subset of the Wall Street Journal corpus. However the improvement in word error rate over the deep network is modest, despite a great increase in framelevel accuracy. We conclude that the hybrid approach with DBLSTM appears to be well suited for tasks where acoustic modelling predominates. Further investigation needs to be conducted to understand how to better leverage the improvements in frame-level accuracy towards better word error rates.},
author = {Graves, Alex and Jaitly, Navdeep and Mohamed, Abdel Rahman},
doi = {10.1109/ASRU.2013.6707742},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Graves, Jaitly, Mohamed - 2013 - Hybrid speech recognition with Deep Bidirectional LSTM.pdf:pdf},
isbn = {9781479927562},
journal = {2013 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2013 - Proceedings},
keywords = {DBLSTM,HMM-RNN hybrid},
pages = {273--278},
title = {{Hybrid speech recognition with Deep Bidirectional LSTM}},
year = {2013}
}
@article{Graves2013,
abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates $\backslash$emph{\{}deep recurrent neural networks{\}}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7{\%} on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
archivePrefix = {arXiv},
arxivId = {arXiv:1303.5778v1},
author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
doi = {10.1109/ICASSP.2013.6638947},
eprint = {arXiv:1303.5778v1},
file = {:Users/anna/Work/Interspeech16/whitepaper/graves{\_}endtoend{\_}13.pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {1520-6149},
journal = {Icassp},
number = {3},
pages = {6645--6649},
title = {{Speech Recognition With Deep Recurrent Neural Networks}},
year = {2013}
}
@article{Graves2004,
author = {Graves, Alex and Schmidhuber, J{\"{u}}rgen},
doi = {10.1016/j.neunet.2005.06.042},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Graves, Schmidhuber - 2004 - Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures.pdf:pdf},
issn = {08936080},
title = {{Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures}},
year = {2004}
}
@article{Grezl2007,
abstract = {In recent years, probabilistic features became an integral part of state-of-the-are LVCSR systems. In this work, we are exploring the possibility of obtaining the features directly from neural net without the necessity of converting output probabilities to features suitable for subsequent GMM-HMM system. We experimented with 5-layer MLP with bottle-neck in the middle layer. After training such a neural net, we used outputs of the bottle-neck as features for GMM-HMM recognition system. The benefits are twofold: first, improvement was gained when these features are used instead of the probabilistic features, second, the size of the system was reduced, as only part of the neural net is used. The experiments were performed on meetings recognition task defined in MST RT'05 evaluation},
author = {Gr{\'{e}}zl, Franti{\v{s}}ek and Karafi{\'{a}}t, Martin and Kont{\'{a}}r, Stanislav and {\v{C}}ernock{\'{y}}, Jan},
doi = {10.1109/ICASSP.2007.367023},
file = {:Users/anna/Downloads/204b78c4d85e9d59496fcc067d62f6f4a7e1.pdf:pdf},
isbn = {1424407281},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Bottle-neck features,LVCSR,Meeting recognition,Probabilistic features,TRAP-based features},
title = {{Probabilistic and bottle-neck features for LVCSR of meetings}},
volume = {4},
year = {2007}
}
@article{Gruhne2007,
author = {Gruhne, Matthias and Schmidt, Konstantin and Dittmar, Christian},
file = {:Users/anna/Work/Papers/ismir15/Phonemes{\_}Singing/0c9605282146adf013000000.pdf:pdf},
journal = {ISMIR},
title = {{Phoneme recognition in popular music}},
year = {2007}
}
@article{Gruhne2005,
author = {Gruhne, Matthias and Schmidt, Konstantin and Dittmar, Christian},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Gruhne, Schmidt, Dittmar - 2005 - Phoneme Detection in Popular Music.pdf:pdf},
journal = {ismir2007.ismir.net},
pages = {2004},
title = {{Phoneme Detection in Popular Music}},
url = {http://ismir2007.ismir.net/posters/ISMIR2007{\_}p369{\_}gruhne{\_}poster.pdf},
year = {2005}
}
@article{Gruhne2007a,
author = {Gruhne, Matthias and Schmidt, Konstantin and Dittmar, Christian},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Gruhne, Schmidt, Dittmar - 2007 - Detecting phonemes within the singing of polyphonic music.pdf:pdf},
journal = {Proceedings of ICoMCS {\ldots}},
number = {December},
pages = {60--63},
title = {{Detecting phonemes within the singing of polyphonic music}},
url = {http://www.marcs.uws.edu.au/links/ICoMusic/ArchiveCD/Full{\_}Paper{\_}PDF/Gruhne{\_}Schmidt{\_}Dittmar.pdf},
year = {2007}
}
@article{Gu1991,
author = {Gu, Hung-yan and Tseng, Chiu-yu and Lee, Lin-shan},
doi = {10.1109/78.91145},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Gu, Tseng, Lee - 1991 - Isolated-utterance speech recognition using hidden Markov models with bounded state durations.pdf:pdf},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
number = {8},
pages = {1743--1752},
title = {{Isolated-utterance speech recognition using hidden Markov models with bounded state durations}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=91145},
volume = {39},
year = {1991}
}
@article{Hannun2014,
abstract = {We present a state-of-the-art speech recognition system developed using end-toend deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a “phoneme.” Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0{\%} error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.},
archivePrefix = {arXiv},
arxivId = {1412.5567v2},
author = {Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and Ng, Andrew Y.},
doi = {arXiv:1412.5567v2},
eprint = {1412.5567v2},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Hannun et al. - 2014 - Deep Speech Scaling up end-to-end speech recognition.pdf:pdf},
journal = {Arxiv},
pages = {1--12},
title = {{Deep Speech: Scaling up end-to-end speech recognition}},
year = {2014}
}
@inproceedings{jens,
address = {Copenhagen, Denmark},
author = {Hansen, J K},
booktitle = {9th Sound and Music Computing Conference (SMC)},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Hansen - 2012 - Recognition of Phonemes in A-cappella Recordings using Temporal Patterns and Mel Frequency Cepstral Coefficients.pdf:pdf},
pages = {494--499},
title = {{Recognition of Phonemes in A-cappella Recordings using Temporal Patterns and Mel Frequency Cepstral Coefficients}},
year = {2012}
}
@inproceedings{inproceedings:low_level_song_level,
author = {Haro, Martin and Herrera, Perfecto},
booktitle = {10th International Society for Music Information Retrieval Conference (ISMIR)},
title = {{From Low-Level to Song-level percussion descriptors of polyphonic music}},
year = {2009}
}
@article{Hauptmann2006,
author = {Hauptmann, Alexander},
file = {:Users/anna/Downloads/Automatic Spoken Document Retrieval.pdf:pdf},
keywords = {audio analysis,information retrieval,speech recognition applications,speech retrieval,spoken document search and retrieval},
title = {{Automatic Spoken Document Retrieval}},
year = {2006}
}
@article{Hazen2006,
abstract = {In this paper we examine the issues of aligning and correcting approximate human generated transcripts for long audio ﬁles. Accurate time-aligned transcriptions help provide easier access to audio materials by aiding downstream applications such as the indexing, summarizing and retrieving of audio segments. Accurate time alignments are also necessary when incorporating audio data into the training data for a speech recognizer's acoustic model. We provide some initial analysis of manual transcriptions which show that there can be signiﬁ- cant differences between the “approximate” manual transcripts generated by typical commercial transcription services and what was actually spoken in the recording. We then present a new alignment approach for approximate transcriptions of long audio ﬁles which is designed to discover and correct errors in the manual transcription during the alignment process.},
author = {Hazen, Tj},
doi = {10.1.1.154.5104},
file = {:Users/anna/Thesis/Papers/IS061258.pdf:pdf},
isbn = {9781604234497},
journal = {Interspeech},
keywords = {[Electronic Manuscript]},
pages = {1606--1609},
title = {{Automatic alignment and error correction of human generated transcripts for long speech recordings.}},
url = {http://groups.csail.mit.edu/sls/publications/2006/IS061258.pdf},
year = {2006}
}
@article{Hazen2009,
author = {Hazen, TJ and Shen, Wade and White, Christopher},
file = {:Users/anna/Thesis/Papers/2009{\_}12{\_}13{\_}Hazen{\_}ASRU{\_}FP.pdf:pdf},
journal = {Automatic Speech Recognition {\&} {\ldots}},
pages = {1--6},
title = {{Query-by-example spoken term detection using phonetic posteriorgram templates}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5372889},
year = {2009}
}
@article{Hazen1997,
author = {Hazen, TJ and Zue, VW},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Hazen, Zue - 1997 - Segment-based automatic language identification.pdf:pdf},
journal = {The Journal of the Acoustical Society of America},
number = {November 1996},
pages = {2323--2331},
title = {{Segment-based automatic language identification}},
url = {http://link.aip.org/link/?JASMAN/101/2323/1},
volume = {101},
year = {1997}
}
@article{Haznedaroglu2007,
author = {Haznedaroğlu, A},
file = {:Users/anna/Thesis/Papers/04298570.pdf:pdf},
journal = {Signal Processing and {\ldots}},
keywords = {language model,spotting using keyword adapted},
title = {{Keyword spotting using keyword adapted language model}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4298570},
year = {2007}
}
@article{He2008,
abstract = {In this article, we studied the objective functions of MMI, MCE, and MPE/MWE for discriminative learning in sequential pattern recognition. We presented an approach that unifies the objective functions of MMI, MCE, and MPE/MWE in a common rational-function form of (25). The exact structure of the rational-function form for each discriminative criterion was derived and studied. While the rational-function form of MMI has been known in the past, we provided the theoretical proof that the similar rational-function form exists for the objective functions of MCE and MPE/MWE. Moreover, we showed that the rational function forms for objective functions of MMI, MCE, and MPE/MWE differ in the constant weighting factors CDT (s1 . . . sR) and these weighting factors depend only on the labeled sequence s1 . . . sR, and are independent of the parameter set - to be optimized. The derived rational-function form for MMI, MCE, and MPE/MWE allows the GT/EBW-based parameter optimization framework to be applied directly in discriminative learning. In the past, lack of the appropriate rational-function form was a difficulty for MCE and MPE/MWE, because without this form, the GT/EBW-based parameter optimization framework cannot be directly applied. Based on the unified rational-function form, in a tutorial style, we derived the GT/EBW-based parameter optimization formulas for both discrete HMMs and CDHMMs in discriminative learning using MMI, MCE, and MPE/MWE criteria. The unifying review provided in this article has been based upon a large number of earlier contributions that have been cited and discussed throughout the article. Here we provide a brief summary of such background work. Extension to large-scale speech recognition tasks was accomplished in the work of [59] and [60]. The dissertation of [47] further improved the MMI criterion to that of MPE/MWE. In a parallel vein, the work of [20] provided an alternative approach to that of [41], with an attempt to more rigorously provide - - a CDHMM model re-estimation formula that gives positive growth of the MMI objective function. A crucial error of this attempt was corrected in [2] for establishing an existence proof of such positive growth. The main goal of this article is to provide an underlying foundation for MMI, MCE, and MPE/MWE at the objective function level to facilitate the development of new parameter optimization techniques and to incorporate other pattern recognition concepts, e.g., discriminative margins [66], into the current discriminative learning paradigm.},
author = {He, Xiaodong and Deng, Li and Chou, Wu},
doi = {10.1109/MSP.2008.926652},
file = {:Users/anna/Work/Interspeech16/whitepaper/he{\_}sequential{\_}08.pdf:pdf},
isbn = {1053-5888},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
number = {September},
pages = {14--36},
title = {{Discriminative Learning in Sequential Pattern Recognition}},
volume = {14},
year = {2008}
}
@book{book:helmholtz,
author = {Helmholtz, Hermann},
publisher = {Dover},
title = {{On the Sensations of Tone}},
year = {1954}
}
@techreport{rasta_plp,
author = {Hermansky, H and Morgan, N and Bayya, A and Kohn, P},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Hermansky et al. - 1991 - {\{}RASTA-PLP{\}} Speech Analysis(2).pdf:pdf},
institution = {ICSI},
number = {TR-91-069},
title = {{{\{}RASTA-PLP{\}} Speech Analysis}},
year = {1991}
}
@inproceedings{traps1,
address = {Sydney, Australia},
author = {Hermansky, H and Sharma, S},
booktitle = {Proceedings of the 5th International Conference on Spoken Language Processing (ICSLP)},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Hermansky, Sharma - 1998 - Traps -- Classifiers Of Temporal Patterns.pdf:pdf},
pages = {1003--1006},
title = {{Traps -- Classifiers Of Temporal Patterns}},
year = {1998}
}
@inproceedings{traps2,
address = {Phoenix, AZ, USA},
author = {Hermansky, H and Sharma, S},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Hermansky, Sharma - 1999 - Temporal patterns (TRAPS) in ASR of noisy speech.pdf:pdf},
pages = {289--292},
title = {{Temporal patterns (TRAPS) in ASR of noisy speech}},
year = {1999}
}
@article{Hermansky1990,
author = {Hermansky, Hynek},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Hermansky - 1990 - Perceptual linear predictive (PLP) analysis of speech(2).pdf:pdf},
journal = {the Journal of the Acoustical Society of America},
number = {4},
pages = {1738--1752},
title = {{Perceptual linear predictive (PLP) analysis of speech}},
url = {http://scitation.aip.org/content/asa/journal/jasa/87/4/10.1121/1.399423},
volume = {87},
year = {1990}
}
@article{Hermansky2000,
abstract = {Hidden Markov model speech recognition systems typically use Gaussian mixture models to estimate the distributions of decorrelated acoustic feature vectors that correspond to individual subword units. By contrast, hybrid connectionist-HMM systems use discriminatively-trained neural networks to estimate the probability distribution among subword units given the acoustic observations. In this work we show a large improvement in word recognition performance by combining neural-net discriminative feature processing with Gaussian-mixture distribution modeling. By training the network to generate the subword probability posteriors, then using transformations of these estimates as the base features for a conventionally-trained Gaussian-mixture based system, we achieve relative error rate reductions of 35{\%} or more on the multicondition Aurora noisy continuous digits task},
author = {Hermansky, Hynek and Ellis, Daniel P.W. and Sharma, Sangita},
doi = {10.1109/ICASSP.2000.862024},
file = {:Users/anna/Downloads/icassp00-nnhmm.pdf:pdf},
isbn = {0-7803-6293-4},
issn = {1520-6149},
journal = {2000 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.00CH37100)},
pages = {1635--1638},
title = {{Tandem connectionist feature extraction for conventional HMM systems}},
volume = {3},
year = {2000}
}
@article{Hieronymus1997,
author = {Hieronymus, J and Kadambe, S},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Hieronymus, Kadambe - 1997 - Robust spoken language identification using large vocabulary speech recognition.pdf:pdf},
isbn = {0818679190},
journal = {{\ldots} , Speech, and Signal Processing, {\ldots}},
pages = {1111--1114},
title = {{Robust spoken language identification using large vocabulary speech recognition}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=596136},
year = {1997}
}
@article{Hinton2006,
archivePrefix = {arXiv},
arxivId = {20},
author = {Hinton, Geoffrey and Salakhutdinov, Ruslan},
doi = {10.1126/science.1127647},
eprint = {20},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Hinton, Salakhutdinov - 2006 - Reducing the dimensionality of data with neural networks.pdf:pdf},
isbn = {3135786504},
issn = {0036-8075},
journal = {Science},
number = {July},
pages = {504--507},
pmid = {16873662},
title = {{Reducing the dimensionality of data with neural networks}},
volume = {313},
year = {2006}
}
@article{Hinton2015,
abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
archivePrefix = {arXiv},
arxivId = {1503.02531},
author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
doi = {10.1063/1.4931082},
eprint = {1503.02531},
file = {:Users/anna/Work/Interspeech16/whitepaper/hinton{\_}distillation{\_}15.pdf:pdf},
issn = {0022-2488},
journal = {NIPS 2014 Deep Learning Workshop},
pages = {1--9},
title = {{Distilling the Knowledge in a Neural Network}},
url = {http://arxiv.org/abs/1503.02531},
year = {2015}
}
@article{Hochreiter1998,
abstract = {Recurrent nets are in principle capable to store past inputs to produce the currently desired output. Because of this property recurrent nets are used in time series prediction and process control. Practical applications involve temporal dependencies spanning many time steps, e.g. between relevant inputs and desired outputs. In this case, however, gradient based learning methods take too much time. The extremely increased learning time arises because the error vanishes as it gets propagated back. In this article the de-caying error flow is theoretically analyzed. Then methods trying to overcome vanishing gradients are briefly discussed. Finally, experiments comparing conventional algorithms and alternative methods are presented. With advanced methods long time lag problems can be solved in reasonable time.},
author = {Hochreiter, Sepp},
doi = {10.1142/S0218488598000094},
file = {:Users/anna/Work/Interspeech16/whitepaper/hochreiter{\_}gradient{\_}98.pdf:pdf},
isbn = {0218-4885},
issn = {0218-4885},
journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
keywords = {long,long-term dependencies,recurrent neural nets,vanishing gradient},
number = {02},
pages = {107--116},
title = {{The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions}},
volume = {06},
year = {1998}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
eprint = {1206.2944},
file = {:Users/anna/Work/Interspeech16/whitepaper/Hochreiter97{\_}lstm.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Learning,Memory,Models,Nerve Net,Nerve Net: physiology,Neural Networks (Computer),Neurological,Psychological,Short-Term,Time Factors},
number = {8},
pages = {1735--80},
pmid = {9377276},
title = {{Long short-term memory.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/9377276},
volume = {9},
year = {1997}
}
@article{Hollien2000,
author = {Hollien, Harry and Mendes-Schwartz, Ana P. and Nielsen, Kenneth},
doi = {10.1016/S0892-1997(00)80038-7},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Hollien, Mendes-Schwartz, Nielsen - 2000 - Perceptual confusions of high-pitched sung vowels.pdf:pdf},
issn = {08921997},
journal = {Journal of Voice},
month = {jun},
number = {2},
pages = {287--298},
title = {{Perceptual confusions of high-pitched sung vowels}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0892199700800387},
volume = {14},
year = {2000}
}
@misc{Holmes1999,
author = {Holmes, W.J. and Russell, M.J.},
booktitle = {Computer Speech {\&} Language},
doi = {10.1006/csla.1998.0048},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Holmes, Russell - 1999 - Probabilistic-trajectory segmental HMMs.pdf:pdf},
issn = {08852308},
title = {{Probabilistic-trajectory segmental HMMs}},
year = {1999}
}
@inproceedings{inproceedings:rhythmic_turkish,
author = {Holzapfel, Andre and Stylianou, Yannis},
booktitle = {Proc. of the Intl. Conf. on Music Information Retrieval (ISMIR)},
title = {{Rhythmic Similarity in traditional Turkish music}},
year = {2009}
}
@book{Horak2010,
author = {Horak, A and Kopecek, I and Pala, K},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Horak, Kopecek, Pala - 2010 - Text, Speech and Dialogue.pdf:pdf},
isbn = {9783642327896},
title = {{Text, Speech and Dialogue}},
url = {http://link.springer.com/content/pdf/10.1007/978-3-642-32790-2.pdf},
year = {2010}
}
@book{Horak2010a,
author = {Horak, A and Kopecek, I and Pala, K},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Horak, Kopecek, Pala - 2010 - Text, Speech and Dialogue(2).pdf:pdf},
isbn = {9783540287896},
title = {{Text, Speech and Dialogue}},
url = {http://link.springer.com/content/pdf/10.1007/978-3-642-32790-2.pdf},
year = {2010}
}
@inproceedings{Hori2016,
author = {Hori, Chiori and Hori, Takaaki and Watanabe, Shinji and Hershey, John R},
booktitle = {Interspeech},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Hori et al. - 2016 - Context-Sensitive and Role-Dependent Spoken Language Understanding using Bidirectional and Attention LSTMs.pdf:pdf},
keywords = {[Electronic Manuscript]},
number = {1},
pages = {3236--3240},
title = {{Context-Sensitive and Role-Dependent Spoken Language Understanding using Bidirectional and Attention LSTMs}},
year = {2016}
}
@phdthesis{Hosford2011,
author = {Hosford, Alexander},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Hosford - 2011 - Automatic Language identification (LID) through machine learning.pdf:pdf},
title = {{Automatic Language identification (LID) through machine learning}},
volume = {12},
year = {2011}
}
@article{Hosom2002,
author = {Hosom, John-paul},
file = {:Users/anna/Thesis/Papers/925868eef1f0130dc9a6bc0fbd30a6afebaf.pdf:pdf},
journal = {Seventh International Conference on Spoken},
number = {1},
pages = {357--360},
title = {{Automatic phoneme alignment based on acoustic-phonetic modeling}},
url = {http://www.isca-speech.org/archive/icslp{\_}2002/i02{\_}0357.html},
year = {2002}
}
@article{Hosoya2005,
author = {Hosoya, T. and Suzuki, M. and Ito, a. and Makino, S.},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Hosoya et al. - 2005 - Lyrics recognition from a singing voice based on finite state automaton for music information retrieval.pdf:pdf},
isbn = {0-9551179-0-9},
journal = {Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)},
keywords = {figure 1 shows,fsa,information in the user,ing to develop a,into consideration,lyrics recognition,melody and lyrics,mir,mir system that uses,s singing voice,voice are not taken,we are attempt-},
pages = {532--535},
title = {{Lyrics recognition from a singing voice based on finite state automaton for music information retrieval}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.59.7615{\&}rep=rep1{\&}type=pdf},
year = {2005}
}
@techreport{techreport:practical_svm,
author = {Hsu, Chih-Wei and Chang, Chih-Chung and Lin, Chih-Jen},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Hsu, Chang, Lin - 2010 - A Practical Guide to Support Vector Classification.pdf:pdf},
institution = {Department of Computer Science and Information Engineering, National Taiwan University, Taipei 106, Taiwan},
title = {{A Practical Guide to Support Vector Classification}},
year = {2010}
}
@inproceedings{Huang2016,
author = {Huang, Che-wei and Narayanan, Shrikanth S Shri},
booktitle = {Interspeech},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Huang, Narayanan - 2016 - Attention Assisted Discovery of Sub-Utterance Structure in Speech Emotion Recognition.pdf:pdf},
keywords = {[Electronic Manuscript]},
pages = {1387--1391},
title = {{Attention Assisted Discovery of Sub-Utterance Structure in Speech Emotion Recognition}},
year = {2016}
}
@inproceedings{jhu_lid1,
author = {Huang, Shuai and Karakos, Damianos and Church, Kenneth and Coppersmith, Glen},
booktitle = {IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Huang et al. - 2011 - Bootstrapping a Spoken Language Identification System Using Unsupervised Integrated Sensing and Processing Decisio.pdf:pdf},
title = {{Bootstrapping a Spoken Language Identification System Using Unsupervised Integrated Sensing and Processing Decision Trees}},
year = {2011}
}
@inproceedings{Huang2016a,
author = {Huang, Yan and Wang, Yongqiang and Gong, Yifan},
booktitle = {Interspeech},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Huang et al. - 2016 - Semi-Supervised Training in Deep Learning Acoustic Model.pdf:pdf},
keywords = {[Electronic Manuscript]},
pages = {3848--3852},
title = {{Semi-Supervised Training in Deep Learning Acoustic Model}},
year = {2016}
}
@article{HuiLin2009,
author = {{Hui Lin} and {Li Deng} and {Dong Yu} and {Yi-fan Gong} and {Alex Acero} and {Chin-Hui Lee}},
doi = {10.1109/ICASSP.2009.4960588},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Hui Lin et al. - 2009 - A study on multilingual acoustic modeling for large vocabulary ASR.pdf:pdf},
isbn = {978-1-4244-2353-8},
journal = {2009 IEEE International Conference on Acoustics, Speech and Signal Processing},
month = {apr},
pages = {4333--4336},
publisher = {Ieee},
title = {{A study on multilingual acoustic modeling for large vocabulary ASR}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4960588},
year = {2009}
}
@inproceedings{inproceedings:ras,
author = {Humphrey, Eric},
booktitle = {11th International Society for Music Information Retrieval Conference (ISMIR)},
title = {{Automatic Characterization of Digital Music for Rhythmic Auditory Stimulation}},
year = {2010}
}
@book{book:clash_of_civilizations,
author = {Huntington, Samuel P},
publisher = {Simon {\&} Schuster},
title = {{The Clash of Civilizations and the Remaking of World Order}},
year = {1996}
}
@article{Imseng2010,
author = {Imseng, David and Magimai-Doss, M and Bourlard, H},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Imseng, Magimai-Doss, Bourlard - 2010 - Hierarchical multilayer perceptron based language identification.pdf:pdf},
journal = {INTERSPEECH},
number = {Ii},
pages = {1--9},
title = {{Hierarchical multilayer perceptron based language identification.}},
url = {http://www.idiap.ch/{~}dimseng/Idiap{\_}IIR{\_}104-2010.pdf},
year = {2010}
}
@inproceedings{Irie2016,
author = {Irie, Kazuki and Alkhouli, Tamer and Schl{\"{u}}ter, Ralf and Ney, Hermann},
booktitle = {Interspeech},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Irie et al. - 2016 - LSTM , GRU , Highway and a Bit of Attention An Empirical Overview for Language Modeling in Speech Recognition.pdf:pdf},
pages = {3519--3523},
title = {{LSTM , GRU , Highway and a Bit of Attention : An Empirical Overview for Language Modeling in Speech Recognition}},
year = {2016}
}
@article{Jackson2003,
author = {Jackson, PJB and Russell, MJ},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Jackson, Russell - 2003 - Improvements in phone-classification accuracy from modelling duration.pdf:pdf},
journal = {ICPhS},
keywords = {1970s,2,3,4,5,duration modelling,ferguson introduced a discrete,for purposes of asr,in the 1980s,ments in methods of,modelling duration,principally,segmental hmms,there were develop-},
pages = {1--4},
title = {{Improvements in phone-classification accuracy from modelling duration}},
url = {http://personal.ee.surrey.ac.uk/Personal/P.Jackson/pub/icphs03/Jac03.pdf},
year = {2003}
}
@article{Jaitly2012,
abstract = {The use of Deep Belief Networks (DBN) to pretrain Neural Networks has recently led to a resurgence in the use of Artiﬁcial Neural Network - Hidden Markov Model (ANN/HMM) hybrid systems for Automatic Speech Recognition (ASR). In this paper we report results of a DBN-pretrained context-dependent ANN/HMM system trained on two datasets that are much larger than any reported previously with DBN-pretrained ANN/HMM systems - 5870 hours of Voice Search and 1400 hours of YouTube data. On the ﬁrst dataset, the pretrained ANN/HMM system outperforms the best Gaussian Mixture Model - Hidden Markov Model (GMM/HMM) baseline, built with a much larger dataset by 3.7{\%} absolute WER, while on the second dataset, it outperforms the GMM/HMM baseline by 4.7{\%} absolute. Maximum Mutual Information (MMI) ﬁne tuning and model combination using Segmental Conditional Random Fields (SCARF) give additional gains of 0.1{\%} and 0.4{\%} on the ﬁrst dataset and 0.5{\%} and 0.9{\%} absolute on the second dataset.},
author = {Jaitly, Navdeep and Nguyen, Patrick and Senior, Andrew and Vanhoucke, Vincent},
file = {:Users/anna/Thesis/Papers/jaitly-interspeech12.pdf:pdf},
journal = {13rh Annual Conference of the International Speech Communication Association},
number = {Cd},
pages = {2--5},
title = {{Application of Pretrained Deep Neural Networks to Large Vocabulary Speech Recognition.}},
url = {http://20.210-193-52.unknown.qala.com.sg/archive/archive{\_}papers/interspeech{\_}2012/i12{\_}2578.pdf},
year = {2012}
}
@article{Jang,
author = {Jang, PJ and Hauptmann, AG},
file = {:Users/anna/Thesis/Papers/icslp98.ps.gz.ps.pdf:pdf},
pages = {1--4},
title = {{Hierarchical Cluster Language Modeling For Rescoring N-Best Hypotheses During Speech Decoding}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Hierarchical+Cluster+Language+Modeling+For+Rescoring+N-Best+Hypotheses+During+Speech+Decoding{\#}0}
}
@article{Jansen2011,
author = {Jansen, A},
file = {:Users/anna/Thesis/Papers/ICASSP2011a.pdf:pdf},
journal = {Acoustics, Speech and Signal Processing (ICASSP), {\ldots}},
pages = {2--5},
title = {{Whole word discriminative point process models}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5947524},
year = {2011}
}
@techreport{Jansen2009,
author = {Jansen, A and Niyogi, P},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Jansen, Niyogi - 2009 - An experimental evaluation of keyword-filler Hidden Markov Models.pdf:pdf},
keywords = {-filler hidden markov models,1989,1990,al,and a separate hidden,and rose and paul,back nearly two,basic idea is to,create one hidden markov,decades to the seminal,hidden markov model dates,model of the keyword,papers of rohlicek et,the,the current state-of-the-art keyword-filler},
pages = {1--10},
title = {{An experimental evaluation of keyword-filler Hidden Markov Models}},
url = {http://www.cs.uchicago.edu/files/tr{\_}authentic/TR-2009-02.pdf},
year = {2009}
}
@inproceedings{jhu_ks1,
author = {Jansen, Aren and Niyogi, P},
booktitle = {Proc. of Interspeech},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Jansen, Niyogi - 2009 - Robust Keyword Spotting with Rapidly Adapting Point Process Models.pdf:pdf},
title = {{Robust Keyword Spotting with Rapidly Adapting Point Process Models}},
year = {2009}
}
@article{Jeon2012,
author = {Jeon, ECHBJ and Park, G and Lee, YK},
file = {:Users/anna/Thesis/Papers/C12-2022.pdf:pdf},
journal = {24th International Conference on {\ldots}},
number = {December},
pages = {217--224},
title = {{Lattice Rescoring for Speech Recognition Using Large Scale Distributed Language Models}},
url = {http://www.kde.cs.tut.ac.jp/{~}aono/pdf/COLING2012/POSTERS/pdf/POSTERS022.pdf},
year = {2012}
}
@book{Jornadas2006,
author = {Jornadas, I V},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Jornadas - 2006 - Tecnolog ´ ıa del Habla.pdf:pdf},
isbn = {8496214826},
title = {{Tecnolog ´ ıa del Habla}},
year = {2006}
}
@book{book:wohltemperierte_gehirn,
author = {Jourdain, Robert},
publisher = {Spektrum Akademischer Verlag},
title = {{Das wohltemperierte Gehirn. Wie Musik im Kopf entsteht und wirkt}},
year = {2001}
}
@article{Jozefowicz2015,
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
doi = {10.1109/CVPR.2015.7298761},
eprint = {1512.03385},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Jozefowicz, Zaremba, Sutskever - 2015 - An empirical exploration of recurrent network architectures.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {ICML},
pmid = {18267787},
title = {{An empirical exploration of recurrent network architectures}},
year = {2015}
}
@inproceedings{Juang1985,
abstract = {In this paper we extend previous work on isolated word recognition based on hidden Markov models by replacing the discrete symbol representation of the speech signal by a continuous Gaussian mixture density. In this manner the inherent quantization error introduced by the discrete representation is essentially eliminated. The resulting recogniier was tested on a vocabulary of the 10 digits across a wide range of talkers and test conditions, and shown to have an error rate at least comparable to that of the best template reeognizers and significantly lower than that of the discrete symbol hidden Markov model system. Several issues involved in the training of the continuous density models and in the implementation of the recognizer are discussed.},
author = {Juang, B H and Rabjner, L R and Levinson, S E and Sondhj, M M},
booktitle = {ICASSP},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Juang et al. - 1985 - Recent Developments in the Application of Hidden Markov Models to Speaker-Independent Isolated Word Recognition(2).pdf:pdf},
pages = {9--11},
title = {{Recent Developments in the Application of Hidden Markov Models to Speaker-Independent Isolated Word Recognition}},
year = {1985}
}
@article{Junkawitsch1996,
author = {Junkawitsch, J and Neubauer, L},
file = {:Users/anna/Thesis/Papers/96jun.ps:ps},
journal = {{\ldots} , 1996. ICSLP 96. {\ldots}},
title = {{A new keyword spotting algorithm with pre-calculated optimal thresholds}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=607208},
year = {1996}
}
@techreport{Jurafsky1995,
author = {Jurafsky, Dan and {The ICSI Speech, Dining}, Dancing and Salsa Society},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Jurafsky, The ICSI Speech, Dining - 1995 - The beginner's guide to the ICSI speech software.pdf:pdf},
pages = {1--27},
title = {{The beginner's guide to the ICSI speech software}},
year = {1995}
}
@article{Kalantari2014,
author = {Kalantari, S and Dean, D and Sridharan, S},
file = {:Users/anna/Thesis/Papers/1569926121.pdf:pdf},
title = {{Topic dependent language modelling for spoken term detection}},
url = {http://eprints.qut.edu.au/75760/},
year = {2014}
}
@article{Kalantari2014a,
author = {Kalantari, S and Dean, D and Sridharan, S},
file = {:Users/anna/Thesis/Papers/Eusipco.pdf:pdf},
number = {September},
pages = {1--5},
title = {{Topic dependent language modelling for spoken term detection}},
url = {http://eprints.qut.edu.au/75760/},
year = {2014}
}
@article{Kamp1971,
author = {Kamp, LJT Van Der and Pols, LCW},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Kamp, Pols - 1971 - Perceptual analysis from confusions between vowels.pdf:pdf},
journal = {Acta Psychologica},
title = {{Perceptual analysis from confusions between vowels}},
url = {http://www.sciencedirect.com/science/article/pii/0001691871900321},
year = {1971}
}
@inproceedings{Kanda2016,
author = {Kanda, Naoyuki and Lu, Xugang and Kawai, Hisashi},
booktitle = {Interspeech},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Kanda, Lu, Kawai - 2016 - Maximum A Posteriori based Decoding for CTC Acoustic Models.pdf:pdf},
keywords = {[Electronic Manuscript]},
pages = {1868--1872},
title = {{Maximum A Posteriori based Decoding for CTC Acoustic Models}},
year = {2016}
}
@article{Katsamanis2011,
abstract = {Long speech-text alignment can facilitate large-scale study of rich spoken language resources that have recently become widely accessible, e.g., collections of audio books, or multime- dia documents. For such resources, the conventional Viterbi- based forced alignment may often be proven inadequate mainly due to mismatched audio and text and/or noisy audio. In this paper, we present SailAlign which is an open-source software toolkit for robust long speech-text alignment that circumvents these restrictions. It implements an adaptive, iterative speech recognition and text alignment scheme that allows for the pro- cessing of very long (and possibly noisy) audio and is robust to transcription errors. SailAlign is evaluated on artificially cre- ated long chunks of the TIMIT database. Audio is artificially contaminated with babble noise, and the corresponding tran- scriptions are corrupted at various levels. We present the corre- sponding word boundary detection results. Finally, we demon- strate the potential use of the software for the exploitation of audio books for the study of read speech.},
author = {Katsamanis, Athanasios and Black, Matthew and Georgiou, Panayiotis G and Goldstein, Louis and Narayanan, Shrikanth S},
file = {:Users/anna/Thesis/Papers/KatsamanisEtAl{\_}SailAlign{\_}VLSRP2011.pdf:pdf},
journal = {Proc of Workshop on New Tools and Methods for VeryLarge Scale Phonetics Research},
keywords = {Index Terms,adaptation,audio-books,imperfect transcriptions,open-source,software,speech-text alignment},
title = {{SailAlign: Robust long speech-text alignment}},
year = {2011}
}
@article{Kenny,
author = {Kenny, Patrick and Ouellet, Pierre and Dehak, Najim and Gupta, Vishwa and Dumouchel, Pierre},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Kenny et al. - Unknown - A Study of Inter-Speaker Variability in Speaker Verification.pdf:pdf},
number = {2},
pages = {1--9},
title = {{A Study of Inter-Speaker Variability in Speaker Verification}}
}
@article{Keshet2009,
annote = {Good overview over KWS development},
author = {Keshet, Joseph and Grangier, David and Bengio, Samy},
doi = {10.1016/j.specom.2008.10.002},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Keshet, Grangier, Bengio - 2009 - Discriminative keyword spotting.pdf:pdf},
issn = {01676393},
journal = {Speech Communication},
keywords = {a high area under,algorithm,and empirically that the,curve,higher confidence than any,ing method resulted with,is spoken would have,it is shown theoretically,other spoken utterance in,proposed train-,roc,spoken,the keyword is not,the most common measure,the receiver operating characteristic,to evaluate keyword spotters,we present an iterative,which},
month = {apr},
number = {4},
pages = {317--329},
title = {{Discriminative keyword spotting}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167639308001489},
volume = {51},
year = {2009}
}
@article{Khadkevich2009,
author = {Khadkevich, M and Omologo, Maurizio},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Khadkevich, Omologo - 2009 - Use of Hidden Markov Models and Factored Language Models for Automatic Chord Recognition.pdf:pdf},
journal = {ISMIR},
number = {Ismir},
pages = {561--566},
title = {{Use of Hidden Markov Models and Factored Language Models for Automatic Chord Recognition.}},
url = {http://ismir2009.ismir.net/proceedings/OS7-4.pdf},
year = {2009}
}
@article{Kim2004,
author = {Kim, JG and Jung, HY and Chung, Hyun-yeol},
file = {:Users/anna/Thesis/Papers/spc4{\_}256.pdf:pdf},
journal = {9th Conference Speech and {\ldots}},
keywords = {pseudo n-gram,spotting approach based on},
pages = {9--12},
title = {{A Keyword Spotting Approach based on Pseudo N-gram Language Model}},
url = {http://www.isca-speech.org/archive{\_}open/specom{\_}04/spc4{\_}256.html},
year = {2004}
}
@article{Kim1994,
author = {Kim, WG and Choi, JY and Youn, DH},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Kim, Choi, Youn - 1994 - HMM with global path constraint in Viterbi decoding for isolated word recognition.pdf:pdf},
isbn = {0780317750},
journal = {Acoustics, Speech, and Signal {\ldots}},
pages = {605--608},
title = {{HMM with global path constraint in Viterbi decoding for isolated word recognition}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=389221},
year = {1994}
}
@inproceedings{Kintzley2012,
author = {Kintzley, Keith and Jansen, Aren},
booktitle = {INTERSPEECH},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Kintzley, Jansen - 2012 - Inverting the Point Process Model for Fast Phonetic Keyword Search.pdf:pdf},
keywords = {a,a con-,a given position within,as a filter-bank where,each filter,evaluating keyword detections can,of a phone at,of filters,relates to the likelihood,s impulse response,then be seen as,this view,train with an array,volution of an impulse,word},
pages = {2--5},
title = {{Inverting the Point Process Model for Fast Phonetic Keyword Search.}},
url = {http://20.210-193-52.unknown.qala.com.sg/archive/archive{\_}papers/interspeech{\_}2012/i12{\_}2438.pdf},
year = {2012}
}
@inproceedings{Kintzley2013,
author = {Kintzley, Keith and Jansen, Aren and Hermansky, Hynek},
booktitle = {INTERSPEECH},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Kintzley, Jansen, Hermansky - 2013 - Text-to-Speech Inspired Duration Modeling for Improved Whole-Word Acoustic Models.pdf:pdf},
keywords = {0,05,a model with $\sigma$,an example of such,duration of 1,gaussian to each phone,given a normalized word,in the dictionary form,s dictionary pronunciation,standard deviation $\sigma$,using equally spaced means,we simply assign one,µ and a fixed},
title = {{Text-to-Speech Inspired Duration Modeling for Improved Whole-Word Acoustic Models}},
url = {http://old-site.clsp.jhu.edu/{~}ajansen/papers/IS2013a.pdf},
year = {2013}
}
@inproceedings{Kintzley2012a,
author = {Kintzley, Keith and Jansen, Aren and Hermansky, Hynek},
booktitle = {INTERSPEECH},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Kintzley, Jansen, Hermansky - 2012 - MAP Estimation of Whole-Word Acoustic Models with Dictionary Priors.pdf:pdf},
keywords = {and also present a,applying these tech-,bayesian approach which unifies,dictionary-based,examples,framework,gaussian mixture models,niques in a point,ples are abundant,process model keyword spotting,the two,when keyword exam-},
pages = {1--4},
title = {{MAP Estimation of Whole-Word Acoustic Models with Dictionary Priors.}},
url = {http://old-site.clsp.jhu.edu/{~}ajansen/papers/IS2012c.pdf},
year = {2012}
}
@inproceedings{Kintzley2011,
author = {Kintzley, Keith and Jansen, Aren and Hermansky, Hynek},
booktitle = {INTERSPEECH},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Kintzley, Jansen, Hermansky - 2011 - Event Selection from Phone Posteriorgrams Using Matched Filters.pdf:pdf},
keywords = {index terms,keyword spotting,matched filters,model,point process,poste-,posteriorgram,spotting system,through phonetic matched filters,which reduce a phone},
title = {{Event Selection from Phone Posteriorgrams Using Matched Filters.}},
url = {http://old-site.clsp.jhu.edu/{~}ajansen/papers/IS2011c.pdf},
year = {2011}
}
@article{Kirchhoff2002,
author = {Kirchhoff, Katrin},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Kirchhoff - 2002 - Mixed-memory Markov models for automatic language identification.pdf:pdf},
isbn = {0780374029},
journal = {Acoustics, Speech, and {\ldots}},
pages = {761--764},
title = {{Mixed-memory Markov models for automatic language identification}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5743829},
year = {2002}
}
@inproceedings{klapuri,
address = {Phoenix, AZ, USA},
author = {Klapuri, A},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
pages = {115--118},
title = {{Sound onset detection by applying psychoacoustic knowledge}},
year = {1999}
}
@inproceedings{inproceedings:analysis_meter,
author = {Klapuri, A P and Eronen, A J and Astola, J T},
booktitle = {IEEE Trans. Speech and Audio Processing},
pages = {342--355},
title = {{Analysis of the meter of acoustic musical signals}},
year = {2004}
}
@article{Knill2014,
author = {Knill, KM and Gales, MJF and Ragni, Anton and Rath, SP},
file = {:Users/anna/Thesis/Papers/interspeech14-knill.pdf:pdf},
journal = {Proc Inter-Speech},
title = {{Language independent and unsupervised acoustic models for speech recognition and keyword spotting}},
url = {http://193.6.4.39/{~}czap/letoltes/IS14/IS2014/PDF/AUTHOR/IS140716.PDF},
year = {2014}
}
@article{Kobayashi2003,
author = {Kobayashi, H.},
doi = {10.1109/LSP.2002.806705},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Kobayashi - 2003 - An efficient forward-backward algorithm for an explicit-duration hidden Markov model.pdf:pdf},
issn = {1070-9908},
journal = {IEEE Signal Processing Letters},
month = {jan},
number = {1},
pages = {11--14},
title = {{An efficient forward-backward algorithm for an explicit-duration hidden Markov model}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1172820},
volume = {10},
year = {2003}
}
@book{Kovacs2007,
author = {Kovacs, Laszlo and Fuhr, Norbert and Meghini, Carlo},
booktitle = {11th European Conference, ECDL 2007},
file = {:Users/anna/Downloads/bok{\%}3A978-3-540-74851-9.pdf:pdf},
isbn = {9783540748502},
title = {{Research and Advanced Technology for Digital Libraries}},
url = {http://link.springer.com/content/pdf/10.1007/3-540-48155-9.pdf},
year = {2007}
}
@article{Krebs2016,
abstract = {In this paper, we propose a system that extracts the down-beat times from a beat-synchronous audio feature stream of a music piece. Two recurrent neural networks are used as a front-end: the first one models rhythmic content on multiple frequency bands, while the second one models the harmonic content of the signal. The output activations are then combined and fed into a dynamic Bayesian network which acts as a rhythmical language model. We show on seven commonly used datasets of Western music that the system is able to achieve state-of-the-art results.},
author = {Krebs, Florian and B{\"{o}}ck, Sebastian and Dorfer, Matthias and Widmer, Gerhard},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Krebs et al. - 2016 - Downbeat Tracking Using Beat-Synchronous Features and Recurrent Neural Networks.pdf:pdf},
journal = {Proceedings of the 17th International Society for Music Information Retrieval Conference},
pages = {129--135},
title = {{Downbeat Tracking Using Beat-Synchronous Features and Recurrent Neural Networks}},
year = {2016}
}
@inproceedings{kruspe11,
address = {Ilmenau, Germany},
author = {Kruspe, A and Lukashevich, H and Abesser, J and Grossmann, H and Dittmar, C},
booktitle = {Proceedings of Audio Engineering Society 42nd Conference},
pages = {44--53},
title = {{Automatic classification of musical pieces into global cultural areas}},
year = {2011}
}
@article{Kruspe,
author = {Kruspe, Anna M and Abe{\ss}er, Jakob and Dittmar, Christian},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Kruspe, Abe{\ss}er, Dittmar - Unknown - Towards coarse-scale event detection in music(2).pdf:pdf},
isbn = {9781450302685},
keywords = {12,2011,bc,canada,chi 2011,coarse-scale events,copyright is held by,event detection,may 7,musical events,owner,s,the author,vancouver},
title = {{Towards coarse-scale event detection in music}}
}
@book{diplomarbeit,
author = {Kruspe, Anna Marie},
publisher = {TU Ilmenau},
series = {Diploma thesis},
title = {{Automatic classification of musical pieces into global cultural areas}},
year = {2011}
}
@inproceedings{Kruspe2014,
author = {Kruspe, Anna and Abesser, Jakob and Dittmar, Christian},
booktitle = {53rd Audio Engineering Society Conference},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Kruspe, Abesser, Dittmar - Unknown - A GMM approach to singing lanuguage identification.pdf:pdf},
title = {{A GMM approach to singing language identification}},
year = {2014}
}
@article{Kumar2010,
author = {Kumar, Pawan and Biswas, Astik and Mishra, AN and Chandra, Mahesh},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Kumar et al. - 2010 - Spoken language identification using hybrid Feature Extraction Methods.pdf:pdf},
journal = {Journal of Telecommunications},
number = {2},
pages = {11--15},
title = {{Spoken language identification using hybrid Feature Extraction Methods}},
url = {http://arxiv.org/abs/1003.5623},
volume = {1},
year = {2010}
}
@inproceedings{inproceedings:cyclic_beat_spectrum,
author = {Kurth, Frank and Gehrmann, Thorsten and M�ller, Meinard},
booktitle = {Proc. of the 7th Int. Conf. on MIR},
pages = {35--40},
title = {{The Cyclic Beat Spectrum: Tempo-Related Audio Features for Time-Scale Invariant Audio Identification}},
year = {2006}
}
@phdthesis{Lal2011,
author = {Lal, Partha},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Lal - 2011 - Cross-lingual automatic speech recognition using tandem features.pdf:pdf},
title = {{Cross-lingual automatic speech recognition using tandem features}},
url = {http://www.era.lib.ed.ac.uk/handle/1842/5773},
year = {2011}
}
@article{Lambert2014,
author = {Lambert, Andrew and Weyde, Tillman and Armstrong, Newton},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Lambert, Weyde, Armstrong - 2014 - Studying the Effect of Metre Perception on Rhythm and Melody Modelling with LSTMs.pdf:pdf},
isbn = {9781577356875},
journal = {Proceedings of the 3rd International Workshop on Musical Metacreation (MUME 2014)},
keywords = {AAAI Technical Report WS-14-18},
pages = {18--24},
title = {{Studying the Effect of Metre Perception on Rhythm and Melody Modelling with LSTMs}},
year = {2014}
}
@article{Laurila1997,
author = {Laurila, K.},
doi = {10.1109/ICASSP.1997.596074},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Laurila - 1997 - Noise robust speech recognition with state duration constraints.pdf:pdf},
isbn = {0-8186-7919-0},
journal = {1997 IEEE International Conference on Acoustics, Speech, and Signal Processing},
pages = {871--874},
publisher = {IEEE Comput. Soc. Press},
title = {{Noise robust speech recognition with state duration constraints}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=596074},
volume = {2},
year = {1997}
}
@article{Lee1989,
author = {Lee, CH and Rabiner, L},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Lee, Rabiner - 1989 - A frame-synchronous network search algorithm for connected word recognition.pdf:pdf},
journal = {Acoustics, Speech and Signal Processing, {\ldots}},
title = {{A frame-synchronous network search algorithm for connected word recognition}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=46547},
year = {1989}
}
@inproceedings{inproceedings:modulation_spectral_contrast_feature,
author = {Lee, Chang-Hsing and Shih, Jau-Ling and Yu, Kun-Ming and Su, Jung-Mau},
booktitle = {Proc. of the IEEE Intl. Conf. on Multimedia and Expo (ICME)},
pages = {204--207},
title = {{Automatic Music Genre Classification using Modulation Spectral Contrast Feature}},
year = {2007}
}
@article{Lee2013,
author = {Lee, HS and Shih, YC},
file = {:Users/anna/Thesis/Papers/15913-F.pdf:pdf},
journal = {Acoustics, Speech and {\ldots}},
title = {{Subspace-based phonotactic language recognition using multivariate dynamic linear models}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6638993},
year = {2013}
}
@inproceedings{inproceedings:epcp,
author = {Lee, Kyogu},
booktitle = {Proc. Intl. Computer Music Conf. (ICMC)},
title = {{Automatic Chord Recognition from Audio Using Enhanced Pitch Class Profile}},
year = {2006}
}
@article{Lehner2015,
abstract = {Singing voice detection aims at identifying the regions in a music recording where at least one person sings. This is a challenging problem that cannot be solved without analysing the temporal evolution of the signal. Current state-of-the-art methods combine timbral with temporal characteristics, by summarising various feature values over time, e.g. by computing their variance. This leads to more contextual information, but also to increased latency, which is problematic if our goal is on-line, real-time singing voice detection. To overcome this problem and reduce the necessity to include context in the features themselves, we introduce a method that uses Long Short-Term Memory Recurrent Neural Networks (LSTM-RNN). In experiments on several data sets, the resulting singing voice detector outperforms the state-of-the-art baselines in terms of accuracy, while at the same time drastically reducing latency and increasing the time resolution of the detector.},
author = {Lehner, Bernhard and Widmer, Gerhard and Bock, Sebastian},
doi = {10.1109/EUSIPCO.2015.7362337},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Lehner, Widmer, Bock - 2015 - A low-latency, real-time-capable singing voice detection method with LSTM recurrent neural networks.pdf:pdf},
isbn = {9780992862633},
journal = {2015 23rd European Signal Processing Conference, EUSIPCO 2015},
keywords = {music information retrieval,recurrent neural nets,singing voice detection},
pages = {21--25},
title = {{A low-latency, real-time-capable singing voice detection method with LSTM recurrent neural networks}},
year = {2015}
}
@article{Lehtonen2005,
author = {Lehtonen, M},
file = {:Users/anna/Thesis/Papers/rr05-41.pdf:pdf},
number = {0},
title = {{Hierarchical approach for spotting keywords}},
url = {http://infoscience.epfl.ch/record/83297},
year = {2005}
}
@article{Lehtonen2005a,
author = {Lehtonen, Mikko and Fousek, Petr and Hermansky, Hynek},
file = {:Users/anna/Thesis/Papers/mlmi05.pdf:pdf},
title = {{Hierarchical approach for spotting keywords}},
url = {http://infoscience.epfl.ch/record/83297},
year = {2005}
}
@techreport{Lei,
author = {Lei, Howard},
file = {:Users/anna/Papers/AFRL{\_}ICSI{\_}visit2{\_}JFA{\_}tutorial{\_}icsitalk.pdf:pdf;:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Lei - Unknown - Joint Factor Analysis (JFA) and i-vector Tutorial(2).pdf:pdf},
title = {{Joint Factor Analysis (JFA) and i-vector Tutorial}}
}
@article{Leia,
author = {Lei, Yun and Ferrer, Luciana and Lawson, Aaron and McLaren, M and Scheffer, Nicolas},
file = {:Users/anna/Thesis/Papers/paper{\_}odyssey14{\_}y.lei{\_}final.pdf:pdf},
journal = {sriinternational.org},
number = {Ml},
title = {{Application of Convolutional Neural Networks to Language Identification in Noisy Conditions}},
url = {http://sriinternational.org/sites/default/files/publications/paper{\_}odyssey14{\_}y.lei{\_}final.pdf}
}
@article{Lei2014,
abstract = {This paper proposes two novel frontends for robust lan- guage identification (LID) using a convolutional neural network (CNN) trained for automatic speech recognition (ASR). In the CNN/i-vector frontend, the CNN is used to obtain the posterior probabilities for i-vector training and extraction instead of a uni- versal background model (UBM). The CNN/posterior frontend is somewhat similar to a phonetic system in that the occupation counts of (tied) triphone states (senones) given by the CNN are used for classification. They are compressed to a low dimen- sional vector using probabilistic principal component analysis (PPCA). Evaluated on heavily degraded speech data, the pro- posed front ends provide significant improvements of up to 50{\%} on average equal error rate compared to a UBM/i-vector base- line. Moreover, the proposed frontends are complementary and give significant gains of up to 20{\%} relative to the best single system when combined. 1.},
author = {Lei, Yun and Ferrer, Luciana and Lawson, Aaron and Mclaren, Mitchell and Scheffer, Nicolas},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Lei et al. - 2014 - Application of Convolutional Neural Networks to Language Identification in Noisy Conditions.pdf:pdf},
journal = {Proceedings of Odyssey 2014 - The Speaker and Language Recognition Workshop},
number = {Ml},
pages = {287--292},
title = {{Application of Convolutional Neural Networks to Language Identification in Noisy Conditions}},
url = {http://www.sri.com/sites/default/files/publications/paper{\_}odyssey14{\_}y.lei{\_}final.pdf},
year = {2014}
}
@article{Levinson1986,
author = {Levinson, SE},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Levinson - 1986 - Continuously variable duration hidden Markov models for speech analysis.pdf:pdf},
journal = {Acoustics, Speech, and Signal Processing, IEEE {\ldots}},
title = {{Continuously variable duration hidden Markov models for speech analysis}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1168801},
year = {1986}
}
@article{Li2005,
author = {Li, Haizhou and Ma, Bin},
doi = {10.3115/1219840.1219904},
file = {:Users/anna/Downloads/p515-li.pdf:pdf},
journal = {Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics - ACL '05},
number = {June},
pages = {515--522},
title = {{A phonotactic language model for spoken language identification}},
url = {http://portal.acm.org/citation.cfm?doid=1219840.1219904},
year = {2005}
}
@article{Li2007,
author = {Li, Haizhou and Ma, Bin and Lee, Chin-Hui},
doi = {10.1109/TASL.2006.876860},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Li, Ma, Lee - 2007 - A Vector Space Modeling Approach to Spoken Language Identification.pdf:pdf},
issn = {1558-7916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
number = {1},
pages = {271--284},
title = {{A Vector Space Modeling Approach to Spoken Language Identification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4032773},
volume = {15},
year = {2007}
}
@article{article:mir_for_nonwestern_music,
author = {Lidy, Thomas and {Silla Jr.}, Carlos N and Cornelis, Olmo and Gouyon, Fabien and Rauber, Andreas and Kaestner, Celso A A and Koerich, Alessandro L},
journal = {Signal Processing},
number = {90},
pages = {1032--1048},
title = {{On the suitability of state-of-the-art music information retrieval methods for analyzing, categorizing and accessing non-Western and ethnic music collections}},
year = {2010}
}
@inproceedings{Lim2016,
abstract = {Previous work on whispered speech recognition has shown that acoustic models (AM) trained on whispered speech can some-what classify unwhispered (neutral) speech sounds, but not vice versa. In fact, AMs trained purely on neutral speech completely fail to recognize whispered speech. Meanwhile, recipes used to train neutral AMs will work just as well for whispered speech, but such methods require a large volume of transcribed whis-pered speech which is expensive to gather. In this work, we propose and investigate the use of bottleneck feature networks to normalize differences between whispered and neutral speech modes. Our extensive experiments show that this type of speech variability can be effectively normalized. We also show that it is possible to transfer this knowledge from two source languages with whispered speech (Mandarin and English), to a new target language (Malay) without whispered speech. Furthermore, we report a substantial reduction in word error rate for cross-mode speech recognition, effectively demonstrate that it is possible to train acoustic models capable of classifying both types of speech without needing any additional whispered speech.},
author = {Lim, Boon Pang and Wong, Faith and Li, Yuyao and Bay, Jia Wei},
booktitle = {Interspeech},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Lim et al. - 2016 - Transfer Learning with Bottleneck Feature Networks for Whispered Speech Recognition.pdf:pdf},
keywords = {Index Terms,bottleneck feature networks,deep neu-ral networks,low-resource speech recognition,multilingual bottle-necks,speech recognition,whispered speech},
pages = {1578--1582},
title = {{Transfer Learning with Bottleneck Feature Networks for Whispered Speech Recognition}},
year = {2016}
}
@inproceedings{prosody,
address = {Philadelphia, PA, USA},
author = {Lin, C.-Y. and Wang, H.-C.},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Lin, Wang - 2005 - Language identification using pitch contour information.pdf:pdf},
pages = {601--604},
title = {{Language identification using pitch contour information}},
year = {2005}
}
@inproceedings{Liu2016,
abstract = {Attention-based encoder-decoder neural network models have recently shown promising results in machine translation and speech recognition. In this work, we propose an attention-based neural network model for joint intent detection and slot filling, both of which are critical steps for many speech understanding and dialog systems. Unlike in machine translation and speech recognition, alignment is explicit in slot filling. We explore different strategies in incorporating this alignment information to the encoder-decoder framework. Learning from the attention mechanism in encoder-decoder model, we further propose introducing attention to the alignment-based RNN models. Such attentions provide additional information to the intent classification and slot label prediction. Our independent task models achieve state-of-the-art intent detection error rate and slot filling F1 score on the benchmark ATIS task. Our joint training model further obtains 0.56{\%} absolute (23.8{\%} relative) error reduction on intent detection and 0.23{\%} absolute gain on slot filling over the independent task models.},
archivePrefix = {arXiv},
arxivId = {1609.01454},
author = {Liu, Bing and Lane, Ian},
booktitle = {Interspeech},
eprint = {1609.01454},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Lane - 2016 - Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling.pdf:pdf},
keywords = {[Electronic Manuscript]},
number = {1},
pages = {2--6},
title = {{Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling}},
url = {http://arxiv.org/abs/1609.01454},
year = {2016}
}
@article{Liu2016a,
author = {Liu, I-ting and Randall, Richard},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Randall - 2016 - Predicting Missing Music Components With Bidirectional Long Short-Term Memory Neural Networks.pdf:pdf},
journal = {Proc. 17th International Society for Music Information Retrieval Conference},
title = {{Predicting Missing Music Components With Bidirectional Long Short-Term Memory Neural Networks}},
year = {2016}
}
@inproceedings{inproceedings:cultural_style,
author = {Liu, Yuxiang and Xiang, Qiaoliang and Wang, Ye and Cai, Lianhong},
booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing},
title = {{Cultural style based music classification of audio signals}},
year = {2009}
}
@article{Ljolje1991,
author = {Ljolje, A and Riley, M D},
doi = {10.1109/ICASSP.1991.150379},
file = {:Users/anna/Thesis/Papers/00150379.pdf:pdf},
isbn = {0-7803-0003-3},
issn = {07367791},
journal = {Proceedings of the International Conference on Acoustics, Speech, and Signal Processing},
pages = {473--476},
title = {{Automatic segmentation and labeling of speech}},
year = {1991}
}
@article{Ljolje1991a,
author = {Ljolje, A and Riley, M D},
doi = {10.1109/ICASSP.1991.150379},
file = {:Users/anna/Thesis/Papers/10.1.1.44.7009.pdf:pdf},
isbn = {0-7803-0003-3},
issn = {07367791},
journal = {Proceedings of the International Conference on Acoustics, Speech, and Signal Processing},
pages = {473--476},
title = {{Automatic segmentation and labeling of speech}},
year = {1991}
}
@article{Lopez-Moreno2016,
author = {Lopez-Moreno, Ignacio and Gonzalez-Dominguez, Javier and Martinez, David and Plchot, Oldřich and Gonzalez-Rodriguez, Joaquin and Moreno, Pedro J.},
doi = {10.1016/j.csl.2016.03.001},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Lopez-Moreno et al. - 2016 - On the use of deep feedforward neural networks for automatic language identification.pdf:pdf},
issn = {08852308},
journal = {Computer Speech {\&} Language},
keywords = {bottleneck,dnn,i-vectors,lid},
pages = {46--59},
title = {{On the use of deep feedforward neural networks for automatic language identification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S088523081530036X},
volume = {40},
year = {2016}
}
@article{Loscos1999,
author = {Loscos, A and Cano, P and Bonada, J},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Loscos, Cano, Bonada - 1999 - Low-delay singing voice alignment to text.pdf:pdf},
journal = {Proceedings of the ICMC},
title = {{Low-delay singing voice alignment to text}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.18.5357{\&}rep=rep1{\&}type=pdf},
year = {1999}
}
@inproceedings{Lu2016,
abstract = {We study the segmental recurrent neural network for end-to-end acoustic modelling. This model connects the segmental conditional random field (CRF) with a recurrent neural network (RNN) used for feature extraction. Compared to most previous CRF-based acoustic models, it does not rely on an external system to provide features or segmentation boundaries. Instead, this model marginalises out all the possible segmentations, and features are extracted from the RNN trained together with the segmental CRF. In essence, this model is self-contained and can be trained end-to-end. In this paper, we discuss practical training and decoding issues as well as the method to speed up the training in the context of speech recognition. We performed experiments on the TIMIT dataset. We achieved 17.3 phone error rate (PER) from the first-pass decoding --- the best reported result using CRFs, despite the fact that we only used a zeroth-order CRF and without using any language model.},
archivePrefix = {arXiv},
arxivId = {1603.00223},
author = {Lu, Liang and Kong, Lingpeng and Dyer, Chris and Smith, Noah A. and Renals, Steve},
booktitle = {Interspeech},
eprint = {1603.00223},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Lu et al. - 2016 - Segmental Recurrent Neural Networks for End-to-end Speech Recognition.pdf:pdf},
keywords = {[Electronic Manuscript]},
pages = {5},
title = {{Segmental Recurrent Neural Networks for End-to-end Speech Recognition}},
url = {http://arxiv.org/abs/1603.00223},
year = {2016}
}
@article{article:mood_detection,
author = {Lu, Lie and Liu, Dan and Zhang, Hong-Jiang},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
month = {jan},
number = {1},
pages = {5--18},
title = {{Automatic Mood Detection and Tracking of Music Audio Signals}},
volume = {14},
year = {2006}
}
@article{Lui2012,
author = {Lui, Marco and Baldwin, Timothy},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Lui, Baldwin - 2012 - langid. py An off-the-shelf language identification tool.pdf:pdf},
journal = {Proceedings of the ACL 2012 System Demonstrations},
number = {July},
pages = {25--30},
title = {{langid. py: An off-the-shelf language identification tool}},
url = {http://dl.acm.org/citation.cfm?id=2390475},
year = {2012}
}
@inproceedings{inproceedings:fs_vs_fst,
author = {Lukashevich, Hanna},
booktitle = {Proceedings of the 126th AES Convention},
month = {may},
title = {{Feature Selection vs. Feature Space Transformation in Automatic Music Genre Classification Tasks}},
year = {2009}
}
@inproceedings{inproceedings:multi_domain_labeling,
author = {Lukashevich, Hanna and Abe�er, Jakob and Dittmar, Christian and Grossmann, Holger},
booktitle = {10th International Society for Music Information Retrieval Conference (ISMIR)},
title = {{From Multi-Labeling to Multi-Domain-Labeling: A novel two-dimensional approach to music genre classification}},
year = {2009}
}
@article{Luong2015,
abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
archivePrefix = {arXiv},
arxivId = {1508.04025},
author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
eprint = {1508.04025},
file = {:Users/anna/Work/Interspeech16/whitepaper/luong{\_}attention{\_}15.pdf:pdf},
isbn = {9781941643327},
issn = {10495258},
journal = {Emnlp},
number = {September},
pages = {11},
title = {{Effective Approaches to Attention-based Neural Machine Translation}},
url = {http://arxiv.org/abs/1508.04025},
year = {2015}
}
@article{Ma2010,
author = {Ma, Chengyuan and Kuo, HKJ and Soltau, Hagen},
file = {:Users/anna/Thesis/Papers/0004394.pdf:pdf},
isbn = {9781424442966},
journal = {{\ldots} Speech and Signal {\ldots}},
pages = {4394--4397},
title = {{A comparative study on system combination schemes for LVCSR}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5495627},
year = {2010}
}
@inproceedings{Ma2013,
author = {Ma, Jeff and Zhang, Bing and Matsoukas, Spyros and Mallidi, Sri Harish and Li, Feipeng and Hermansky, Hynek},
booktitle = {INTERSPEECH},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Ma et al. - 2013 - Improvements in Language Identification on the RATS Noisy Speech Corpus.pdf:pdf},
pages = {2--6},
title = {{Improvements in Language Identification on the RATS Noisy Speech Corpus}},
year = {2013}
}
@phdthesis{Ma2008,
author = {Ma, Ning},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Ma - 2008 - Informing multisource decoding in robust automatic speech recognition.pdf:pdf},
number = {July},
title = {{Informing multisource decoding in robust automatic speech recognition}},
url = {http://staffwww.dcs.shef.ac.uk/people/N.Ma/pubs/ma2008-thesis.pdf},
year = {2008}
}
@article{Mamou2013,
author = {Mamou, Jonathan and Cui, J and Cui, Xiaodong},
file = {:Users/anna/Thesis/Papers/ICASSP13{\_}ibm1.pdf:pdf},
isbn = {9781479903566},
journal = {{\ldots} , Speech and Signal {\ldots}},
pages = {8272--8276},
title = {{System combination and score normalization for spoken term detection}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6639278},
year = {2013}
}
@article{Mandal2013,
author = {Mandal, Anupam and {Prasanna Kumar}, K. R. and Mitra, Pabitra},
doi = {10.1007/s10772-013-9217-1},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Mandal, Prasanna Kumar, Mitra - 2013 - Recent developments in spoken term detection a survey.pdf:pdf},
issn = {1381-2416},
journal = {International Journal of Speech Technology},
keywords = {and retrieval are also,dexing mechanisms are used,getting increasing attention,lvcsr,query-by-example,records,spoken term detection,subword-lattice,template matching,these in-,to organize spoken data},
month = {dec},
title = {{Recent developments in spoken term detection: a survey}},
url = {http://link.springer.com/10.1007/s10772-013-9217-1},
year = {2013}
}
@book{book:information_retrieval,
author = {Manning, Christopher D and Raghavan, Prabhakar and Sch�tze, Hinrich},
chapter = {8: Evaluat},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Manning, Raghavan, Sch�tze - 2009 - An Introduction to Information Retrieval.pdf:pdf},
publisher = {Cambridge University Press},
title = {{An Introduction to Information Retrieval}},
year = {2009}
}
@article{Martin2010,
author = {Martin, Alvin and Greenberg, Craig},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Martin, Greenberg - 2010 - The 2009 NIST Language Recognition Evaluation.pdf:pdf},
journal = {Proceedings of Odyssey},
number = {July},
pages = {165--171},
title = {{The 2009 NIST Language Recognition Evaluation}},
url = {http://www.isca-speech.org/archive{\_}open/archive{\_}papers/odyssey{\_}2010/papers/od10{\_}030.pdf},
year = {2010}
}
@article{Martinez2012,
author = {Martinez, D and Burget, L},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Martinez, Burget - 2012 - IVector-based prosodic system for language identification.pdf:pdf},
isbn = {9781467300469},
journal = {Acoustics, Speech and {\ldots}},
pages = {4861--4864},
title = {{IVector-based prosodic system for language identification}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6289008},
year = {2012}
}
@article{Martnez2011,
author = {Martınez, D and Plchot, O and Burget, L},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Martınez, Plchot, Burget - 2011 - Language Recognition in iVectors Space.pdf:pdf},
journal = {{\ldots} of Interspeech, Firenze, {\ldots}},
number = {August},
pages = {861--864},
title = {{Language Recognition in iVectors Space}},
url = {http://www.fit.vutbr.cz/research/groups/speech/publi/2011/martinez{\_}interspeech2011{\_}291.pdf},
year = {2011}
}
@article{Mat2003,
author = {Matˇ, Pavel and Schwarz, Petr and Cernock, Jan},
file = {:Users/anna/Downloads/ra2005.pdf:pdf},
pages = {1--4},
title = {{Phonotactic Language Identification ∗}},
year = {2003}
}
@inproceedings{matejka,
address = {Brno, Czech Republic},
author = {Matejka, P and Szoeke, I and Schwarz, P and Cernocky, J},
booktitle = {Proceedings of 7th International Conference on Text, Speech, and Dialogue (TSD)},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Matˇ, Sz - Unknown - Automatic Language Identification using Phoneme and Automatically Derived Unit Strings.pdf:pdf},
pages = {147--154},
title = {{Automatic language identification using phoneme and automatically derived unit strings}},
year = {2004}
}
@inproceedings{Matejka2012,
author = {Matejka, Pavel and Plchot, Oldrich and Soufifar, Mehdi and Glembek, Ondrej and D'Haro, Luis Fernando and Vesely, Karel and Grezl, Fr and Ma, Jeff and Matsoukas, Spyros and Dehak, Najim},
booktitle = {INTERSPEECH},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Matejka et al. - 2012 - Patrol Team Language Identification System for DARPA RATS P1 Evaluation.pdf:pdf},
title = {{Patrol Team Language Identification System for DARPA RATS P1 Evaluation}},
url = {http://20.210-193-52.unknown.qala.com.sg/archive/archive{\_}papers/interspeech{\_}2012/i12{\_}0050.pdf},
year = {2012}
}
@article{Matejka2005,
author = {Matejka, Pavel and Schwarz, Petr and Cernock{\'{y}}, Jan and Chytil, Pavel},
file = {:Users/anna/Downloads/PavelMatejka.ps.pdf:pdf},
journal = {Interspeech},
pages = {2237--2240},
title = {{Phonotactic Language Identification using High Quality Phoneme Recognition}},
year = {2005}
}
@article{Matejka2014,
abstract = {This paper presents the application of Neural Network Bot- tleneck (BN) features in Language Identification (LID).BNfea- tures are generally used for Large Vocabulary Speech Recogni- tion in conjunction with conventional acoustic features, such as MFCC or PLP.We compare the BNfeatures to several common types of acoustic features used in the state-of-the-art LID sys- tems. The test set is from DARPA RATS (Robust Automatic Transcription of Speech) program, which seeks to advance state-of-the-art detection capabilities on audio from highly de- graded radio communication channels. On this type of noisy data, we show that in average, the BN features provide a 45{\%} relative improvement in the Cavgor Equal Error Rate (EER) metrics across several test duration conditions, with respect to our single best acoustic features.},
author = {Matejka, Pavel and Zhang, Le and Ng, Tim and Mallidi, Sri Harish and Glembek, Ondrej and Ma, Jeff and Zhang, Bing},
file = {:Users/anna/Work/Interspeech16/whitepaper/matejka{\_}bottleneck{\_}14.pdf:pdf},
journal = {Proceedings of Odyssey 2014 - The Speaker and Language Recognition Workshop},
number = {June},
pages = {3--8},
title = {{Neural Network Bottleneck Features for Language Identification}},
url = {http://cs.uef.fi/odyssey2014/program/pdfs/35.pdf},
year = {2014}
}
@article{article:quatieri,
author = {McAulay, Robert J and Quatieri, Thomas F},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/McAulay, Quatieri - 1986 - Speech AnalysisSynthesis Based on a Sinusoidal Representation.pdf:pdf},
journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
month = {aug},
number = {4},
pages = {744--754},
title = {{Speech Analysis/Synthesis Based on a Sinusoidal Representation}},
volume = {34},
year = {1986}
}
@inproceedings{inproceedings:genre_high_level,
author = {McKay, Cory and Fujinaga, Ichiro},
booktitle = {Proc. of the Intl. Conf. on Music Information Retrieval (ISMIR)},
title = {{Automatic Genre Classification using large High-Level musical feature sets}},
year = {2004}
}
@article{McLaren2013,
author = {McLaren, M and Lawson, Aaron and Lei, Yun and Scheffer, Nicolas},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/McLaren et al. - 2013 - Adaptive gaussian backend for robust language identification.pdf:pdf},
journal = {Submitted to Interspeech},
title = {{Adaptive gaussian backend for robust language identification}},
url = {http://www.sri.com/sites/default/files/publications/adaptive{\_}gaussian{\_}backend{\_}for{\_}robust{\_}language{\_}identification-final.pdf},
year = {2013}
}
@inproceedings{Mclaren2016,
author = {Mclaren, Mitchell and Castan, Diego and Ferrer, Luciana and Lawson, Aaron},
booktitle = {Interspeech},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Mclaren et al. - 2016 - On the Issue of Calibration in DNN-based Speaker Recognition Systems.pdf:pdf},
keywords = {[Electronic Manuscript]},
pages = {1825--1829},
title = {{On the Issue of Calibration in DNN-based Speaker Recognition Systems}},
year = {2016}
}
@article{McVicar2012,
author = {McVicar, M and Bie, Tijl De},
file = {:Users/anna/Downloads/cmmr2012{\_}submission{\_}61.pdf:pdf},
journal = {9th International Symposium on Computer Music Modelling and Retrieval (CMMR 2012)},
keywords = {canonical correlation analysis,fm,last,million,mood detection,musixmatch,song dataset},
number = {June},
pages = {19--22},
title = {{CCA and a multi-way extension for investigating common components between audio, lyrics and tags}},
url = {http://www.mattmcvicar.com/wp-content/uploads/2010/12/cmmr2012{\_}submission{\_}61.pdf},
year = {2012}
}
@article{McVicar2014,
author = {McVicar, M and Ellis, DPW and Goto, M},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/McVicar, Ellis, Goto - 2014 - LEVERAGING REPETITION FOR IMPROVED AUTOMATIC LYRIC TRANSCRIPTION IN POPULAR MUSIC.pdf:pdf},
isbn = {9781479928934},
journal = {mattmcvicar.com},
pages = {3141--3145},
title = {{LEVERAGING REPETITION FOR IMPROVED AUTOMATIC LYRIC TRANSCRIPTION IN POPULAR MUSIC}},
url = {http://www.mattmcvicar.com/wp-content/uploads/2014/05/ICASSP{\_}2014.pdf},
year = {2014}
}
@article{McVicar2011,
abstract = {Understanding the mood of music holds great potential for recommendation and genre identiﬁcation problems. Unfortu- nately, hand-annotating music with mood tags is usually an expensive, time-consuming and subjective process, to such an extent that automatic mood recognition methods are re- quired. In this paper we present a new unsupervised learn- ing approach for mood recognition, based on the lyrics and the audio of a song. Our system thus eliminates the need for ground truth mood annotations, even for training the system. We hypothesize that lyrics and audio are both partially de- termined by the mood, and that there are no other strong com- mon effects affecting these aspects of music. Based on this as- sumption, mood can be detected by performing a multi-modal analysis, identifying what lyrics and audio have in common. We demonstrate the effectiveness of this using Canonical Cor- relation Analysis, and conﬁrm our hypothesis in a subsequent analysis of the results.},
author = {McVicar, M. and Freeman, Tim and {De Bie}, T.},
file = {:Users/anna/Downloads/ismir2011{\_}writeup.pdf:pdf},
isbn = {9780615548654},
journal = {12th International Society for Music Information Retrieval Conference (ISMIR 2011)},
number = {Ismir},
pages = {783--788},
title = {{Mining the Correlation Between Lyrical and Audio Features and the Emergence of Mood}},
url = {http://ismir2011.ismir.net/papers/OS9-2.pdf},
year = {2011}
}
@inproceedings{Mcvicar2014,
author = {Mcvicar, Matt},
file = {:Users/anna/Downloads/ICASSP{\_}2014.pdf:pdf},
isbn = {9781479928934},
pages = {3141--3145},
title = {{LEVERAGING REPETITION FOR IMPROVED AUTOMATIC LYRIC TRANSCRIPTION IN POPULAR MUSIC}},
year = {2014}
}
@inproceedings{mehrabani,
address = {Prague, Czech Republic},
author = {Mehrabani, M and Hansen, J H L},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Mehrabani, Hansen - 2011 - Language identification for singing.pdf:pdf},
pages = {4408--4411},
title = {{Language identification for singing}},
year = {2011}
}
@article{Mesaros2012,
author = {Mesaros, A},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Mesaros - 2012 - Singing Voice Recognition for Music Information Retrieval.pdf:pdf},
isbn = {9789521528934},
journal = {{\ldots} teknillinen yliopisto. Julkaisu-Tampere University of {\ldots}},
title = {{Singing Voice Recognition for Music Information Retrieval}},
url = {https://dspace.cc.tut.fi/dpub/handle/123456789/21404},
year = {2012}
}
@article{Mesaros2010,
author = {Mesaros, A and Virtanen, T},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Mesaros, Virtanen - 2010 - Recognition of phonemes and words in singing.pdf:pdf},
journal = {Acoustics Speech and Signal {\ldots}},
pages = {1--4},
title = {{Recognition of phonemes and words in singing}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5495585},
year = {2010}
}
@article{Mesaros2008,
abstract = {This paper proposes an algorithmfor aligning singing in poly- phonic music audio with textual lyrics. As preprocessing, the sys- tem uses a voice separation algorithm based on melody transcrip- tion and sinusoidal modeling. The alignment is based on a hid- den Markov model speech recognizer where the acoustic model is adapted to singing voice. The textual input is preprocessed to create a language model consisting of a sequence of phonemes, pauses and possible instrumental breaks. Viterbi algorithmis used to align the audio features with the text. On a test set consisting of 17 commercial recordings, the system achieves an average ab- solute error of 1.40 seconds in aligning lines of the lyrics.},
author = {Mesaros, a and Virtanen, T},
file = {:Users/anna/Downloads/autalign{\_}cr.pdf:pdf},
isbn = {9789512295173},
journal = {Int. Conference on Digital Audio Effects (DAFx-08)},
pages = {1--4},
title = {{Automatic alignment of music audio and lyrics}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.212.6683{\&}rep=rep1{\&}type=pdf},
year = {2008}
}
@article{Mesaros2011,
author = {Mesaros, Annamaria and Virtanen, T},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Mesaros, Virtanen - Unknown - AUTOMATIC UNDERSTANDING OF LYRICS FROM SINGING.pdf:pdf},
journal = {Akustiikkap{\"{a}}iv{\"{a}}t},
pages = {1--6},
title = {{AUTOMATIC UNDERSTANDING OF LYRICS FROM SINGING}},
url = {http://www.akustinenseura.fi/wp-content/uploads/2013/08/Mesaros.pdf},
year = {2011}
}
@article{Mesaros2010a,
author = {Mesaros, Annamaria and Virtanen, Tuomas},
doi = {10.1155/2010/546047},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Mesaros, Virtanen - 2010 - Automatic Recognition of Lyrics in Singing.pdf:pdf},
issn = {1687-4714},
journal = {EURASIP Journal on Audio, Speech, and Music Processing},
pages = {1--11},
title = {{Automatic Recognition of Lyrics in Singing}},
url = {http://asmp.eurasipjournals.com/content/2010/1/546047},
volume = {2010},
year = {2010}
}
@article{Mesaros2009,
abstract = {This paper describes a study of the speaker adaptation techniques that can be applied for adapting a speech recog- nizer to singing voice. Maximum likelihood linear regres- sion (MLLR) techniques are studied, with speciﬁc details in choosing the number and types of transforms. The recog- nition performance of the different methods is measured in terms of phoneme recognition rate and singing-to-lyrics alignment errors of the adapted recognizers. Different meth- ods improve the correct recognition rate with up to 10 per- centage units, compared to the non-adapted system. In singing-to-alignment we obtain a best of 0.94 seconds mean absolute alignment error, compared to 1.26 seconds for the non-adapted system. Global adaptation was found to provide the most improvement in the performance, but small further improvement was obtained with regression tree adaptation.},
author = {Mesaros, Annamaria and Virtanen, Tuomas},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Mesaros, Virtanen - 2009 - Adaptation of a speech recognizer for singing voice.pdf:pdf},
issn = {22195491},
journal = {European Signal Processing Conference},
number = {1},
pages = {1779--1783},
title = {{Adaptation of a speech recognizer for singing voice}},
year = {2009}
}
@book{book:atlas_musik1,
author = {Michels, Ulrich},
edition = {5},
month = {sep},
publisher = {Deutscher Taschenbuch-Verlag, B�renreiter-Verlag},
title = {{dtv-Atlas zur Musik}},
volume = {1},
year = {1980}
}
@inproceedings{inproceedings:african_tone_scales,
author = {Moelants, Dirk and Cornelis, Olmo and Leman, Marc},
booktitle = {Proc. of the Intl. Conf. on Music Information Retrieval (ISMIR)},
title = {{Exploring African Tone Scales}},
year = {2009}
}
@article{Mohamed2012,
abstract = {Deep Belief Networks (DBNs) are a very competitive alternative to Gaussian mixture models for relating states of a hidden Markov model to frames of coefficients derived from the acoustic input. They are competitive for three reasons: DBNs can be fine-tuned as neural networks; DBNs have many non-linear hidden layers; and DBNs are generatively pre-trained. This paper illustrates how each of these three aspects contributes to the DBN's good recognition performance using both phone recognition performance on the TIMIT corpus and a dimensionally reduced visualization of the relationships between the feature vectors learned by the DBNs that preserves the similarity structure of the feature vectors at multiple scales. The same two methods are also used to investigate the most suitable type of input representation for a DBN.},
author = {Mohamed, Abdel Rahman and Hinton, Geoffrey and Penn, Gerald},
doi = {10.1109/ICASSP.2012.6288863},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Mohamed, Hinton, Penn - 2012 - Understanding how deep belief networks perform acoustic modelling.pdf:pdf},
isbn = {9781467300469},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Deep belief networks,acoustic modeling,neural networks},
pages = {4273--4276},
title = {{Understanding how deep belief networks perform acoustic modelling}},
year = {2012}
}
@article{Mohamed2012a,
abstract = {Gaussian mixture models are currently the dominant technique for modeling the emission distribution of hidden Markov models for speech recognition. We show that better phone recognition on the TIMIT dataset can be achieved by replacing Gaussian mixture models by deep neural networks that contain many layers of features and a very large number of parameters. These networks are first pre-trained as a multi-layer generative model of a window of spectral feature vectors without making use of any discriminative information. Once the generative pre-training has designed the features, we perform discriminative fine-tuning using backpropagation to adjust the features slightly to make them better at predicting a probability distribution over the states of monophone hidden Markov models.},
author = {Mohamed, Abdel-rahman and Dahl, George E. and Hinton, Geoffrey},
doi = {10.1109/TASL.2011.2109382},
file = {:Users/anna/Work/Papers/ismir15/Phonemes{\_}Singing/speechDBN{\_}jrnl.pdf:pdf},
isbn = {1558-7916 VO - 20},
issn = {1558-7916},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
number = {c},
pages = {14--22},
title = {{Acoustic Modeling Using Deep Belief Networks}},
volume = {20},
year = {2012}
}
@article{Mohamed2011,
author = {Mohamed, Abdel-rahman and Sainath, TN},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Mohamed, Sainath - 2011 - Deep belief networks using discriminative features for phone recognition.pdf:pdf},
journal = {{\ldots} , Speech and Signal {\ldots}},
title = {{Deep belief networks using discriminative features for phone recognition}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5947494},
year = {2011}
}
@article{Mohamed2010,
author = {Mohamed, Abdel-rahman and Yu, Dong and Deng, Li},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Mohamed, Yu, Deng - 2010 - Investigation of full-sequence training of deep belief networks for speech recognition.pdf:pdf},
journal = {INTERSPEECH},
keywords = {[Electronic Manuscript]},
number = {September},
pages = {2846--2849},
title = {{Investigation of full-sequence training of deep belief networks for speech recognition.}},
url = {http://www.msr-waypoint.net/pubs/135406/MMI-DBN-interspeech2010.pdf},
year = {2010}
}
@article{moore,
author = {Moore, B and Glasberg, B and Baer, T},
journal = {J. Audio Eng. Soc.},
number = {4},
pages = {224--240},
title = {{A model for the prediction of thresholds, loudness, and partial loudness}},
volume = {45},
year = {1997}
}
@article{Moreno2009,
abstract = {This paper addresses the problem of aligning long speech recordings to their transcripts. Previous work has focused on using highly tuned language models trained on the transcripts to reduce the search space. In this paper we propose the use of a factor automaton, a well known method to represent all substrings from a string. This automaton encodes a highly constrained language model trained on the transcripts. We show competitive results with n-gram models in several testing scenarios. Preliminary experiments show perfect alignments at a reduced computational load and with a smaller memory footprint when compared to n-gram models.},
author = {Moreno, Pedro J. and Alberti, Christopher},
doi = {10.1109/ICASSP.2009.4960722},
file = {:Users/anna/Thesis/Papers/b967b5f077049fc6cda554a554576d233ddf.pdf:pdf},
isbn = {9781424423545},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Finite state transducers,Speech alignment,Speech recognition},
pages = {4869--4872},
title = {{A factor automaton approach for the forced alignment of long speech recordings}},
year = {2009}
}
@article{Motlicek2012,
author = {Motlicek, Petr and Valente, Fabio and Szoke, I},
file = {:Users/anna/Thesis/Papers/06288898.pdf:pdf},
isbn = {9781467300469},
journal = {Acoustics, Speech and Signal {\ldots}},
keywords = {acoustic kws systems search,corresponding con fi -,dences are represented by,due to the especially,els and a background,fi ned words in,first,high num-,keyword mod-,likelihood ratios given the,model,parameterized spoken data,prede-,spotting systems},
pages = {4413--4416},
title = {{Improving acoustic based keyword spotting using LVCSR lattices}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6288898},
year = {2012}
}
@article{Moyal2013,
author = {Moyal, A and Aharonson, V and Tetariy, E and Gishri, M},
doi = {10.1007/978-1-4614-6489-1},
file = {:Users/anna/Thesis/Papers/9781461464884-c1.pdf:pdf},
isbn = {9781461464891},
journal = {{\ldots} Methods for Large Speech {\ldots}},
keywords = {acoustic,and comparison of the,followed by a discussion,in detail the three,kws,kws and phonetic search,kws methods,lvcsr kws,spotting methods,this chapter will review},
number = {Thambiratnam 2005},
pages = {7--12},
title = {{Keyword Spotting Methods}},
url = {http://link.springer.com/chapter/10.1007/978-1-4614-6489-1{\_}2/fulltext.html},
year = {2013}
}
@article{Moyal2013a,
address = {New York, NY},
author = {Moyal, Ami and Aharonson, Vered and Tetariy, Ella and Gishri, Michal},
doi = {10.1007/978-1-4614-6489-1},
file = {:Users/anna/Thesis/Papers/9781461464884-c1.pdf:pdf},
isbn = {978-1-4614-6488-4},
keywords = {acoustic,and comparison of the,followed by a discussion,in detail the three,kws,kws and phonetic search,kws methods,lvcsr kws,spotting methods,this chapter will review},
number = {Thambiratnam 2005},
pages = {7--12},
publisher = {Springer New York},
series = {SpringerBriefs in Electrical and Computer Engineering},
title = {{Phonetic Search Methods for Large Speech Databases}},
url = {http://link.springer.com/10.1007/978-1-4614-6489-1},
year = {2013}
}
@inproceedings{Mun2016,
author = {Mun, Seongkyu and Shon, Suwon and Kim, Wooil and Ko, Hanseok},
booktitle = {Interspeech},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Mun et al. - 2016 - Deep Neural Network Bottleneck Features for Acoustic Event Recognition.pdf:pdf},
keywords = {[Electronic Manuscript]},
pages = {2954--2957},
title = {{Deep Neural Network Bottleneck Features for Acoustic Event Recognition}},
year = {2016}
}
@article{Munson2002,
author = {Munson, Benjamin},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Munson - 2002 - Phoneme Perception Errors.pdf:pdf},
journal = {JASA},
pages = {1--4},
title = {{Phoneme Perception Errors}},
year = {2002}
}
@article{Munson2003,
author = {Munson, Benjamin and Donaldson, Gail S. and Allen, Shanna L. and Collison, Elizabeth a. and Nelson, David a.},
doi = {10.1121/1.1536630},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Munson et al. - 2003 - Patterns of phoneme perception errors by listeners with cochlear implants as a function of overall speech percept.pdf:pdf},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
number = {2},
pages = {925},
title = {{Patterns of phoneme perception errors by listeners with cochlear implants as a function of overall speech perception ability}},
url = {http://scitation.aip.org/content/asa/journal/jasa/113/2/10.1121/1.1536630},
volume = {113},
year = {2003}
}
@article{muthusamy,
author = {Muthusamy, Y K and Barnard, E and Cole, R A},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Muthusamy, Barnard, Cole - 1994 - Reviewing automatic language identification.pdf:pdf},
journal = {IEEE Signal Procesing Magazine},
month = {oct},
number = {4},
pages = {33--41},
title = {{Reviewing automatic language identification}},
volume = {11},
year = {1994}
}
@article{Muthusamy1994,
author = {Muthusamy, Y.K. and Jain, N. and Cole, R.a.},
doi = {10.1109/ICASSP.1994.389288},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Muthusamy, Jain, Cole - 1994 - Perceptual benchmarks for automatic language identification.pdf:pdf},
isbn = {0-7803-1775-0},
journal = {Proceedings of ICASSP '94. IEEE International Conference on Acoustics, Speech and Signal Processing},
pages = {I/333--I/336},
publisher = {Ieee},
title = {{Perceptual benchmarks for automatic language identification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=389288},
volume = {i},
year = {1994}
}
@article{Nakagawa2013,
author = {Nakagawa, Seiichi and Iwami, Keisuke and Fujii, Yasuhisa and Yamamoto, Kazumasa},
doi = {10.1016/j.specom.2012.12.001},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Nakagawa et al. - 2013 - A robustfast spoken term detection method based on a syllable n-gram index with a distance metric.pdf:pdf},
issn = {01676393},
journal = {Speech Communication},
keywords = {distant n -gram,mis-recognition,n -gram,out-of-vocabulary,spoken term detection,syllable recognition},
month = {mar},
number = {3},
pages = {470--485},
title = {{A robust/fast spoken term detection method based on a syllable n-gram index with a distance metric}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167639312001392},
volume = {55},
year = {2013}
}
@book{book:garland,
annote = {$\backslash$url{\{}http://glnd.alexanderstreet.com/{\}}},
author = {Nettl, Bruno and Stone, Ruth M and Porter, James and (Hrsg.), Timothy Rice},
publisher = {Garland},
title = {{The Garland Encyclopedia of World Music}},
year = {1998}
}
@inproceedings{Ng2012,
author = {Ng, Tim and Zhang, Bing and Nguyen, Long and Matsoukas, Spyros and Zhou, Xinhui and Mesgarani, Nima and Vesel, Karel and Matˇ, Pavel},
booktitle = {INTERSPEECH},
file = {:Users/anna/Papers/ng{\_}interspeech2012{\_}1052{\_}pp{\_}1{\_}4.pdf:pdf},
pages = {3--6},
title = {{Developing a Speech Activity Detection System for the DARPA RATS Program}},
year = {2012}
}
@article{Ni2012,
abstract = {We present a new system for the harmonic analysis of popular musical audio. It is focused on chord estimation, although the proposed system additionally estimates the key sequence and bass notes. It is distinct from competing approaches in two main ways. First, it makes use of a new improved chromagram repre- sentation of audio that takes the human perception of loudness into account. Furthermore, it is the first system for joint estima- tion of chords, keys, and bass notes that is fully based on machine learning, requiring no expert knowledge to tune the parameters. This means that it will benefit from future increases in available annotated audio files, broadening its applicability to a wider range of genres. In all of three evaluation scenarios, including a new one that allows evaluation on audio for which no complete ground truth annotation is available, the proposed system is shown to be faster, more memory efficient, and more accurate than the state-of-the- art.},
archivePrefix = {arXiv},
arxivId = {arXiv:1107.4969v1},
author = {Ni, Yizhao and McVicar, Matt and Santos-Rodriguez, Ral and {De Bie}, Tijl},
doi = {10.1109/TASL.2012.2188516},
eprint = {arXiv:1107.4969v1},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Ni et al. - 2012 - An end-to-end machine learning system for harmonic analysis of music.pdf:pdf},
issn = {15587916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {Audio chord estimation,harmony progression analyzer (HPA),loudness-based chromagram,machine learning,meta-song evaluation},
number = {6},
pages = {1771--1783},
title = {{An end-to-end machine learning system for harmonic analysis of music}},
volume = {20},
year = {2012}
}
@article{Norouzian2012,
author = {Norouzian, Atta and Jansen, Aren},
file = {:Users/anna/Thesis/Papers/ppm{\_}std.pdf:pdf},
journal = {{\ldots}},
pages = {2--5},
title = {{Exploiting Discriminative Point Process Models for Spoken Term Detection.}},
url = {http://20.210-193-52.unknown.qala.com.sg/archive/archive{\_}papers/interspeech{\_}2012/i12{\_}2442.pdf},
year = {2012}
}
@article{Novotney2010,
author = {Novotney, Scott and Callison-Burch, C},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Novotney, Callison-Burch - 2010 - Cheap, fast and good enough Automatic speech recognition with non-expert transcription.pdf:pdf},
journal = {{\ldots} : The 2010 Annual Conference of the {\ldots}},
number = {June},
pages = {207--215},
title = {{Cheap, fast and good enough: Automatic speech recognition with non-expert transcription}},
url = {http://dl.acm.org/citation.cfm?id=1858023},
year = {2010}
}
@article{Ondel2012,
author = {Ondel, Lucas},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Ondel - 2012 - Accoustic Keyword Spotting with Kaldi Toolkit.pdf:pdf},
title = {{Accoustic Keyword Spotting with Kaldi Toolkit}},
year = {2012}
}
@inproceedings{inproceedings:separation_harmonic_percussive,
author = {Ono, Nobutaka and Miyamoto, Kenichi and Roux, Jonathan Le and Kameoka, Hirokazu and Sagayama, Shigeki},
booktitle = {Proceedings of the 16th European Signal Processing Conference (EUSIPCO)},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Ono et al. - 2008 - Separation of a monaural audio signal into harmonicpercussive components by complementary diffusion on spectrogram.pdf:pdf},
month = {aug},
title = {{Separation of a monaural audio signal into harmonic/percussive components by complementary diffusion on spectrogram}},
year = {2008}
}
@article{Orio2006,
author = {Orio, Nicola},
doi = {10.1561/1500000002},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Orio - 2006 - Music Retrieval A Tutorial and Review.pdf:pdf},
issn = {1554-0669},
journal = {Foundations and Trends{\textregistered} in Information Retrieval},
number = {1},
pages = {1--96},
title = {{Music Retrieval: A Tutorial and Review}},
url = {http://www.nowpublishers.com/product.aspx?product=INR{\&}doi=1500000002},
volume = {1},
year = {2006}
}
@article{Ourania2008,
author = {Ourania, Vagia},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Ourania - 2008 - Singing Phoneme Class Detection In Polyphonic Music Recordings.pdf:pdf},
number = {September},
title = {{Singing Phoneme Class Detection In Polyphonic Music Recordings}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Singing+Phoneme+Class+Detection+In+Polyphonic+Music+Recordings{\#}0},
year = {2008}
}
@inproceedings{inproceedings:western_taxonomy,
author = {Pachet, Francois and Cazaly, Daniel},
booktitle = {Content-Based Multimedia Information Access Conference (RIAO)},
title = {{A Taxonomy of Musical Genres}},
year = {2000}
}
@article{Palaz2013,
abstract = {Most phoneme recognition state-of-the-art systems rely on a classical neural network classifiers, fed with highly tuned features, such as MFCC or PLP features. Recent advances in ``deep learning'' approaches questioned such systems, but while some attempts were made with simpler features such as spectrograms, state-of-the-art systems still rely on MFCCs. This might be viewed as a kind of failure from deep learning approaches, which are often claimed to have the ability to train with raw signals, alleviating the need of hand-crafted features. In this paper, we investigate a convolutional neural network approach for raw speech signals. While convolutional architectures got tremendous success in computer vision or text processing, they seem to have been let down in the past recent years in the speech processing field. We show that it is possible to learn an end-to-end phoneme sequence classifier system directly from raw signal, with similar performance on the TIMIT and WSJ datasets than existing systems based on MFCC, questioning the need of complex hand-crafted features on large datasets.},
archivePrefix = {arXiv},
arxivId = {1312.2137},
author = {Palaz, Dimitri and Collobert, Ronan and -Doss, Mathew Magimai.},
eprint = {1312.2137},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Palaz, Collobert, -Doss - 2013 - End-to-end Phoneme Sequence Recognition using Convolutional Neural Networks.pdf:pdf},
pages = {2--9},
title = {{End-to-end Phoneme Sequence Recognition using Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1312.2137},
year = {2013}
}
@article{Palaz2013a,
abstract = {In hybrid hidden Markov model/artificial neural networks (HMM/ANN) automatic speech recognition (ASR) system, the phoneme class conditional probabilities are estimated by first extracting acoustic features from the speech signal based on prior knowledge such as, speech perception or/and speech pro- duction knowledge, and, then modeling the acoustic features with an ANN. Recent advances in machine learning techniques, more specifically in the field of image processing and text pro- cessing, have shown that such divide and conquer strategy (i.e., separating feature extraction and modeling steps) may not be necessary. Motivated from these studies, in the framework of convolutional neural networks (CNNs), this paper investigates a novel approach, where the input to the ANN is raw speech signal and the output is phoneme class conditional probabil- ity estimates. On TIMIT phoneme recognition task, we study different ANN architectures to show the benefit of CNNs and compare the proposed approach against conventional approach where, spectral-based feature MFCC is extracted and modeled by a multilayer perceptron. Our studies show that the proposed approach can yield comparable or better phoneme recognition performance when compared to the conventional approach. It indicates that CNNs can learn features relevant for phoneme classification automatically from the raw speech signal. Copyright {\textcopyright} 2013 ISCA.},
archivePrefix = {arXiv},
arxivId = {arXiv:1304.1018v2},
author = {Palaz, Dimitri and Collobert, Ronan and Magimai, Mathew and Epfl, De Lausanne},
eprint = {arXiv:1304.1018v2},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Palaz et al. - 2013 - Estimating Phoneme Class Conditional Probabilities from Raw Speech Signal using Convolutional Neural Networks.pdf:pdf},
journal = {Interspeech},
keywords = {[Electronic Manuscript]},
number = {August},
pages = {1766--1770},
title = {{Estimating Phoneme Class Conditional Probabilities from Raw Speech Signal using Convolutional Neural Networks}},
year = {2013}
}
@article{Palaz2015,
abstract = {Automatic speech recognition systems typically model the rela-tionship between the acoustic speech signal and the phones in two separate steps: feature extraction and classifier training. In our recent works, we have shown that, in the framework of con-volutional neural networks (CNN), the relationship between the raw speech signal and the phones can be directly modeled and ASR systems competitive to standard approach can be built. In this paper, we first analyze and show that, between the first two convolutional layers, the CNN learns (in parts) and models the phone-specific spectral envelope information of 2-4 ms speech. Given that we show that the CNN-based approach yields ASR trends similar to standard short-term spectral based ASR sys-tem under mismatched (noisy) conditions, with the CNN-based approach being more robust.},
author = {Palaz, Dimitri and Magimai, Mathew and Collobert, Ronan},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Palaz, Magimai, Collobert - 2015 - Analysis of CNN-based Speech Recognition System using Raw Speech as Input.pdf:pdf},
journal = {INTERSPEECH},
pages = {3--7},
title = {{Analysis of CNN-based Speech Recognition System using Raw Speech as Input}},
year = {2015}
}
@inproceedings{inproceedings:genre_multilinear,
author = {Panagakis, Ioannis and Benetos, Emmanouil and Kotropoulos, Constantine},
booktitle = {9th International Society for Music Information Retrieval Conference (ISMIR)},
pages = {583--588},
title = {{Music Genre Classification: A Multilinear Approach}},
year = {2008}
}
@inproceedings{Panchapagesan2016,
author = {Panchapagesan, Sankaran and Sun, Ming and Khare, Aparna and Matsoukas, Spyros and Mandal, Arindam},
booktitle = {Interspeech},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Panchapagesan et al. - 2016 - Multi-task learning and Weighted Cross-entropy for DNN-based Keyword Spotting.pdf:pdf},
keywords = {allel - the main,and an auxiliary task,dnn acoustic model is,in par-,keyword-specific phone,leads to comparable accuracy,of predicting lvcsr senones,over,show that multi-task learning,states,task of predicting the,trained with two tasks,we},
pages = {760--764},
title = {{Multi-task learning and Weighted Cross-entropy for DNN-based Keyword Spotting}},
year = {2016}
}
@article{Parikh2005,
author = {Parikh, Gaurang and Loizou, Philipos C.},
doi = {10.1121/1.2118407},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Parikh, Loizou - 2005 - The influence of noise on vowel and consonant cues.pdf:pdf},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
number = {6},
pages = {3874},
title = {{The influence of noise on vowel and consonant cues}},
url = {http://scitation.aip.org/content/asa/journal/jasa/118/6/10.1121/1.2118407},
volume = {118},
year = {2005}
}
@article{article:pulse_salience,
author = {Parncutt, Richard},
journal = {Music Perception},
number = {4},
pages = {409--464},
title = {{A Perceptual Model of Pulse Salience and Metrical Accent in Musical Rhythms}},
volume = {11},
year = {1994}
}
@article{Peche2007,
abstract = {We investigate the addition of a new language, for which limited resources are available, to a phonotactic language identification system. Two classes of approaches are studied: in the first class, only existing phonetic recognizers are employed, whereas an additional phonetic recognizer in the new language is created for the second class. It is found that the number of acoustic recognizers employed plays a crucial role in determining the recognition accuracy for the new language. We study different approaches to incorporating a language for which audio-only data is available (no pronunciation dictionaries or transcriptions) and find that if more than about 2 000 training utterances are available, a bootstrapped acoustic model for the new language can improve accuracy substantially.},
author = {Peche, M. and Davel, M. and Barnard, E.},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Peche, Davel, Barnard - 2007 - Phonotactic spoken language identification with limited training data.pdf:pdf},
isbn = {9781605603162},
journal = {International Speech Communication Association - 8th Annual Conference of the International Speech Communication Association, Interspeech 2007},
keywords = {Generalization,Resource scarce languages,Spoken language identification},
pages = {1661--1664},
title = {{Phonotactic spoken language identification with limited training data}},
volume = {3},
year = {2007}
}
@techreport{techreport:cuidado,
author = {Peeters, Geoffroy},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Peeters - 2004 - A large set of audio features for sound description (similarity and classification) in the CUIDADO project.pdf:pdf},
institution = {CUIDADO I.S.T. project},
publisher = {IRCAM},
title = {{A large set of audio features for sound description (similarity and classification) in the CUIDADO project}},
year = {2004}
}
@inproceedings{inproceedings:irmfsp,
author = {Peeters, Geoffroy and Rodet, Xavier},
booktitle = {Proc. of the 6th Int. Conference on Digital Audio Effects (DAFX-03)},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Peeters, Rodet - 2003 - Hierarchical Gaussian Tree with Inertia Ratio Maximization for the classification of large musical instrument da.pdf:pdf},
month = {sep},
pages = {1--6},
title = {{Hierarchical Gaussian Tree with Inertia Ratio Maximization for the classification of large musical instrument databases}},
year = {2003}
}
@article{Pellegrino2000,
author = {Pellegrino, F and Andr{\'{e}}-Obrecht, R},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Pellegrino, Andr{\'{e}}-Obrecht - 2000 - Automatic language identification an alternative approach to phonetic modelling.pdf:pdf},
journal = {Signal Processing},
pages = {1231--1244},
title = {{Automatic language identification : an alternative approach to phonetic modelling}},
url = {http://www.sciencedirect.com/science/article/pii/S0165168400000323},
volume = {80},
year = {2000}
}
@inproceedings{jhu_ks2,
author = {Pinto, J and Lovitt, A and Hermansky, Hynek},
booktitle = {Proceedings of the International Conference on Spoken Language Processing},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Pinto, Lovitt, Hermansky - 2007 - Exploiting Phoneme Similarities in Hybrid {\{}HMM-ANN{\}} Keyword Spotting.pdf:pdf},
title = {{Exploiting Phoneme Similarities in Hybrid {\{}HMM-ANN{\}} Keyword Spotting}},
year = {2007}
}
@article{Pinto2008,
author = {Pinto, Joel and Szoke, Igor and Prasanna, SRM and Hermansky, Hynek},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Pinto et al. - 2008 - Fast approximate spoken term detection from sequence of phonemes.pdf:pdf},
journal = {Proc. SIGIR},
title = {{Fast approximate spoken term detection from sequence of phonemes}},
url = {ftp://ftp.idiap.ch/pub/reports/2008/pinto-idiap-rr-08-45.pdf},
year = {2008}
}
@article{article:roughness_critical_bands,
author = {Plomp, R and Levelt, W J M},
journal = {Journal of the Acoustical Society of America},
pages = {548--560},
title = {{Tonal Consonance and Critical Bandwidth}},
volume = {38},
year = {1965}
}
@inproceedings{inproceedings:h2a,
author = {Pohle, Tim and Knees, Peter and Seyerlehner, Klaus and Widmer, Gerhard},
booktitle = {13th Int. Conference on Digital Audio Effects (DAFx-10)},
month = {sep},
title = {{A High-Level Audio Feature for Music Retrieval and Sorting}},
year = {2010}
}
@inproceedings{events_portelo,
address = {Taipei, Taiwan},
author = {Portelo, Jos{\'{e}} and Bugalho, Miguel and Trancoso, Isabel and Neto, Jo{\~{a}}o Paulo and Abad, Alberto and Serralheiro, Ant{\'{o}}nio},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
pages = {1973--1976},
publisher = {IEEE},
title = {{Non-speech audio event detection}},
year = {2009}
}
@article{Povey,
author = {Povey, Daniel},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Povey - Unknown - Kaldi Lecture 1.pdf:pdf},
title = {{Kaldi Lecture 1}}
}
@article{Povey2006,
author = {Povey, Daniel},
doi = {10.1002/0470012609.ch21},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Povey - 2006 - Kaldi Lecture 3.pdf:pdf},
isbn = {9780470855386},
month = {jan},
pages = {183--188},
title = {{Kaldi Lecture 3}},
year = {2006}
}
@article{Povey2006a,
author = {Povey, Daniel},
doi = {10.1002/0470012609.ch21},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Povey - 2006 - Kaldi Lecture 2.pdf:pdf},
isbn = {9780470855386},
month = {jan},
pages = {183--188},
title = {{Kaldi Lecture 2}},
year = {2006}
}
@article{Povey2006b,
author = {Povey, Daniel},
doi = {10.1002/0470012609.ch21},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Povey - 2006 - Kaldi Lecture 4.pdf:pdf},
isbn = {9780470855386},
month = {jan},
pages = {183--188},
title = {{Kaldi Lecture 4}},
year = {2006}
}
@article{Povey2011,
author = {Povey, Daniel and Ghoshal, Arnab},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Povey, Ghoshal - 2011 - The kaldi speech recognition toolkit.pdf:pdf},
journal = {{\ldots} speech recognition {\ldots}},
title = {{The kaldi speech recognition toolkit}},
url = {http://www.researchgate.net/publication/228828379{\_}The{\_}Kaldi{\_}speech{\_}recognition{\_}toolkit/file/79e4150743dc6ce65c.pdf},
year = {2011}
}
@article{Prabhavalkar2013,
author = {Prabhavalkar, Rohit},
file = {:Users/anna/Thesis/Papers/prabhavalkar{\_}ICASSP2013.pdf:pdf},
journal = {{\ldots} , Speech and Signal {\ldots}},
title = {{Discriminative articulatory models for spoken term detection in low-resource conversational settings}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6639281},
year = {2013}
}
@article{article:vibrato_measurements,
author = {Prame, E},
journal = {STL-QPSR},
number = {4},
pages = {73--86},
title = {{Measurements of the vibrato rate of ten singers}},
volume = {33},
year = {1992}
}
@article{article:pressnitzer,
author = {Pressnitzer, Daniel and McAdams, Stephen},
journal = {J. Acoust. Soc. Am.},
month = {may},
number = {5},
pages = {2773--2782},
title = {{Two phase effects in roughness perception}},
volume = {105},
year = {1999}
}
@inproceedings{inproceedings:musical_memory,
author = {Proutskova, Polina},
booktitle = {Proc. of the Intl. Conf. on Music Information Retrieval (ISMIR)},
title = {{Musical Memory of the World - Data infrastructure in ethnomusicological archives}},
year = {2007}
}
@inproceedings{Pundak2016,
author = {Pundak, Golan and Sainath, Tara N},
booktitle = {Interspeech},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Pundak, Sainath - 2016 - Lower Frame Rate Neural Network Acoustic Models.pdf:pdf},
keywords = {[Electronic Manuscript]},
pages = {22--26},
title = {{Lower Frame Rate Neural Network Acoustic Models}},
year = {2016}
}
@inproceedings{Pylkkonen2003,
author = {Pylkk{\"{o}}nen, J. and Kurimo, M.},
booktitle = {Proceedings of European Conf. on Speech Technology (EUROSPEECH)},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Pylkk{\"{o}}nen, Kurimo - 2003 - Duration modeling techniques for continuous speech recognition.pdf:pdf},
title = {{Duration modeling techniques for continuous speech recognition}},
year = {2003}
}
@article{Rabiner1989,
author = {Rabiner, L},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Rabiner - 1989 - A tutorial on hidden Markov models and selected applications in speech recognition.pdf:pdf},
journal = {Proceedings of the IEEE},
title = {{A tutorial on hidden Markov models and selected applications in speech recognition}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=18626},
year = {1989}
}
@article{Rabiner1989a,
author = {Rabiner, L and Wilpon, JG and Soong, FK},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Rabiner, Wilpon, Soong - 1989 - High performance connected digit recognition using hidden Markov models.pdf:pdf},
journal = {Acoustics, Speech and {\ldots}},
title = {{High performance connected digit recognition using hidden Markov models}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=31269},
year = {1989}
}
@article{Radford2015,
abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
archivePrefix = {arXiv},
arxivId = {1511.06434},
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
doi = {10.1051/0004-6361/201527329},
eprint = {1511.06434},
file = {:Users/anna/Work/Interspeech16/whitepaper/radford{\_}adversarial{\_}16.pdf:pdf},
issn = {0004-6361},
journal = {arXiv},
pages = {1--15},
title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1511.06434},
year = {2015}
}
@article{Razzazi2007,
author = {Razzazi, Farbod and Sayadiyan, Abolghasem},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Razzazi, Sayadiyan - 2007 - Evaluation of soft segment modeling on a context independent phoneme classification system.pdf:pdf},
journal = {Arabian Journal for Science and {\ldots}},
number = {1},
pages = {49--65},
title = {{Evaluation of soft segment modeling on a context independent phoneme classification system}},
url = {http://ajse.kfupm.edu.sa/articles/321b{\_}p.05.pdf},
volume = {32},
year = {2007}
}
@inproceedings{inproceedings:singing_voice_detection,
author = {Regnier, Lise and Peeters, Geoffroy},
booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Regnier, Peeters - 2009 - Singing voice detection in music tracks using direct voice vibrato detection.pdf:pdf},
pages = {1685--1688},
title = {{Singing voice detection in music tracks using direct voice vibrato detection}},
year = {2009}
}
@article{Renals,
author = {Renals, Steve},
file = {:Users/anna/Thesis/Papers/multi-domain{\_}acoustic.pdf:pdf},
title = {{Multi-domain acoustic modelling for speech recognition}}
}
@inproceedings{Renals2014,
abstract = {Distant conversational speech recognition is challenging owing to the presence of multiple, overlapping talkers, additional non-speech acoustic sources, and the effects of reverberation. In this paper we review work on distant speech recognition, with an emphasis on approaches which combine multichannel signal processing with acoustic modelling, and investigate the use of hybrid neural network / hidden Markov model acoustic models for distant speech recognition of meetings recorded using microphone arrays. In particular we investigate the use of convolutional and fully-connected neural networks with different activation functions (sigmoid, rectiﬁed linear, and maxout). We performed experiments on the AMI and ICSI meeting corpora, with results indicating that neural network models are capable of signiﬁcant improvements in accuracy compared with discriminatively trained Gaussian mixture models.},
author = {Renals, Steve and Swietojanski, Pawel},
booktitle = {2014 4th Joint Workshop on Hands-Free Speech Communication and Microphone Arrays, HSCMA 2014},
doi = {10.1109/HSCMA.2014.6843274},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Renals, Swietojanski - 2014 - Neural networks for distant speech recognition.pdf:pdf},
isbn = {9781479931095},
keywords = {AMI corpus,ICSI corpus,beam-forming,convolutional neural networks,distant speech recognition,maxout networks,meetings,rectifier unit},
pages = {172--176},
title = {{Neural networks for distant speech recognition}},
year = {2014}
}
@article{Reynolds2000,
author = {Reynolds, Douglas a. and Quatieri, Thomas F. and Dunn, Robert B.},
doi = {10.1006/dspr.1999.0361},
file = {:Users/anna/Papers/Speaker Verification Using Adapted Gaussain Mixture Models.pdf:pdf},
issn = {10512004},
journal = {Digital Signal Processing},
keywords = {gaussian mixture models,handset normalization,likelihood,nist,ratio detector,speaker recognition,universal background model},
month = {jan},
number = {1-3},
pages = {19--41},
title = {{Speaker Verification Using Adapted Gaussian Mixture Models}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1051200499903615},
volume = {10},
year = {2000}
}
@article{Rigaud2016,
author = {Rigaud, Francois and Radenen, Mathieu},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Rigaud, Radenen - 2016 - Singing Voice Melody Transcription using Deep Neural Networks.pdf:pdf},
journal = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)2},
pages = {737--743},
title = {{Singing Voice Melody Transcription using Deep Neural Networks}},
year = {2016}
}
@article{Rohlicek1989,
author = {Rohlicek, JR and Russell, W},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Rohlicek, Russell - 1989 - Continuous hidden Markov modeling for speaker-independent word spotting.pdf:pdf},
journal = {Acoustics, Speech, and {\ldots}},
pages = {627--630},
title = {{Continuous hidden Markov modeling for speaker-independent word spotting}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=266505},
year = {1989}
}
@inproceedings{rose_paul,
author = {Rose, R C and Paul, D B},
booktitle = {Proc. of the IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP)},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Rose, Paul - 1990 - A hidden markov model based keyword recognition system.pdf:pdf},
pages = {129--132},
title = {{A hidden markov model based keyword recognition system}},
year = {1990}
}
@article{Rouvier2008,
author = {Rouvier, M},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Rouvier - 2008 - On-the-fly term spotting by phonetic filtering and request-driven decoding.pdf:pdf},
isbn = {9781424434725},
journal = {{\ldots} Workshop, 2008. SLT {\ldots}},
pages = {305--308},
title = {{On-the-fly term spotting by phonetic filtering and request-driven decoding}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4777901},
year = {2008}
}
@article{Russell1985,
author = {Russell, M and Moore, R},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Russell, Moore - 1985 - Explicit modelling of state occupancy in hidden Markov models for automatic speech recognition.pdf:pdf},
journal = {{\ldots} , Speech, and Signal Processing, IEEE {\ldots}},
number = {1},
pages = {5--8},
title = {{Explicit modelling of state occupancy in hidden Markov models for automatic speech recognition}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1168477},
year = {1985}
}
@inproceedings{Russell1987,
author = {Russell, Martin J and Cook, Anneliese E},
booktitle = {ICASSP},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Russell, Cook - 1987 - Experimental evaluation of duration modelling techniques for automatic speech recognition.pdf:pdf},
title = {{Experimental evaluation of duration modelling techniques for automatic speech recognition}},
year = {1987}
}
@article{Sainath2013,
abstract = {Convolutional Neural Networks (CNNs) are an alternative type of neural network that can be used to reduce spectral variations and model spectral correlations which exist in signals. Since speech signals exhibit both of these properties, CNNs are a more effective model for speech compared to Deep Neural Networks (DNNs). In this paper, we explore applying CNNs to large vocabulary speech tasks. First, we determine the appropriate architecture to make CNNs effective compared to DNNs for LVCSR tasks. Specifically, we focus on how many convolutional layers are needed, what is the optimal number of hidden units, what is the best pooling strategy, and the best input feature type for CNNs. We then explore the be- havior of neural network features extracted from CNNs on a vari- ety of LVCSR tasks, comparing CNNs to DNNs and GMMs. We find that CNNs offer between a 13-30{\%} relative improvement over GMMs, and a 4-12{\%} relative improvement over DNNs, on a 400-hr Broadcast News and 300-hr Switchboard task.},
archivePrefix = {arXiv},
arxivId = {1309.1501},
author = {Sainath, Tara N. and Mohamed, Abdel Rahman and Kingsbury, Brian and Ramabhadran, Bhuvana},
doi = {10.1109/ICASSP.2013.6639347},
eprint = {1309.1501},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Sainath et al. - 2013 - Deep convolutional neural networks for LVCSR.pdf:pdf},
isbn = {9781479903566},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Neural Networks,Speech Recognition},
pages = {8614--8618},
title = {{Deep convolutional neural networks for LVCSR}},
year = {2013}
}
@article{Sak2014,
abstract = {Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.},
archivePrefix = {arXiv},
arxivId = {1402.1128},
author = {Sak, Haşim and Senior, Andrew and Beaufays, Fran{\c{c}}oise},
doi = {arXiv:1402.1128},
eprint = {1402.1128},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Sak, Senior, Beaufays - 2014 - Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recogniti.pdf:pdf},
journal = {Neural and Evolutionary Computing},
number = {Cd},
title = {{Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition}},
url = {http://arxiv.org/abs/1402.1128},
year = {2014}
}
@article{Sakamoto2013,
author = {Sakamoto, Nagisa and Nakagawa, Seiichi},
doi = {10.1109/APSIPA.2013.6694366},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Sakamoto, Nakagawa - 2013 - Robustfast out-of-vocabulary spoken term detection by N-gram index with exact distance through textspeech in.pdf:pdf},
isbn = {978-986-90006-0-4},
journal = {2013 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference},
month = {oct},
pages = {1--4},
publisher = {Apsipa},
title = {{Robust/fast out-of-vocabulary spoken term detection by N-gram index with exact distance through text/speech input}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6694366},
year = {2013}
}
@article{Sangwan2010,
author = {Sangwan, A and Mehrabani, M and Hansen, J H L},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Sangwan, Mehrabani, Hansen - 2010 - Automatic language analysis and identification based on speech production knowledge.pdf:pdf},
isbn = {9781424442966},
journal = {Acoustics Speech and {\ldots}},
number = {Cv},
pages = {5006--5009},
title = {{Automatic language analysis and identification based on speech production knowledge}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5495066},
year = {2010}
}
@article{Sargent2011,
author = {Sargent, G and Bimbot, F and Vincent, Emmanuel},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Sargent, Bimbot, Vincent - 2011 - A regularity-constrained Viterbi algorithm and its application to the structural segmentation of songs.pdf:pdf},
journal = {International Society for Music {\ldots}},
number = {Ismir},
pages = {483--488},
title = {{A regularity-constrained Viterbi algorithm and its application to the structural segmentation of songs}},
url = {http://hal.inria.fr/inria-00616274/},
year = {2011}
}
@article{Savic1991,
author = {Savic, Michael and Acosta, Elena and Gupta, SK},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Savic, Acosta, Gupta - 1991 - An automatic language identification system.pdf:pdf},
isbn = {2977719110000},
journal = {Acoustics, Speech, and Signal {\ldots}},
pages = {817--820},
title = {{An automatic language identification system}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=150462},
year = {1991}
}
@article{article:genre_survey,
author = {Scaringella, Nicolas and Zoia, Giorgio and Mlynek, Daniel},
journal = {IEEE Signal Processing Magazine},
number = {2},
pages = {133--141},
title = {{Automatic genre classification of music content: a survey}},
volume = {23},
year = {2006}
}
@techreport{scheirer,
author = {Scheirer, E},
institution = {Machine Listening Group, MIT Media Laboratory},
title = {{Tempo and beat analysis of acoustic musical signals}},
year = {1996}
}
@article{article:scheirer,
author = {Scheirer, Eric D},
journal = {J. Acoust. Soc. Am.},
month = {jan},
number = {1},
pages = {588--601},
title = {{Tempo and beat analysis of acoustic musical signals}},
volume = {103},
year = {1998}
}
@article{article:children_implicit_harmony,
author = {Schellenberg, E Glenn and Bigand, Emmanuel and Poulin-Charronnat, Benedicte and Garnier, C�cilia and Stevens, Catherine},
journal = {Developmental Science},
number = {6},
pages = {551--566},
title = {{Children's implicit knowledge of harmony in Western music}},
volume = {8},
year = {2005}
}
@phdthesis{phdthesis:automatic_transcription,
address = {Stanford, CA, USA},
author = {Schloss, W Andrew},
school = {Stanford University, Department of Music},
title = {{On the automatic transcription of percussive music - from acoustic signal to high-level analysis}},
year = {1985}
}
@article{Schneider2008,
author = {Schneider, D and Schon, J and Eickeler, S},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Schneider, Schon, Eickeler - 2008 - Towards large scale vocabulary independent spoken term detection advances in the Fraunhofer IAIS aud.pdf:pdf},
isbn = {9789036526975},
journal = {{\ldots} Spontaneous Conversational Speech {\ldots}},
number = {July},
title = {{Towards large scale vocabulary independent spoken term detection: advances in the Fraunhofer IAIS audiomining system}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?rep=rep1{\&}type=pdf{\&}doi=10.1.1.216.3563{\#}page=39},
year = {2008}
}
@article{Schon2005,
abstract = {One approach to comparing the neural bases of language and music is through the use of song, which is a unique and ecological combination of these two cognitive domains. In song, language and music are merged into one acoustic signal with two salient dimensions. By manipulating either the linguistic or musical dimensions (or both) of song and studying their relationships, it is possible to gain important information about the neural networks underlying language and music cognition. We will present a brief review followed by recent behavioral, electrophysiological, and neuroimaging studies concerned with the functional and structural relationships of music and language. These results, together with the previous studies in the field, help understanding whether the different levels of music and language processing are independent or interactive.},
author = {Sch{\"{o}}n, Daniele and Gordon, Reyna Leigh and Besson, Mireille},
doi = {10.1196/annals.1360.006},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Sch{\"{o}}n, Gordon, Besson - 2005 - Musical and linguistic processing in song perception.pdf:pdf},
issn = {0077-8923},
journal = {Annals of the New York Academy of Sciences},
keywords = {Acoustic Stimulation,Auditory Perception,Brain,Brain Mapping,Brain: anatomy {\&} histology,Brain: pathology,Cognition,Electrophysiology,Humans,Language,Linguistics,Magnetic Resonance Imaging,Music,Pitch Discrimination,Pitch Perception},
month = {dec},
pages = {71--81},
pmid = {16597752},
title = {{Musical and linguistic processing in song perception.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16597752},
volume = {1060},
year = {2005}
}
@article{Schwarz2004,
author = {Schwarz, Petr and Mat{\v{e}}jka, P and {\v{C}}ernock{\'{y}}, J},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Schwarz, Mat{\v{e}}jka, {\v{C}}ernock{\'{y}} - 2004 - Towards lower error rates in phoneme recognition.pdf:pdf},
journal = {Text, Speech and Dialogue},
title = {{Towards lower error rates in phoneme recognition}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-30120-2{\_}59},
year = {2004}
}
@inproceedings{schwenninger,
address = {Victoria, Canada},
author = {Schwenninger, J and Brueckner, R and Willett, D and Hennecke, M E},
booktitle = {7th International Conference on Music Information Retrieval (ISMIR)},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Schwenninger et al. - 2006 - Language Identification in Vocal Music.pdf:pdf},
pages = {377--379},
title = {{Language Identification in Vocal Music}},
year = {2006}
}
@article{Seide2004,
author = {Seide, Frank and Yu, Peng and Ma, Chengyuan and Chang, Eric},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Seide et al. - 2004 - Vocabulary-independent search in spontaneous speech.pdf:pdf},
isbn = {0780384849},
journal = {Acoustics, Speech, and Signal {\ldots}},
number = {49},
pages = {253--256},
title = {{Vocabulary-independent search in spontaneous speech}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1325970},
year = {2004}
}
@article{Seltzer2013,
abstract = {In this paper we demonstrate how to improve the performance of deep neural network (DNN) acoustic models using multi-task learning. In multi-task learning, the network is trained to perform both the primary classification task and one or more secondary tasks using a shared representation. The additional model parameters associated with the secondary tasks represent a very small increase in the number of trained parameters, and can be discarded at runtime. In this paper, we explore three natural choices for the secondary task: the phone label, the phone context, and the state context. We demonstrate that, even on a strong baseline, multi-task learning can provide a significant decrease in error rate. Using phone context, the phonetic error rate (PER) on TIMIT is reduced from 21.63{\%} to 20.25{\%} on the core test set, and surpassing the best performance in the literature for a DNN that uses a standard feed-forward network architecture.},
author = {Seltzer, Michael L. and Droppo, Jasha},
doi = {10.1109/ICASSP.2013.6639012},
file = {:Users/anna/Work/Interspeech16/whitepaper/seltzer{\_}multitask{\_}13.pdf:pdf},
isbn = {9781479903566},
journal = {Icassp},
pages = {6965--6969},
title = {{Multi-Task Learning in Deep Neural Networks for Improved Phoneme Recognition}},
year = {2013}
}
@article{Senior2015,
author = {Senior, Andrew and Sak, Hasim and {de Chaumont Quitry}, Felix and Sainath, Tara and Rao, Kanishka},
file = {:Users/anna/Work/Interspeech16/whitepaper/senior{\_}ctc{\_}lstm{\_}15.pdf:pdf},
isbn = {9781479972913},
journal = {2015 IEEE Workshop on Automatic {\ldots}},
pages = {604--609},
title = {{Acoustic modelling with CD-CTC-SMBR LSTM RNNS}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=7404851},
year = {2015}
}
@book{book:sethares,
author = {Sethares, William A},
publisher = {Springer Verlag},
title = {{Tuning, Timbre, Spectrum, Scale}},
year = {1998}
}
@inproceedings{Shen2016,
abstract = {Recurrent neural network architectures combining with attention mechanism, or neural attention model, have shown promising performance recently for the tasks including speech recognition, image caption generation, visual question answering and machine translation. In this paper, neural attention model is applied on two sequence classification tasks, dialogue act detection and key term extraction. In the sequence labeling tasks, the model input is a sequence, and the output is the label of the input sequence. The major difficulty of sequence labeling is that when the input sequence is long, it can include many noisy or irrelevant part. If the information in the whole sequence is treated equally, the noisy or irrelevant part may degrade the classification performance. The attention mechanism is helpful for sequence classification task because it is capable of highlighting important part among the entire sequence for the classification task. The experimental results show that with the attention mechanism, discernible improvements were achieved in the sequence labeling task considered here. The roles of the attention mechanism in the tasks are further analyzed and visualized in this paper.},
archivePrefix = {arXiv},
arxivId = {1604.00077},
author = {Shen, Sheng-syun and Lee, Hung-yi},
booktitle = {Interspeech},
eprint = {1604.00077},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Shen, Lee - 2016 - Neural Attention Models for Sequence Classification Analysis and Application to Key Term Extraction and Dialogue Act.pdf:pdf},
keywords = {[Electronic Manuscript]},
pages = {2716--2720},
title = {{Neural Attention Models for Sequence Classification: Analysis and Application to Key Term Extraction and Dialogue Act Detection}},
url = {http://arxiv.org/abs/1604.00077},
year = {2016}
}
@inproceedings{Shinohara2016,
author = {Shinohara, Yusuke},
booktitle = {Interspeech},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Shinohara - 2016 - Adversarial Multi-task Learning of Deep Neural Networks for Robust Speech Recognition.pdf:pdf},
keywords = {[Electronic Manuscript]},
pages = {2369--2372},
title = {{Adversarial Multi-task Learning of Deep Neural Networks for Robust Speech Recognition}},
year = {2016}
}
@techreport{Shum2010,
author = {Shum, Stephen and Dehak, Najim and Glass, Jim},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Shum, Dehak, Glass - 2010 - Speaker Recognition The Total Variability Approach.pdf:pdf},
pages = {1--7},
title = {{Speaker Recognition: The Total Variability Approach}},
year = {2010}
}
@article{Sigtia2016,
abstract = {{We present a neural network model for polyphonic music transcription. The architecture of the proposed model is analogous to speech recognition systems and comprises an acoustic model and a music language mode{\}}. The acoustic model is a neural network used for estimating the probabilities of pitches in a frame of audio. The language model is a recurrent neural network that models the correlations between pitch combinations over time. The proposed model is general and can be used to transcribe polyphonic music without imposing any constraints on the polyphony or the number or type of instruments. The acoustic and language model predictions are combined using a probabilistic graphical model. Inference over the output variables is performed using the beam search algorithm. We investigate various neural network architectures for the acoustic models and compare their performance to two popular state-of-the-art acoustic models. We also present an efficient variant of beam search that improves performance and reduces run-times by an order of magnitude, making the model suitable for real-time applications. We evaluate the model's performance on the MAPS dataset and show that the proposed model outperforms state-of-the-art transcription systems.},
archivePrefix = {arXiv},
arxivId = {1508.01774},
author = {Sigtia, Siddharth and Benetos, Emmanouil and Dixon, Simon},
doi = {10.1109/TASLP.2016.2533858},
eprint = {1508.01774},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Sigtia, Benetos, Dixon - 2016 - An End-to-End Neural Network for Polyphonic Music Transcription.pdf:pdf},
issn = {2329-9290},
journal = {Ieee/Acmtransactions on Audio, Speech, and Language Processing},
number = {5},
pages = {1--13},
title = {{An End-to-End Neural Network for Polyphonic Music Transcription}},
url = {http://arxiv.org/abs/1508.01774},
volume = {24},
year = {2016}
}
@inproceedings{inproceedings:latin_ensemble_classifiers,
author = {{Silla Jr.}, Carlos N and Kaestner, Celso A A and Koerich, Alessandro L},
booktitle = {Proceedings of the IEEE International Conference on Systems, Man and Cybernetics},
month = {oct},
pages = {1687--1692},
title = {{Automatic music genre classification using ensemble of classifiers}},
year = {2007}
}
@inproceedings{singer,
address = {Geneva, Switzerland},
author = {Singer, E and Torres-Carrasquillo, P A and Gleason, T P and Campbell, W M and Reynolds, D A},
booktitle = {Proceedings of Eurospeech},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Singer et al. - 2003 - Acoustic, phonetic, and discriminative approaches to automatic language identification.pdf:pdf},
pages = {1345--1348},
title = {{Acoustic, phonetic, and discriminative approaches to automatic language identification}},
year = {2003}
}
@article{Siniscalchi2009,
author = {Siniscalchi, SM},
file = {:Users/anna/Thesis/Papers/0c96051ad0814c0fe1000000.pdf:pdf},
isbn = {9781424423545},
journal = {Acoustics, Speech and {\ldots}},
pages = {3865--3868},
title = {{A phonetic feature based lattice rescoting approach to LVCSR}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4960471},
year = {2009}
}
@article{Siniscalchi2014,
author = {Siniscalchi, SM and Lee, Chin-hui},
file = {:Users/anna/Thesis/Papers/11-133-3-PB.pdf:pdf},
journal = {Loquens},
keywords = {- los sistemas,artificial neural networks,atributos,autom{\'{a}}tico de habla y,autom{\'{a}}tico del habla basada,bajos {\'{i}}ndices de error,coincidencia de,de locutor suelen basarse,de reconocimiento para una,de trabajo,detecci{\'{o}}n de los atributos del habla,en la detecci{\'{o}}n de,en un sistema de,gracias a este modo,hidden Markov models,hidden markov models,knowledge-rich systems,modelos ocultos de Markov,m{\'{a}}s novedosos de reconocimiento,patrones,redes neuronales artificiales,resumen,se han obtenido unos,sistemas ricos en conocimientos,speech attribute detection,una estrategia de procesamiento,va-},
number = {January},
pages = {1--12},
title = {{An attribute detection based approach to automatic speech processing}},
url = {http://loquens.revistas.csic.es/index.php/loquens/article/viewArticle/11},
volume = {1},
year = {2014}
}
@article{Siniscalchi2009a,
author = {Siniscalchi, SM and Reed, Jeremy and Svendsen, Torbj{\o}rn and Lee, Chin-hui},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Siniscalchi et al. - 2009 - Exploring universal attribute characterization of spoken languages for spoken language recognition.pdf:pdf},
journal = {INTERSPEECH},
keywords = {[Electronic Manuscript]},
pages = {168--171},
title = {{Exploring universal attribute characterization of spoken languages for spoken language recognition.}},
url = {http://www.researchgate.net/publication/221490050{\_}Exploring{\_}universal{\_}attribute{\_}characterization{\_}of{\_}spoken{\_}languages{\_}for{\_}spoken{\_}language{\_}recognition/file/72e7e5185106f39028.pdf},
year = {2009}
}
@article{Slade2013,
author = {Slade, GW},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Slade - 2013 - The Viterbi algorithm demystified.pdf:pdf},
journal = {southernfriedsilicon.com},
title = {{The Viterbi algorithm demystified}},
url = {http://southernfriedsilicon.com/ViterbiAlgorithm121220.pdf},
year = {2013}
}
@article{Smith2013,
author = {Smith, Jc},
file = {:Users/anna/Downloads/jeffsmiththesis.pdf:pdf},
number = {December},
title = {{Correlation Analyses of Encoded Music Performance}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:CORRELATION+ANALYSES+OF+ENCODED+MUSIC+PERFORMANCE{\#}7},
year = {2013}
}
@misc{website:national_geographic,
annote = {$\backslash$url{\{}http://worldmusic.nationalgeographic.com/{\}}},
author = {Society, Online-Quelle: National Geographic},
title = {{Home: National Geographic World Music}}
}
@article{Soufifar2011,
author = {Soufifar, Mehdi and Kockmann, Marcel and Burget, L},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Soufifar, Kockmann, Burget - 2011 - iVector Approach to Phonotactic Language Recognition.pdf:pdf},
journal = {INTERSPEECH},
number = {August},
pages = {2913--2916},
title = {{iVector Approach to Phonotactic Language Recognition.}},
url = {http://www.fit.vutbr.cz/research/groups/speech/publi/2011/soufifar{\_}interspeech2011{\_}703.pdf},
year = {2011}
}
@article{Srikanth,
author = {Srikanth, Ronanki and Bo, Li and Salsman, James},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Srikanth, Bo, Salsman - Unknown - Automatic Pronunciation Evaluation And Mispronunciation Detection Using CMUSphinx.pdf:pdf},
number = {December 2012},
pages = {61--68},
title = {{Automatic Pronunciation Evaluation And Mispronunciation Detection Using CMUSphinx}}
}
@article{Sundberg1979,
abstract = {.This article reviews investigations of the singing voice which possess an interest from a perceptional point of view. Acoustic function, formant frequencies, phonation, pitch, and phrasing in singing are discussed. It is found that singing differs from speech in a highly adequate manner, e.g. for the purpose of increasing the loudness of the voice at a minimum cost as regards vocal effort. The terminology used for different types of voice timbre seems to depend on the properties and use of the voice organ rather than on specific acoustic signal characteristics.},
author = {Sundberg, J.},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Sundberg - 1979 - Quarterly Progress and Status Report Perception of singing.pdf:pdf},
keywords = {for speech,music and hearing,quarterly progress and,t},
pages = {1--48},
title = {{Quarterly Progress and Status Report: Perception of singing}},
year = {1979}
}
@misc{Sundberg2013,
abstract = {This chapter describes the function of the singing voice and reviews quantitative research which mainly has concerned Western classical styles. Phonatory and resonatory contributions to voice timbre as well as acoustic correlates of various perceptual voice qualities are discussed. Also described are various aspects of vibrato, intonation and musical expressiveness. Keywords phonation, formant, vibrato, intonation},
author = {Sundberg, Johan},
booktitle = {The Psychology of Music},
doi = {10.1016/B978-0-12-381460-9.00003-1},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Sundberg - 2013 - Perception of Singing.doc:doc},
isbn = {9780123814609},
pages = {69--105},
title = {{Perception of Singing}},
year = {2013}
}
@article{Sundermeyer2012,
abstract = {Neural networks have become increasingly popular for the task of language modeling. Whereas feed-forward networks only exploit a fixed context length to predict the next word of a se- quence, conceptually, standard recurrent neural networks can take into account all of the predecessor words. On the other hand, it is well known that recurrent networks are difficult to train and therefore are unlikely to show the full potential of re- current models. These problems are addressed by a the Long Short-Term Memory neural network architecture. In this work, we ana- lyze this type of network on an English and a large French language modeling task. Experiments show improvements of about 8 {\%} relative in perplexity over standard recurrent neural network LMs. In addition, we gain considerable improvements in WER on top of a state-of-the-art speech recognition system.},
author = {Sundermeyer, Martin and Schl{\"{u}}ter, Ralf and Ney, Hermann},
file = {:Users/anna/Work/Interspeech16/whitepaper/sundermeyer{\_}lstm{\_}12.pdf:pdf},
isbn = {9781622767595},
journal = {Proc. Interspeech},
keywords = {LSTM neural networks,language modeling,recurrent neural networks},
title = {{LSTM Neural Networks for Language Modeling}},
year = {2012}
}
@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Fi-nally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
doi = {10.1007/s10107-014-0839-0},
eprint = {1409.3215},
file = {:Users/anna/Work/Interspeech16/whitepaper/sutskever{\_}endtoend{\_}14.pdf:pdf},
isbn = {1409.3215},
issn = {09205691},
journal = {Advances in Neural Information Processing Systems (NIPS)},
pages = {3104--3112},
pmid = {2079951},
title = {{Sequence to sequence learning with neural networks}},
url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural},
year = {2014}
}
@article{Synnaeve2014,
abstract = {We trained a Siamese network with multi-task same/different information on a speech dataset, we found that learning a good representation for one task helps in the other too, and joint-learning of both tasks gave the best results. The first task was to discriminate between two same or different words, and the second was to discriminate between two same or different talkers.},
archivePrefix = {arXiv},
arxivId = {1412.6645},
author = {Synnaeve, Gabriel and Dupoux, Emmanuel},
eprint = {1412.6645},
file = {:Users/anna/Work/Interspeech16/whitepaper/synnaeve{\_}siamese{\_}15.pdf:pdf},
journal = {Under review of ICLR workshop},
number = {2014},
pages = {5},
title = {{Weakly Supervised Multi-Embeddings Learning of Acoustic Models}},
url = {http://arxiv.org/abs/1412.6645},
volume = {2},
year = {2014}
}
@article{Szepannek2010,
author = {Szepannek, Gero and Gruhne, Matthias and Bischl, Bernd},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Szepannek, Gruhne, Bischl - 2010 - Perceptually based phoneme recognition in popular music.pdf:pdf},
journal = {Classification as a Tool {\ldots}},
keywords = {auditory models,classifier optimization,feature extraction,music},
pages = {1--8},
title = {{Perceptually based phoneme recognition in popular music}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-10745-0{\_}83},
year = {2010}
}
@article{Szepannek2010a,
address = {Berlin, Heidelberg},
author = {Szepannek, Gero and Gruhne, Matthias and Bischl, Bernd and Krey, Sebastian and Harczos, Tamas and Klefenz, Frank and Dittmar, Christian and Weihs, Claus},
doi = {10.1007/978-3-642-10745-0},
editor = {Locarek-Junge, Hermann and Weihs, Claus},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Szepannek et al. - 2010 - Perceptually Based Phoneme Recognition in Popular Music.pdf:pdf},
isbn = {978-3-642-10744-3},
publisher = {Springer Berlin Heidelberg},
series = {Studies in Classification, Data Analysis, and Knowledge Organization},
title = {{Perceptually Based Phoneme Recognition in Popular Music}},
url = {http://link.springer.com/10.1007/978-3-642-10745-0},
year = {2010}
}
@article{Szoke2008,
author = {Szoke, I and Fapso, M and Burget, L and Cernocky, J},
file = {:Users/anna/Thesis/Papers/SZO08b.pdf:pdf},
journal = {Proceedings of the 31st Annual {\ldots}},
keywords = {decoding,indexing,recognition,speech,subword,term,word},
number = {July},
title = {{Hybrid word-subword decoding for spoken term detection}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Hybrid+word-subword+decoding+for+spoken+term+detection+.{\#}0},
year = {2008}
}
@article{Szoke2008a,
author = {Sz{\"{o}}ke, I and Fap{\v{s}}o, M and Karafi{\'{a}}t, M and Burget, L},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Sz{\"{o}}ke et al. - 2008 - Spoken term detection system based on combination of LVCSR and phonetic search.pdf:pdf},
journal = {Machine Learning for {\ldots}},
pages = {237--247},
title = {{Spoken term detection system based on combination of LVCSR and phonetic search}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-78155-4{\_}21},
year = {2008}
}
@article{Szoke2005,
author = {Szoke, I and Schwarz, Petr and Matejka, P},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Szoke, Schwarz, Matejka - 2005 - Comparison of keyword spotting approaches for informal continuous speech.pdf:pdf},
journal = {Proc. of Joint Workshop {\ldots}},
keywords = {a phoneme,and a hybrid ap-,generated by large,on carefully defined test,phoneme lattices generated by,proach making use of,recognizer,spotting,spotting in word lattices,the systems are compared,vocabulary continuous speech recognition},
number = {506811},
title = {{Comparison of keyword spotting approaches for informal continuous speech}},
url = {http://www.fit.vutbr.cz/{~}szoke/papers/eurospeech{\_}2005.pdf},
year = {2005}
}
@article{Szoke2005a,
author = {Sz{\"{o}}ke, I and Schwarz, Petr and Mat{\v{e}}jka, P},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Sz{\"{o}}ke, Schwarz, Mat{\v{e}}jka - 2005 - Phoneme based acoustics keyword spotting in informal continuous speech.pdf:pdf},
journal = {Text, Speech and {\ldots}},
pages = {302--309},
title = {{Phoneme based acoustics keyword spotting in informal continuous speech}},
url = {http://link.springer.com/chapter/10.1007/11551874{\_}39},
year = {2005}
}
@article{Szoke2005b,
author = {Sz{\"{o}}ke, I and Schwarz, Petr and Matejka, P and Burget, L},
file = {:Users/anna/Thesis/Papers/mlmi{\_}2005.pdf:pdf},
journal = {Interspeech},
keywords = {and we,by the ambiguity,detection of selected words,differs from searching in,have to count on,in,inaccuracies of recognition systems,kws,kws in spoken speech,speech utterances,spotting,string matching,systems are used for,the estimation of,therefore,to make an exact,we are never able,written text},
number = {506811},
title = {{Comparison of keyword spotting approaches for informal continuous speech.}},
url = {http://www.fit.vutbr.cz/{~}szoke/papers/mlmi{\_}2005.pdf},
year = {2005}
}
@article{Tao2008,
author = {Tao, TC},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Tao - 2008 - A review of contributions by Australian research institutions into speech processing.pdf:pdf},
title = {{A review of contributions by Australian research institutions into speech processing}},
url = {http://oai.dtic.mil/oai/oai?verb=getRecord{\&}metadataPrefix=html{\&}identifier=ADA496567},
year = {2008}
}
@article{Tejedor2006,
author = {Tejedor, Javier and Col{\'{a}}s, Jos{\'{e}}},
file = {:Users/anna/Thesis/Papers/4jth{\_}106.pdf:pdf},
journal = {Proc. IV Jornadas de Tecnologıa del Habla},
keywords = {albayzin,hmm,is based on hidden,markov model,on a,results,set of data from,spotting system in order,test results are reported,the geographic corpus of,the keyword technique used,to achieve the best},
pages = {255--260},
title = {{Spanish keyword spotting system based on filler models, pseudo N-gram language model and a confidence measure}},
url = {http://jth2006.unizar.es/finals/4jth{\_}106.pdf},
year = {2006}
}
@article{Tejedor2013,
author = {Tejedor, Javier and Toledano, DT and Anguera, Xavier},
doi = {10.1186/1687-4722-2013-23},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Tejedor, Toledano, Anguera - 2013 - Query-by-Example Spoken Term Detection ALBAYZIN 2012 evaluation overview, systems, results, and disc.pdf:pdf},
issn = {1687-4722},
journal = {EURASIP Journal on {\ldots}},
keywords = {international evaluation,query-by-example,search on spontaneous speech,spoken term detection},
number = {1},
pages = {23},
title = {{Query-by-Example Spoken Term Detection ALBAYZIN 2012 evaluation: overview, systems, results, and discussion}},
url = {http://asmp.eurasipjournals.com/content/2013/1/23 http://link.springer.com/article/10.1186/1687-4722-2013-23},
volume = {2013},
year = {2013}
}
@article{Thambiratnam2005,
abstract = {Keyword Spotting is the task of detecting keywords of interest within continu- ous speech. The applications of this technology range from call centre dialogue systems to covert speech surveillance devices. Keyword spotting is particularly well suited to data mining tasks such as real-time keyword monitoring and unre- stricted vocabulary audio document indexing. However, to date, many keyword spotting approaches have suered from poor detection rates, high false alarm rates, or slow execution times, thus reducing their commercial viability. This work investigates the application of keyword spotting to data mining tasks. The thesis makes a number of major contributions to the eld of keyword spotting. The rst major contribution is the development of a novel keyword verication method named Cohort Word Verication. This method combines high level lin- guistic information with cohort-based verication techniques to obtain dramatic improvements in verication performance, in particular for the problematic short duration target word class. The second major contribution is the development of a novel audio document indexing technique named Dynamic Match Lattice Spotting. This technique aug- ments lattice-based audio indexing principles with dynamic sequence matching techniques to provide robustness to erroneous lattice realisations. The resulting algorithm obtains signicant improvement in detection rate over lattice-based audio document indexing while still maintaining extremely fast search speeds. The third major contribution is the study of multiple verier fusion for the task of keyword verication. The reported experiments demonstrate that substantial improvements in verication performance can be obtained through the fusion of multiple keyword veriers. The research focuses on combinations of speech background model based veriers and cohort word veriers. The nal major contribution is a comprehensive study of the eects of limited training data for keyword spotting. This study is performed with consideration as to how these eects impact the immediate development and deployment of speech technologies for non-English languages.},
author = {Thambiratnam, a J Kishan},
file = {:Users/anna/Downloads/Albert{\_}Thambiratnam{\_}Thesis.pdf:pdf},
keywords = {Audio Indexing,Confidence Scoring,Data Mining,Keyword Spotting,Keyword Veri-,Speech Recognition,Utterance Verification,Wordspotting,fication},
number = {March},
pages = {248},
title = {{Acoustic keyword spotting in speech with applications to data mining}},
year = {2005}
}
@article{Thambiratnam2007,
author = {Thambiratnam, Kishan and Sridharan, Sridha},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Thambiratnam, Sridharan - 2007 - Rapid yet accurate speech indexing using dynamic match lattice spotting.pdf:pdf},
journal = {Audio, Speech, and Language {\ldots}},
number = {11},
pages = {1--11},
title = {{Rapid yet accurate speech indexing using dynamic match lattice spotting}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4032797},
volume = {1},
year = {2007}
}
@misc{TheMendeleySupportTeam2011,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
address = {London},
author = {{The Mendeley Support Team}},
booktitle = {Mendeley Desktop},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/The Mendeley Support Team - 2011 - Getting Started with Mendeley.pdf:pdf},
keywords = {Mendeley,how-to,user manual},
pages = {1--16},
publisher = {Mendeley Ltd.},
title = {{Getting Started with Mendeley}},
url = {http://www.mendeley.com},
year = {2011}
}
@inproceedings{inproceedings:vibrato_questions,
author = {Timmers, Renee and Desain, Peter},
booktitle = {Proceedings of 6th ICMP},
title = {{Vibrato: Questions and Answers from Musicians and Science}},
year = {2000}
}
@inproceedings{inproceedings:folk_neural_networks,
author = {Toiviainen, Petri and Eerola, Tuomas},
booktitle = {Proceedings of the 7th International Symposium on Systematic and Comparative Musicology and 3rd International Conference on Cognitive Musicology},
pages = {41--45},
title = {{A method for comparative analysis of folk music based on musical feature extraction and neural networks}},
year = {2001}
}
@article{Tong2006,
author = {Tong, Rong and Ma, Bin and Zhu, Donglai and Li, Haizhou and Chng, Eng Siong},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Tong et al. - 2006 - Integrating acoustic, prosodic and phonotactic features for spoken language identification.pdf:pdf},
journal = {Acoustics, Speech and {\ldots}},
pages = {205--208},
title = {{Integrating acoustic, prosodic and phonotactic features for spoken language identification}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1659993},
year = {2006}
}
@inproceedings{torres,
address = {Denver, CO, USA},
author = {Torres-Carrasquillo, P A and Singer, E and Kohler, M A and Greene, R J and Reynolds, D A and {J. R. Deller}, Jr.},
booktitle = {International Conference on Spoken Language Processing (ICSLP)},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Torres-Carrasquillo et al. - 2002 - Approaches to language identification using Gaussian mixture models and shifted delta cepstral featu.pdf:pdf},
pages = {89--92},
title = {{Approaches to language identification using Gaussian mixture models and shifted delta cepstral features}},
url = {http://llwebprod2.ll.mit.edu/mission/communications/publications/publication-files/full{\_}papers/020916{\_}Torres.pdf},
year = {2002}
}
@article{Torres-Carrasquillo2002,
author = {Torres-Carrasquillo, PA and Reynolds, D A and {J. R. Deller}, Jr.},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Torres-Carrasquillo, Reynolds, J. R. Deller - 2002 - Language identification using Gaussian mixture model tokenization.pdf:pdf},
journal = {{\ldots} , Speech, and Signal {\ldots}},
pages = {757--760},
title = {{Language identification using Gaussian mixture model tokenization}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5743828},
year = {2002}
}
@article{Trigeorgis2016,
abstract = {The automatic recognition of spontaneous emotions from speech is a challenging task. On the one hand, acoustic features need to be robust enough to capture the emotional content for various styles of speaking, and while on the other, machine learning algorithms need to be insensitive to outliers while being able to model the context. Whereas the latter has been tackled by the use of Long Short-Term Memory (LSTM) networks, the former is still under very active in-vestigations, even though more than a decade of research has provided a large set of acoustic descriptors. In this pa-per, we propose a solution to the problem of 'context-aware' emotional relevant feature extraction, by combining Convo-lutional Neural Networks (CNNs) with LSTM networks, in order to automatically learn the best representation of the speech signal directly from the raw time representation. In this novel work on the so-called end-to-end speech emotion recognition, we show that the use of the proposed topology significantly outperforms the traditional approaches based on signal processing techniques for the prediction of sponta-neous and natural emotions on the RECOLA database.},
author = {Trigeorgis, George and Ringeval, Fabien and Brueckner, Raymond and Marchi, Erik and Nicolaou, Mihalis A and Schuller, Bj{\"{o}}rn and Zafeiriou, Stefanos},
file = {:Users/anna/Downloads/660ef3ae-64d7-4616-8026-8f21a96f4f12.pdf:pdf},
isbn = {9781479999880},
journal = {Icassp 2016},
pages = {5200--5204},
title = {{Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network}},
year = {2016}
}
@inproceedings{tsai_wang,
address = {Barcelona, Spain},
author = {Tsai, W.-H. and Wang, H.-M.},
booktitle = {5th International Conference on Music Information Retrieval (ISMIR)},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Tsai, Wang - 2004 - Towards Automatic Identification Of Singing Language In Popular Music Recordings.pdf:pdf},
pages = {568--576},
title = {{Towards Automatic Identification Of Singing Language In Popular Music Recordings}},
year = {2004}
}
@inproceedings{Tucker2016,
author = {Tucker, George and Wu, Minhua and Sun, Ming and Panchapagesan, Sankaran and Fu, Gengshen and Vitaladevuni, Shiv},
booktitle = {Interspeech},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Tucker et al. - 2016 - Model compression applied to small-footprint keyword spotting.pdf:pdf},
keywords = {[Electronic Manuscript]},
pages = {1878--1882},
title = {{Model compression applied to small-footprint keyword spotting}},
year = {2016}
}
@article{article:genre_classification,
author = {Tzanetakis, George and Cook, Perry},
journal = {IEEE Transactions on Speech and Audio Processing},
number = {5},
pages = {293--302},
title = {{Musical Genre Classification of Audio Signals}},
volume = {10},
year = {2002}
}
@article{article:computational_ethnomusicology,
author = {Tzanetakis, George and Kapur, Ajay and Schloss, W Andrew and Wright, Matthew},
journal = {Journal of Interdisciplinary Music Studies},
number = {2},
pages = {1--24},
title = {{Computational Ethnomusicology}},
volume = {1},
year = {2007}
}
@article{Uria2011,
author = {Ur{\'{i}}a, B},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Ur{\'{i}}a - 2011 - A Deep Belief Network for the Acoustic-Articulatory Inversion Mapping Problem.pdf:pdf},
journal = {Citeseer},
title = {{A Deep Belief Network for the Acoustic-Articulatory Inversion Mapping Problem}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.227.5070{\&}rep=rep1{\&}type=pdf},
year = {2011}
}
@phdthesis{phdthesis:roughness,
address = {Los Angeles, CA, USA},
author = {Vassilakis, Panteleimon Nestor},
school = {University of California},
title = {{Perceptual and Physical Properties of Amplitude Fluctuation and their Musical Significance}},
year = {2001}
}
@article{article:roughness,
author = {Vassilakis, Pantelis N},
journal = {Perspectives in Systematic Musicology},
pages = {119--144},
series = {Selected Reports in Ethnomusicology},
title = {{Auditory Roughness as a Means of Musical Expression}},
volume = {12},
year = {2005}
}
@inproceedings{source_separation,
author = {Vinyes, M and Bonada, J and Loscos, A},
booktitle = {Proceedings of Audio Engineering Society 120th Convention},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Vinyes, Bonada, Loscos - 2006 - Demixing Commercial Music Productions via Human-Assisted Time-Frequency Masking.pdf:pdf},
title = {{Demixing Commercial Music Productions via Human-Assisted Time-Frequency Masking}},
year = {2006}
}
@inproceedings{inproceedings:latin_genre,
author = {V{\"{o}}lkel, Thomas and Abe{\ss}er, Jakob and Dittmar, Christan and Gro{\ss}mann, Holger},
booktitle = {Proceedings of the Audio Mostly Conference},
title = {{Automatic Genre Classification of Latin American Music using Characteristic Rhythmic Patterns}},
year = {2010}
}
@article{article:sachs_hornbostel,
author = {von Hornbostel, Erich M and Sachs, Curt},
journal = {Zeitschrift f{\"{u}}r Ethnologie},
number = {4-5},
pages = {553--590},
title = {{Systematik der Musikinstrumente - Ein Versuch}},
volume = {46},
year = {1914}
}
@article{W.Byrne,
author = {{W. Byrne} and {P. Beyerlein} and {J.M. Huerta} and {S. Khudanpur} and {B. Marthi} and {J. Morgan} and {N. Peterek} and {J. Picone} and {D. Vergyri} and {T. Wang}},
doi = {10.1109/ICASSP.2000.859138},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/W. Byrne et al. - Unknown - Towards language independent acoustic modeling.pdf:pdf},
isbn = {0-7803-6293-4},
journal = {2000 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.00CH37100)},
number = {1},
pages = {II1029--II1032},
publisher = {Ieee},
title = {{Towards language independent acoustic modeling}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=859138},
volume = {2}
}
@article{Wallace2010,
author = {Wallace, RG},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Wallace - 2010 - Fast and accurate phonetic spoken term detection.pdf:pdf},
number = {August},
title = {{Fast and accurate phonetic spoken term detection}},
url = {http://eprints.qut.edu.au/39610/},
year = {2010}
}
@article{Wallace2007,
author = {Wallace, RG and Vogt, RJ and Sridharan, Sridha},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Wallace, Vogt, Sridharan - 2007 - A phonetic search approach to the 2006 NIST spoken term detection evaluation.pdf:pdf},
pages = {2385--2388},
title = {{A phonetic search approach to the 2006 NIST spoken term detection evaluation}},
url = {http://eprints.qut.edu.au/15476/},
year = {2007}
}
@article{Wallace2009,
address = {New York, New York, USA},
author = {Wallace, Roy and Baker, Brendan and Vogt, Robbie and Sridharan, Sridha},
doi = {10.1145/1631127.1631132},
file = {:Users/anna/Thesis/Papers/p31-wallace.pdf:pdf},
isbn = {9781605587622},
journal = {Proceedings of the third workshop on Searching spontaneous conversational speech - SSCS '09},
keywords = {all or part of,language modelling,or hard copies of,permission to make digital,phoneme lat-,phoneme recognition,spoken term detection,this work for,tice},
pages = {31--36},
publisher = {ACM Press},
title = {{The effect of language models on phonetic decoding for spoken term detection}},
url = {http://portal.acm.org/citation.cfm?doid=1631127.1631132},
year = {2009}
}
@article{Wang2003,
abstract = {We have developed and commercially deployed a flexible audio search engine. The algorithm is noise and distortion resistant, computationally efficient, and massively scalable, capable of quickly identifying a short segment of music captured through a cellphone microphone in the presence of foreground voices and other dominant noise, and through voice codec compression, out of a database of over a million tracks. The algorithm uses a combinatorially hashed time-frequency constellation analysis of the audio, yielding unusual properties such as transparency, in which multiple tracks mixed together may each be identified. Furthermore, for applications such as radio monitoring, search times on the order of a few milliseconds per query are attained, even on a massive music database.},
author = {Wang, Avery Li-Chun},
doi = {10.1109/IITAW.2009.110},
file = {:Users/anna/Downloads/Wang03-shazam.pdf:pdf},
isbn = {2974619401},
issn = {{\textless}null{\textgreater}},
journal = {Proceedings of the 4th International Society for Music Information Retrieval Conference (ISMIR 203), Baltimore, Maryland (USA), 26-30 October 2003},
pages = {7--13},
title = {{An Industrial Strength Audio Search Algorithm.}},
url = {http://www.ee.columbia.edu/{~}dpwe/papers/Wang03-shazam.pdf},
year = {2003}
}
@article{Wang2010,
author = {Wang, Chung-Che and Jang, Jyh-shing Roger and Wang, Wennen},
file = {:Users/anna/Downloads/10.1.1.231.2880.pdf:pdf},
isbn = {9789039353813},
journal = {11th ISMIR},
number = {Ismir},
pages = {45--50},
title = {{An Improved Query by Singing/Humming System Using Melody and Lyrics Information}},
year = {2010}
}
@article{Wang2013,
author = {Wang, Guangsen and Sim, KC},
file = {:Users/anna/Thesis/Papers/205{\_}Context-dependent-Wang-2927709.pdf:pdf},
journal = {{\ldots} and Conference (APSIPA), 2013 Asia-Pacific},
keywords = {a hybrid language,and phone tokens is,and the arbitrary,are,context,fixed length word context,length word context,model with both word,the word n-gram count,three different spotting graphs,to provide these contexts,trained using only},
title = {{Context dependent acoustic keyword spotting using deep neural network}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6694175},
year = {2013}
}
@inproceedings{Wang2016,
author = {Wang, Wenfu and Xu, Shuang and Xu, Bo},
booktitle = {Interspeech},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Xu, Xu - 2016 - First Step Towards End-to-end Parametric TTS Synthesis Generating Spectral Parameters with Neural Attention.pdf:pdf},
keywords = {[Electronic Manuscript]},
number = {2013},
pages = {2243--2247},
title = {{First Step Towards End-to-end Parametric TTS Synthesis : Generating Spectral Parameters with Neural Attention}},
year = {2016}
}
@article{Wang2008,
abstract = {Voice search is the technology underlying many spoken dialog systems (SDSs) that provide users with the information they request with a spoken query. The information normally exists in a large database, and the query has to be compared with a field in the database to obtain the relevant information. The contents of the field, such as business or product names, are often unstructured text. This article categorized spoken dialog technology into form filling, call routing, and voice search, and reviewed the voice search technology. The categorization was made from the technological perspective. It is important to note that a single SDS may apply the technology from multiple categories. Robustness is the central issue in voice search. The technology in acoustic modeling aims at improved robustness to environment noise, different channel conditions, and speaker variance; the pronunciation research addresses the problem of unseen word pronunciation and pronunciation variance; the language model research focuses on linguistic variance; the studies in search give rise to improved robustness to linguistic variance and ASR errors; the dialog management research enables graceful recovery from confusions and understanding errors; and the learning in the feedback loop speeds up system tuning for more robust performance. While tremendous achievements have been accomplished in the past decade on voice search, large challenges remain. Many voice search dialog systems have automation rates around or below 50{\%} in field trials.},
author = {Wang, Ye Yi and Yu, Dong and Ju, Yun Cheng and Acero, Alex},
doi = {10.1109/MSP.2008.918411},
file = {:Users/anna/Downloads/04490199.pdf:pdf},
isbn = {1053-5888},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
number = {3},
pages = {29--38},
title = {{An introduction to voice search}},
volume = {25},
year = {2008}
}
@article{Wang2004,
abstract = {We present a prototype that automatically aligns acoustic musical signals with their corresponding textual lyrics, in a manner similar to manually-aligned karaoke. We tackle this problem using a multimodal approach, where the appropriate pairing of audio and text processing helps create a more accurate system. Our audio processing technique uses a combination of top-down and bottom-up approaches, combining the strength of low-level audio features and high-level musical knowledge to determine the hierarchical rhythm structure, singing voice and chorus sections in the musical audio. Text processing is also employed to approximate the length of the sung passages using the textual lyrics. Results show an average error of less than one bar for per-line alignment of the lyrics on a test bed of 20 songs (sampled from CD audio and carefully selected for variety). We perform holistic and per-component testing and analysis and outline steps for further development.},
author = {Wang, Ye and Kan, Min-Yen and Nwe, Tin Lay and Shenoy, Arun and Yin, Jun},
doi = {10.1145/1027527.1027576},
file = {:Users/anna/Downloads/p212-wang.pdf:pdf},
isbn = {1581138938},
journal = {Proceedings of the 12th annual ACM international conference on Multimedia},
keywords = {audio,audio synchronization,karaoke,lyric alignment,lyrics,music knowledge,text synergy,vocal detection},
pages = {212--219},
title = {{LyricAlly: automatic synchronization of acoustic musical signals and textual lyrics}},
url = {http://dl.acm.org/citation.cfm?id=1027576},
volume = {0},
year = {2004}
}
@article{Weber2003,
author = {Weber, Andrea and Smits, Roel},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Weber, Smits - 2003 - Consonant and vowel confusion patterns by American English listeners.pdf:pdf},
journal = {{\ldots} of the 15th International Congress of {\ldots}},
pages = {1--4},
title = {{Consonant and vowel confusion patterns by American English listeners}},
url = {http://pubman.mpdl.mpg.de/pubman/item/escidoc:67125:6/component/escidoc:67126/Consonant+And+Vowel+Confusion+Patterns+By.pdf},
year = {2003}
}
@inproceedings{Weber2016,
author = {Weber, Philip and Bai, Linxue and Russell, Martin and Jan{\v{c}}ovi{\v{c}}, Peter and Houghton, Stephen},
booktitle = {Interspeech},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Weber et al. - 2016 - Interpretation of Low Dimensional Neural Network Bottleneck Features in Terms of Human Perception and Production.pdf:pdf},
keywords = {[Electronic Manuscript]},
pages = {3384--3388},
title = {{Interpretation of Low Dimensional Neural Network Bottleneck Features in Terms of Human Perception and Production}},
year = {2016}
}
@article{Weintraub1995,
author = {Weintraub, Mitchel},
file = {:Users/anna/Thesis/Papers/00479532.pdf:pdf},
isbn = {0780324315},
journal = {Acoustics, Speech, and Signal Processing, 1995 {\ldots}},
keywords = {from the viterbi backtrace,hypothesized if it appeared,in the viterbi backtrace,the one best answer,using,we used},
number = {3},
pages = {297--300},
title = {{LVCSR Log-likelihood ratio scoring for keyword spotting}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=479532},
year = {1995}
}
@article{Weintraub1993,
author = {Weintraub, Mitchel},
file = {:Users/anna/Thesis/Papers/00319341.pdf:pdf},
journal = {Acoustics, Speech, and Signal Processing, 1993 {\ldots}},
keywords = {-spotting using sri,s decipherm large-vocabuarly,speech-recognition system},
pages = {463--466},
title = {{Keyword-spotting using SRI's DECIPHER large-vocabulary speech-recognition system}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=319341},
year = {1993}
}
@article{Weiqiang2010,
author = {Weiqiang, Zhang and Tao, Hou and Jia, Liu},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Weiqiang, Tao, Jia - 2010 - Discriminative Score Fusion for Language.pdf:pdf},
journal = {Chinese Journal of Electronics},
keywords = {heteroscedastic linear discriminate analysis,hlda,language identification,lid,maximum mutual information,mmi,score fu-,sion},
number = {1},
pages = {124--128},
title = {{Discriminative Score Fusion for Language}},
volume = {19},
year = {2010}
}
@article{Weston2015,
abstract = {We propose DEEPMEMORY, a novel deep architecture for sequence-to-sequence learning, which performs the task through a series of nonlinear transformations from the representation of the input sequence (e.g., a Chinese sentence) to the final output sequence (e.g., translation to English). Inspired by the recently proposed Neural Turing Machine (Graves et al., 2014), we store the intermediate representations in stacked layers of memories, and use read-write operations on the memories to realize the nonlinear transformations between the representations. The types of transformations are designed in advance but the parameters are learned from data. Through layer-by-layer transformations, DEEPMEMORY can model complicated relations between sequences necessary for applications such as machine translation between distant languages. The architecture can be trained with normal back-propagation on sequenceto-sequence data, and the learning can be easily scaled up to a large corpus. DEEPMEMORY is broad enough to subsume the state-of-the-art neural translation model in (Bahdanau et al., 2015) as its special case, while significantly improving upon the model with its deeper architecture. Remarkably, DEEPMEMORY, being purely neural network-based, can achieve performance comparable to the traditional phrase-based machine translation system Moses with a small vocabulary and a modest parameter size.},
archivePrefix = {arXiv},
arxivId = {1506.06442},
author = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Mikolov, Tomas and Rush, Alexander M. and Vinyals, Oriol and Le, Quoc V. and Meng, Fandong and Lu, Zhengdong and Tu, Zhaopeng and Li, Hang and Liu, Qun},
doi = {10.1016/j.jpowsour.2014.09.131},
eprint = {1506.06442},
file = {:Users/anna/Downloads/1506.06442v1.pdf:pdf},
isbn = {1502.05698},
journal = {arXiv preprint},
keywords = {chatbots,dialog systems,neural networks},
pages = {1--13},
title = {{A Deep Memory-based Architecture for Sequence-to-Sequence Learning}},
url = {http://arxiv.org/abs/1506.06442{\%}5Cnhttp://arxiv.org/abs/1502.05698},
volume = {37},
year = {2015}
}
@article{Wollmer2009,
author = {Wollmer, M and Eyben, Florian},
file = {:Users/anna/Thesis/Papers/09woe5.pdf:pdf},
isbn = {9781424454792},
journal = {{\ldots} Speech Recognition {\&} {\ldots}},
keywords = {and the garbage model,from low flexibility since,hmms for the keywords,in the,new,occurrences of the keywords,parts of the speech,presumes,sequence,that there are enough,training corpus and suffers,using whole word},
pages = {349--353},
title = {{Robust vocabulary independent keyword spotting with graphical models}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5373544},
year = {2009}
}
@article{Wollmer2009a,
author = {Wollmer, Martin and Eyben, Florian},
doi = {10.1109/ASRU.2009.5373544},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Wollmer, Eyben - 2009 - Robust vocabulary independent keyword spotting with graphical models.pdf:pdf},
isbn = {978-1-4244-5478-5},
journal = {{\ldots} Speech Recognition {\&} {\ldots}},
keywords = {and the garbage model,from low flexibility since,hmms for the keywords,in the,new,occurrences of the keywords,parts of the speech,presumes,sequence,that there are enough,training corpus and suffers,using whole word},
month = {dec},
pages = {349--353},
publisher = {Ieee},
title = {{Robust vocabulary independent keyword spotting with graphical models}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5373544 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5373544},
year = {2009}
}
@techreport{techreport:kefir,
author = {Wolter, Kay},
institution = {Fraunhofer IDMT},
title = {{kefir Guide}},
year = {2010}
}
@inproceedings{Wong2000,
author = {Wong, E and Pelecanos, J and Myers, S and Sridharan, S},
booktitle = {{\ldots} International Conference on {\ldots}},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Wong et al. - 2000 - Language identification using efficient Gaussian mixture model analysis.pdf:pdf},
title = {{Language identification using efficient Gaussian mixture model analysis}},
url = {http://assta.org/sst/SST-00/cache/SST-00-Chapter3-p10.pdf},
year = {2000}
}
@inproceedings{Wong2016,
author = {Wong, Jeremy H M and Gales, Mark J F},
booktitle = {Interspeech},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Wong, Gales - 2016 - Sequence Student-Teacher Training of Deep Neural Networks.pdf:pdf},
keywords = {[Electronic Manuscript]},
pages = {2761--2765},
title = {{Sequence Student-Teacher Training of Deep Neural Networks}},
year = {2016}
}
@article{Wu2016,
abstract = {Recently, recurrent neural networks (RNNs) as powerful sequence models have re-emerged as a potential acoustic model for statistical parametric speech synthesis (SPSS). The long short-term memory (LSTM) architecture is particularly attractive because it addresses the vanishing gradient problem in standard RNNs, making them easier to train. Although recent studies have demonstrated that LSTMs can achieve significantly better performance on SPSS than deep feed-forward neural networks, little is known about why. Here we attempt to answer two questions: a) why do LSTMs work well as a sequence model for SPSS; b) which component (e.g., input gate, output gate, forget gate) is most important. We present a visual analysis alongside a series of experiments, resulting in a proposal for a simplified architecture. The simplified architecture has significantly fewer parameters than an LSTM, thus reducing generation complexity considerably without degrading quality.},
archivePrefix = {arXiv},
arxivId = {1601.02539},
author = {Wu, Zhizheng and King, Simon},
doi = {10.1109/ICASSP.2016.7472657},
eprint = {1601.02539},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Wu, King - 2016 - Investigating gated recurrent networks for speech synthesis.pdf:pdf},
isbn = {9781479999880},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Speech synthesis,acoustic modelling,gated recurrent network,long short-term memory,recurrent network network},
pages = {5140--5144},
title = {{Investigating gated recurrent networks for speech synthesis}},
volume = {2016-May},
year = {2016}
}
@article{Xu2009,
author = {Xu, Xin and Naito, Masaki and Kato, Tsuneo and Kawai, H},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Xu et al. - 2009 - Robust and Fast Lyric Search based on Phonetic Confusion Matrix.pdf:pdf},
journal = {ISMIR},
number = {Ismir},
pages = {417--422},
title = {{Robust and Fast Lyric Search based on Phonetic Confusion Matrix.}},
url = {http://www.ismir2009.ismir.net/proceedings/PS3-5.pdf},
year = {2009}
}
@article{Yaman2012,
abstract = {Bottleneck neural networks have recently found success in a variety of speech recognition tasks. This paper presents an approach in which they are utilized in the front-end of a speaker recognition system. The network inputs are mel-frequency cepstral coefficients (MFCCs) from multiple consec-utive frames and the outputs are speaker labels. We propose using a recording-level criterion that is optimized via an online learning algorithm. We furthermore propose retraining a net-work to focus on its errors when leveraging scores from an inde-pendently trained system. We ran experiments on the same-and different-microphone tasks of the 2010 NIST Speaker Recogni-tion Evaluation. We found that the proposed bottleneck feature extraction paradigm performs slightly worse than MFCCs but provides complementary information in combination. We also found that the proposed combination strategy with re-training improved the EER by 14{\%} and 18{\%} relative over the baseline MFCC system in the same-and different-microphone tasks re-spectively.},
author = {Yaman, Sibel and Pelecanos, Jason and Sarikaya, Ruhi},
file = {:Users/anna/Work/Interspeech16/whitepaper/yaman{\_}bottleneck{\_}11.pdf:pdf},
journal = {Proceedings of Odyssey 2012 - The Speaker and Language Recognition Workshop},
pages = {105--108},
title = {{Bottleneck Features for Speaker Recognition}},
year = {2012}
}
@inproceedings{yan_barnard,
address = {Atlanta, GA, USA},
author = {Yan, Y and Barnard, E},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Yan, Barnard - 1996 - Experiments For An Approach To Language Identification With Conversational Telephone Speech.pdf:pdf},
pages = {789--792},
title = {{Experiments For An Approach To Language Identification With Conversational Telephone Speech}},
year = {1996}
}
@article{Yoma2001,
author = {Yoma, N.B. and McInnes, F.R. and Jack, M.a. and Stump, S.D. and Ling, L.L.},
doi = {10.1109/89.902285},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Yoma et al. - 2001 - On including temporal constraints in Viterbi alignment for speech recognition in noise.pdf:pdf},
issn = {10636676},
journal = {IEEE Transactions on Speech and Audio Processing},
number = {2},
pages = {179--182},
title = {{On including temporal constraints in Viterbi alignment for speech recognition in noise}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=902285},
volume = {9},
year = {2001}
}
@article{Yoma1998,
author = {Yoma, NB and McInnes, FR and Jack, MA},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Yoma, McInnes, Jack - 1998 - Weighted Viterbi algorithm and state duration modelling for speech recognition in noise.pdf:pdf},
isbn = {0780344286},
journal = {Acoustics, Speech and {\ldots}},
number = {3},
pages = {6--9},
title = {{Weighted Viterbi algorithm and state duration modelling for speech recognition in noise}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=675363},
year = {1998}
}
@article{Yoma2002,
author = {Yoma, NB and S{\'{a}}nchez, JS},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Yoma, S{\'{a}}nchez - 2002 - MAP speaker adaptation of state duration distributions for speech recognition.pdf:pdf},
journal = {Speech and Audio Processing, IEEE {\ldots}},
number = {7},
pages = {443--450},
title = {{MAP speaker adaptation of state duration distributions for speech recognition}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1045276},
volume = {10},
year = {2002}
}
@article{Young1997,
author = {Young, SJ and Brown, MG},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Young, Brown - 1997 - Acoustic indexing for multimedia retrieval and browsing.pdf:pdf},
journal = {{\ldots} , Speech, and Signal {\ldots}},
pages = {2--5},
title = {{Acoustic indexing for multimedia retrieval and browsing}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=599600},
year = {1997}
}
@article{Yu2011,
abstract = {Bottleneck features have been shown to be effective in improving the accuracy of automatic speech recognition (ASR) systems. Conventionally, bottleneck features are extracted from a multi-layer perceptron (MLP) trained to predict context-independent monophone states. The MLP typically has three hidden layers and is trained using the backpropagation algorithm. In this paper, we propose two improvements to the training of bottleneck features motivated by recent advances in the use of deep neural networks (DNNs) for speech recognition. First, we show how the use of unsupervised pretraining of a DNN enhances the network's discriminative power and improves the bottleneck features it generates. Second, we show that a neural network trained to predict context-dependent senone targets produces better bottleneck features than one trained to predict monophone states. Bottleneck features trained using the proposed methods produced a 16{\%} relative reduction in sentence error rate over conventional bottleneck features on a large vocabulary business search task. Copyright {\textcopyright} 2011 ISCA.},
author = {Yu, Dong and Seltzer, Michael L.},
file = {:Users/anna/Work/Interspeech16/whitepaper/yu{\_}bottleneck{\_}11.pdf:pdf},
isbn = {19909772 (ISSN)},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Bottleneck features,Deep belief network,Deep neural network,Pretraining},
number = {August},
pages = {237--240},
title = {{Improved bottleneck features using pretrained deep neural networks}},
year = {2011}
}
@inproceedings{Yu2016,
author = {Yu, Dong and Xiong, Wayne and Droppo, Jasha and Stolcke, Andreas and Ye, Guoli and Li, Jinyu and Zweig, Geoffrey},
booktitle = {Interspeech},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Yu et al. - 2016 - Deep Convolutional Neural Networks with Layer-wise Context Expansion and Attention.pdf:pdf},
keywords = {[Electronic Manuscript]},
pages = {17--21},
title = {{Deep Convolutional Neural Networks with Layer-wise Context Expansion and Attention}},
year = {2016}
}
@article{Yu2004,
author = {Yu, P and Seide, FTB},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Yu, Seide - 2004 - A hybrid wordphoneme-based approach for improved vocabulary-independent search in spontaneous speech.pdf:pdf},
journal = {Interspeech},
number = {49},
title = {{A hybrid word/phoneme-based approach for improved vocabulary-independent search in spontaneous speech.}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:A+HYBRID+WORD+/+PHONEME-BASED+APPROACH+FOR+IMPROVED+VOCABULARY-INDEPENDENT+SEARCH+IN+SPONTANEOUS+SPEECH+Peng+Yu+and+Frank+Seide{\#}0},
year = {2004}
}
@article{Yu2006,
author = {Yu, SZ and Kobayashi, Hisashi},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Yu, Kobayashi - 2006 - Practical implementation of an efficient forward-backward algorithm for an explicit-duration hidden Markov model.pdf:pdf},
journal = {Signal Processing, IEEE Transactions on},
number = {5},
pages = {1947--1951},
title = {{Practical implementation of an efficient forward-backward algorithm for an explicit-duration hidden Markov model}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1621425},
volume = {54},
year = {2006}
}
@article{Zen2015,
abstract = {Long short-term memory recurrent neural networks (LSTM-RNNs) have been applied to various speech applications including acoustic modeling for statistical parametric speech synthesis. One of the con- cerns for applying them to text-to-speech applications is its effect on latency. To address this concern, this paper proposes a low-latency, streaming speech synthesis architecture using unidirectional LSTM- RNNs with a recurrent output layer. The use of unidirectional RNN architecture allows frame-synchronous streaming inference of out- put acoustic features given input linguistic features. The recurrent output layer further encourages smooth transition between acoustic features at consecutive frames. Experimental results in subjective listening tests show that the proposed architecture can synthesize natural sounding speech without requiring utterance-level batch pro- cessing.},
author = {Zen, Heiga and Sak, Hasim},
doi = {10.1109/ICASSP.2015.7178816},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Zen, Sak - 2015 - Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synt.pdf:pdf},
isbn = {9781467369978},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Statistical parametric speech synthesis,long short-term memory,low-latency,recurrent neural networks},
pages = {4470--4474},
title = {{Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis}},
volume = {2015-Augus},
year = {2015}
}
@phdthesis{Zhang2009,
author = {Zhang, Y},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Zhang - 2009 - Unsupervised spoken keyword spotting and learning of acoustically meaningful units.pdf:pdf},
number = {2006},
title = {{Unsupervised spoken keyword spotting and learning of acoustically meaningful units}},
url = {http://groups.csail.mit.edu/sls/publications/2009/Thesis{\_}Zhang{\_}09.09.pdf},
year = {2009}
}
@article{Zhang,
author = {Zhang, Yan and Id, Sunet},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Id - Unknown - Speech Recognition Using Deep Learning Algorithms.pdf:pdf},
pages = {1--5},
title = {{Speech Recognition Using Deep Learning Algorithms}}
}
@inproceedings{Zhao2016,
author = {Zhao, Zhiwei and Wu, Youzheng},
booktitle = {Interspeech},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Zhao, Wu - 2016 - Attention-based Convolutional Neural Networks for Sentence Classification.pdf:pdf},
keywords = {[Electronic Manuscript]},
pages = {705--709},
title = {{Attention-based Convolutional Neural Networks for Sentence Classification}},
year = {2016}
}
@article{Zhou2011,
author = {Zhou, Xinhui and Garcia-Romero, Daniel and Duraiswami, Ramani and Espy-Wilson, Carol and Shamma, Shihab},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Zhou et al. - 2011 - Linear versus mel frequency cepstral coefficients for speaker recognition.pdf:pdf},
isbn = {9781467303675},
journal = {{\ldots} Speech Recognition {\ldots}},
pages = {559--564},
title = {{Linear versus mel frequency cepstral coefficients for speaker recognition}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6163888},
year = {2011}
}
@inproceedings{Zhuang2016,
author = {Zhuang, Yimeng and Chang, Xuankai and Qian, Yanmin and Yu, Kai and Jiao, Shanghai},
booktitle = {Interspeech},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Zhuang et al. - 2016 - Unrestricted Vocabulary Keyword Spotting using LSTM-CTC Key Laboratory of Shanghai Education Commission for Intel.pdf:pdf},
keywords = {[Electronic Manuscript],aims to detect predefined,continuous speech,direct deep learning approaches,fixed keyword vocabulary and,have been used for,however,keywords in,kws,kws and achieved great,recently,spotting,success,these approaches mostly assume},
pages = {938--942},
title = {{Unrestricted Vocabulary Keyword Spotting using LSTM-CTC Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering SpeechLab , Department of Computer Science and Engineering}},
year = {2016}
}
@article{Ziaei2009,
author = {Ziaei, Ali and Ahadi, SM},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Ziaei, Ahadi - 2009 - A new approach for spoken language identification based on sequence kernel SVMs.pdf:pdf},
isbn = {9781424432981},
journal = {{\ldots} Processing, 2009 16th {\ldots}},
pages = {1--4},
title = {{A new approach for spoken language identification based on sequence kernel SVMs}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5201071},
year = {2009}
}
@article{zissman_,
author = {Zissman, M A},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Zissman - 1996 - Comparison of four approaches to automatic language identification of telephone speech.pdf:pdf},
journal = {IEEE Transactions on Speech and Audio Processing},
month = {jan},
number = {1},
pages = {31--44},
title = {{Comparison of four approaches to automatic language identification of telephone speech}},
volume = {4},
year = {1996}
}
@article{zissman_berkling,
author = {Zissman, M A and Berkling, K},
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Zissman, Berkling - 2001 - Automatic Language Identification.pdf:pdf},
journal = {Speech Communication},
number = {1-2},
title = {{Automatic Language Identification}},
volume = {35},
year = {2001}
}
@article{Zue1990,
abstract = {Automatic speech recognition by computers can provide the most natural and efficient method of communication between humans and computers. While in recent years high performance speech recognition systems are beginning to emerge from research institutions, scientists unequivocally agree that the deployment of speech recognition systems into realistic operating environments will require many hours of speech data to help us model the inherent variability in the speech signal. This paper describes the experiences of researchers at MIT in the collection of two large speech databases which have somewhat complementary objectives. The timit database was designed to be task and speaker-independent, and is suitable for general acoustic-phonetic research. The voyager database, on the other hand, was intended for development and evaluation of a system which incorporates both speech and natural language processing. This database is particularly valuable as a source of spontaneous utterances elicited in a realistic goal-oriented environment. ?? 1990.},
author = {Zue, Victor and Seneff, Stephanie and Glass, James},
doi = {10.1016/0167-6393(90)90010-7},
file = {:Users/anna/Downloads/speech-communication-zue90.pdf:pdf},
isbn = {0167-6393},
issn = {01676393},
journal = {Speech Communication},
keywords = {Speech corpora,speech database,speech recognition},
number = {4},
pages = {351--356},
title = {{Speech database development at MIT: Timit and beyond}},
volume = {9},
year = {1990}
}
@article{,
file = {:Users/anna/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - 10.1.1.49.5993.pdf.pdf:pdf},
title = {10.1.1.49.5993.pdf}
}

@book{book:beghtol,
 author    = {LD Beghtol},
 title     = {{Magnetic Fields' 69 Love Songs: A Field Guide (33 1/3)}},
 publisher = {Bloomsbury Academic},
 adress		 = {},
 year      = {2006},
 edition   = {}
 }
 
 @article{rollingstone,
 title={{500 Greatest Albums of All Time (465. The Magnetic Fields, '69 Love Songs')}},
 journal={Rolling Stone},
 year=2012,
 month=MAY,
 url={http://www.rollingstone.com/music/lists/500-greatest-albums-of-all-time-20120531/the-magnetic-fields-69-love-songs-20120525}
 }

