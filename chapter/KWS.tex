\chapter{Sung Keyword Spotting} \label{chap:kws}
%Keyword spotting is another task for which the new acoustic models described in chapter \ref{chap:phonerec} were employed. A keyword-filler HMM algorithm was selected due to its independence on phoneme durations, which are highly variable in singing as shown in section \ref{sec:sota_speechtosinging}. As described in section \ref{subsec:tech_kws}, keyword-filler HMMs consist of two sub-HMMs: One to model the keyword, and one to model everything else (= filler). The keyword HMM has a simple left-to-right topology with one state per keyword phoneme. The filler HMM is a fully connected loop of all phonemes. When the Viterbi path with the highest likelihood passes through the keyword HMM rather than the filler loop, the keyword is detected. Keyword detection was performed on whole songs, which is a realistic assumption for many practical applications. The \textit{ACAP} and \textit{DampTest} data sets were used for evaluation with the keyword set described in section \ref{sec:data_keywords}. Song-wise $F_1$ measures were calculated for evaluation.


In \cite{kws_overview}, three basic principles for keyword spotting in speech are mentioned:
\begin{description}
\item[LVCSR-based keyword spotting] {In this approach, a full transcription of the audio recording is performed using Large-Vocabulary Continuous Speech Recognition (LVCSR), which can then be searched for the keyword. 
%Both acoustic models and language models are employed for this step. The resulting text can then be searched for the requested keywords. 
This is expensive to implement and offers no tolerance for transcription errors - if the keyword is not transcribed correctly, it will never be found later.}
\item[Acoustic keyword spotting] {
%In contrast to the LVCSR approach, no full transcription is performed here. 
Acoustic KWS algorithms only search for the keyword on the basis of its acoustic properties. %This is commonly done with Hidden Markov Models (HMMs) and can either be performed directly on the audio or feature data, or on phoneme posteriorgrams obtained from an acoustic model.\\
This approach is easy to implement and provides some tolerance for pronunciation variations. However, it does not take any a-priori knowledge about the language into account (e.g. about plausible word or phoneme sequences).}
\item[Phonetic search keyword spotting]{
%Just like in LVCSR-based keyword spotting, 
Again, a full transcription of the audio recording is performed, but the full lattices are retained instead of just the final transcription. A phonetic search for the keyword can then be run on these lattices. This approach combines the a-priori knowledge of the LVCSR-based approach with the robustness of the acoustic approach.}
\end{description}

As described in section \ref{sec:sota_speechtosinging}, there are significant differences between speech and singing signals, which means that ASR approaches for keyword spotting cannot simply be transferred to singing. In particular, both LVCSR-based keyword spotting and Phonetic search keyword spotting depend heavily on predictable phoneme durations (within certain limits). When a certain word is pronounced, its phonemes will usually have approximately the same duration across speakers. The language model employed in both approaches will take this information into account. However, phoneme durations in singing are not as predictable in speech, as figure \ref{fig:phoneme_stats} demonstrates. For this reason, a simpler acoustic approach using keyword-filler HMMs is employed in this work.\\
Keyword-filler HMMs have been described in \cite{szoeke} and \cite{jansen}. In general, two separate HMMs are created: One for the requested keyword, and one for all non-keyword regions (=filler). The keyword HMM has a simple left-to-right topology with one state per keyword phoneme, while the filler HMM is a fully connected loop of states for all phonemes. These two HMMs are then joined. Using this composite HMM, Viterbi decoding is performed on the phoneme posteriorgrams. Whenever the Viterbi path passes through the keyword HMM, the keyword is detected. The likelihood of this path can then be compared to an alternative path through the filler HMM, resulting in a detection score. A threshold can be employed to only return highly scored occurrences. Additionally, the parameter $\beta$ can be tuned to adjust the model. It determines the likelihood of transitioning from the filler HMM to the keyword HMM. The whole process is illustrated in figure \ref{fig:kf_hmm}.

\begin{figure}
 \centerline{\framebox{
 \includegraphics[width=.8\textwidth]{images/kw_filler_hmm.png}}}
 \caption{Keyword-filler HMM for the keyword ``greasy" with filler path on the left hand side and two possible keyword pronunciation paths on the right hand side. The parameter $\beta$ determines the transition probability between the filler HMM and the keyword HMM. \cite{jansen}}
 \label{fig:kf_hmm}
\end{figure}
Integration with the phoneme recognition system is shown in figure \ref{fig:process_training_kws}. Effectively, the keyword-filler HMM is added as a post-processing step after classification, and thus performed on the posteriorgrams.
\begin{figure}
	\begin{center}
		\includegraphics[width=0.8\textwidth]{images/process_classification_kws.png}
		\caption{Schematic of the procedure for keyword spotting.}
		\label{fig:process_training_kws}
	\end{center}
\end{figure}

Keyword spotting results are considered correct when they are detected within the expected songs, and are evaluated according to the $F_1$ measure. This measure is the harmonic mean of precision $P$ and recall $R$:
 \begin{equation}
    F_1 = 2 \frac{P \cdot R}{P + R}
 \end{equation}  
This measure is especially suited for cases where the classes are not balanced; in keyword spotting, occurrence of a keyword is much rarer than non-occurrence. In continuous speech recognition, the Figure Of Merit measure is often used \cite{fom}; however, since timing is not an issue in many applications for sung keyword spotting, this measure is not employed here.\\

Keyword detection was performed on whole songs, which is a realistic assumption for many practical applications. The \textit{ACAP} and \textit{DampTest} data sets were used for evaluation with the keyword set described in section \ref{sec:data_keywords}. Song-wise $F_1$ measures were calculated for evaluation. As in the phoneme recognition experiments, no cross validation is employed because the training and test data sets serve different purposes.


\section{Keyword spotting using keyword-filler HMMs} \label{sec:kws_kwfhmm}
\subsection{Comparison of acoustic models}
Phoneme posteriorgrams were generated with the various acoustic models described in section \ref{sec:phonerec_acap}. The results in terms of $F_1$ measure across the whole \textit{DampTest} sets are shown in figure \ref{fig:kws_exp1}. Figure \ref{fig:kws_exp1_acap} shows the results of the same experiment on the small \textit{ACAP} data set.

Across all keywords, a document-wise $F_1$ measure of $0.44$ is obtained using the posteriorgrams generated with the \textit{TIMIT} model on the \textit{DampTest} data sets. This result remains the same for the \textit{TimitM} models trained on ``songified'' speech. In this experiment, using models trained on the \textit{DAMP}-based singing data sets only improves the results slightly, with $F_1$ measures of $0.47$ for the \textit{DampB} model, and $0.46$ with the much smaller \textit{DampBB\_small} model. Surprisingly, in this case, the model trained on the medium-size balanced data set \textit{DampBB} performs a little worse than the smallest one; however, this might just be due to some statistical fluctuation. In general, results on these test data sets are somewhat inconclusive. There are several reasons for this: First, the annotations were generated automatically and the keywords were picked from the aligned lyrics. The singers do not always perform or pronounce them correctly. Additionally, the keyword approach can be tuned easily for high recall; then, the precision becomes the deciding factor for $F_1$ calculation. Considering the size of the data set, keyword occurrences are relatively rare, which makes obtaining a high precision more difficult and blurs the $F_1$ measures between approaches.\\

On the hand-annotated \textit{ACAP} test set, the differences are somewhat more pronounced. The $F_1$ measure is $0.48$ for the \textit{TIMIT} model, and rises to $0.52$ with the \textit{DampB} model. The \textit{TimitM} and \textit{DampBB} models both produce $F_1$ measures of $0.49$. The higher over-all values are caused by the more accurate annotations and by the higher-quality singing. Additionally, the data set is much smaller with fewer occurrences of each keyword, which emphasizes both positive and negative tendencies in the detection.\\

In general, recalls are usually close to $1$, and precisions often in the range of $0.2$ to $0.5$ (with much lower and higher outliers). For this reason, an approach that could exploit a configuration with high recalls and then discard unlikely occurrences may offer an improvement. This idea is explored further in section \ref{sec:kws_duration}.

\begin{figure}
        \centering
        \begin{subfigure}[t]{0.4\textwidth}
		 \includegraphics[width=\textwidth]{images/kws_exp1.png}
                \caption{\textit{DampTest}}
                \label{fig:kws_exp1}

        \end{subfigure}%
         %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[t]{0.4\textwidth}
                \includegraphics[width=\textwidth]{images/kws_exp1_acap.png}
                \caption{\textit{ACAP}}
                \label{fig:kws_exp1_acap}
        \end{subfigure}
        \caption{$F_1$ measures for keyword spotting results using posteriorgrams generated with various acoustic models.}
          \end{figure}
%\vspace{-5px}

\subsection{Gender-specific acoustic models}
%TODO: ACAP??
Keyword spotting was also performed on the posteriorgrams generated with the gender-dependent models trained on \textit{DampF} and \textit{DampM} (also described in section \ref{sec:phonerec_acap}). The results are shown in figure \ref{fig:kws_exp2}.

Similar to the phoneme recognition results from Experiment C, the gender-dependent models offer no improvements over the mixed-gender ones of the same size, and are in the same range as the one trained on much more data (\textit{DampB}). The $F_1$ measures for the female test set are $0.48$ for the \textit{DampB} model, and $0.47$ for both the \textit{DampBB} and the \textit{DampFB} model. For the male test set, they are $0.47$ for the \textit{DampB} model, and $0.45$ for the other two.



\subsection{Individual analysis of keyword results}
%TODO: ACAP??
Figure \ref{fig:kws_exp3} shows the individual $F_1$ measures for each keyword using the best model (\textit{DampB}), ordered by their occurrence in the \textit{DampTest} sets from high to low (i.e. number of songs which include the song). There is a tendency for more frequent keywords to be detected more accurately. This happens because a high recall is often achievable, while the precision depends very much on the accuracy of the input posteriorgrams. The more frequent a keyword, the easier it also becomes to achieve a higher precision for it.

As shown in literature \cite{phdthesis:thambiratnam}, the detection accuracy also depends on the length of the keyword: Keywords with more phonemes are usually easier to detect. This explains the relative peak for ``every" in contrast to ``think" or ``night". Since keyword detection systems tend to perform better for longer words and most of the keywords only have 3 or 4 phonemes, the results achieved so far are especially interesting.

One potential source of confusion are sequences of phonemes that overlap with keywords, but are not included in the calculation of the precision. Identically spelled parts of words were included, but split phrases and different spellings were not (e.g. ``away" as part of ``castaway" would be counted, but ``a way" would not be counted as ``away"). This lowers the results artificially and could be an avenue for future improvement. Additionally, only one pronunciation for each keyword was provided, but there may be several possible.

\begin{figure}
 \begin{center}
                \includegraphics[width=0.5\textwidth]{images/kws_exp2.png}
                \caption{$F_1$ measures for keyword spotting results on the \textit{DampTestM} and \textit{DampTestF} data sets using mixed-gender and gender-dependent models.}
                \label{fig:kws_exp2}
                 \end{center}
 \end{figure}

\begin{figure}
 \begin{center}
% 	\begin{subfigure}[t]{0.4\textwidth}
                \includegraphics[width=.6\textwidth]{images/kws_exp3.png}
%               \caption{\textit{DampTest}}
%                \label{fig:kws_exp3_damp}
%       	\end{subfigure}
% 	\begin{subfigure}[t]{0.4\textwidth}
% 		\includegraphics[width=\textwidth]{images/kws_exp3_acap.png}
% 		\caption{\textit{ACAP}}
% 		\label{fig:kws_exp3_acap}
% 	\end{subfigure}
       	
     	 \caption{Individual $F_1$ measures for the results for each keyword, using the acoustic model trained on \textit{DampB}.} 		\label{fig:kws_exp3}
                 \end{center}
 \end{figure}
%\vspace{-5px}





\section{Keyword spotting using duration-informed keyword-filler HMMs}\label{sec:kws_duration}
\subsection{Approach}
As mentioned above, a high recall is easily achievable with the described approach, but the comparatively low precision decreases the over-all result. Therefore, using side information to reject false positives would be a helpful next step.\\
One such source of information are the durations of the detected phonemes. As shown in figure \ref{fig:phoneme_stats}, each phoneme in the \textit{TIMIT} speech database has a fairly fixed duration. In singing, the vowels' durations vary a lot, but the consonants' are still quite predictable. Standard HMMs do not impose any restrictions on the state durations, resulting in a geometric distribution which does not correspond to naturally observed distributions of phoneme durations.\\
As first described in \cite{ferguson}, introducing restrictions on state durations can improve the recognition results. In \cite{juang}, Juang et al. present two basic approaches for duration modeling in HMMs: Internal duration modeling and Post-processor duration modeling.\\

In both approaches, parametric state duration models for each phoneme need to be calculated first \cite{levinson}. Several distributions has been tested for this task (e.g. Gaussian ones), but Burshtein showed that gamma distributions are best at modeling naturally occurring phoneme duration distributions \cite{burshtein}:
\begin{equation}\label{eq:d}
d(\tau) = K \exp \{ - \alpha \tau \} \tau^{p-1}
\end{equation}
where $\tau = 0,1,2,...$ are the possible state durations in frames and $K$ is a normalizing factor. The parameters $\alpha$ and $p$ are estimated according to
\begin{equation}
\hat{\alpha} = \frac{E \{\tau\}}{VAR\{\tau\}} , \hat{p} = \frac{E^{2} \{\tau\}}{VAR\{\tau\}}
\end{equation}
where $E$ is the distribution mean and $VAR$ is the distribution variance. An example is shown in figure \ref{fig:distributions}. In this work, $E$ and $VAR$ are estimated from the phonetically annotated data sets.

\begin{figure}
 \begin{center}
                \includegraphics[width=0.7\textwidth]{images/distributions.png}
                \caption{An example of the empiric duration distribution of one phoneme state (solid line) and three approximations: Gaussian (dashed), geometric(dash-dot), and gamma (dotted). \cite{burshtein}}
                \label{fig:distributions}
                 \end{center}
 \end{figure}

In internal duration modeling, the durations are incorporated directly into the Viterbi alignment. This means that the Viterbi output will already be a state sequence that is optimal with regards to the a-priori phoneme duration knowledge. It is, however, computationally expensive. In previous experiments \cite{kruspe_kws2}, this approach did not produce better results than the much easier to implement post-processor duration modeling. Therefore, this section will focus on that approach.\\

When using post-processor duration modeling, knowledge about plausible phoneme durations is imposed on the result of the Viterbi alignment, the obtained state sequence. This is computationally cheap, but only results in a new likelihood score for the calculated sequence and does not provide better possible state sequences. As described in \cite{juang}, the state sequence obtained from the Viterbi alignment can be re-scored according to:
\begin{equation} \label{eq:pp_ll}
 \log \hat{f} = \log f + \gamma \sum_{k=1}^{N} d_{k}(\tau_k) 
\end{equation}
where $f$ is the original likelihood of the sequence, $\gamma$ is a weighting factor, $k=1...N$ are the discrete states in the state sequence, $\tau_k$ are their durations, and $d_{k}(\tau_k)$ is, again, the probability of state $k$ being active for the duration $\tau_k$.\\
Using keyword-filler HMMs, only one state sequence per utterance is obtained, which either contains the keyword or not. It is therefore not possible to compare these likelihood scores and equation \ref{eq:pp_ll} cannot be applied directly. To still be able to integrate post-processor duration modeling, the HMM parameters are tuned to obtain a high recall value. Then, the duration likelihood (second half of equation \ref{eq:pp_ll}) is calculated for all found occurrences of the keyword and normalized by the number of states taken into account:
\begin{equation}
 dl =  \frac{1}{N} \sum_{k=1}^{N} d_{k}(\tau_)
\end{equation}
Then all occurrences where $dl$ is below a certain threshold are discarded.\\

For the presented results, duration statistics from the \textit{ACAP} data set were used, and only the consonants' durations were taken into account (since vowel durations vary much more as shown in figure \ref{fig:phoneme_stats}). However, additional experiments showed that the result only varies slightly when using speech statistics instead, and when also discarding unlikely vowel durations. This probably happens because the keywords do not contain many states anyway, and because the duration distribution for vowels has a large variance, allowing for a wide range of durations.

\subsection{Results}

\begin{figure}
        \centering
        \begin{subfigure}[c]{0.4\textwidth}
		\includegraphics[width=\textwidth]{images/kws_exp4_acap.png}
                \caption{$F_1$ measures for keyword spotting results on the \textit{ACAP} data set with post-processor duration modeling.}
                \label{fig:kws_exp4_acap}
                
        \end{subfigure}%
         %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[c]{0.3\textwidth}
                \includegraphics[width=\textwidth]{images/kws_exp4.png}
                \caption{$F_1$ measures for keyword spotting results on the \textit{DampTest} data sets with post-processor duration modeling.}
                \label{fig:kws_exp4}
                
        \end{subfigure}
        \caption{$F_1$ measures for keyword spotting results using posteriorgrams generated with various acoustic models with post-processor duration modeling.}
          \end{figure}


Results with and without post-processor duration modeling for the \textit{ACAP} data set are shown in figure \ref{fig:kws_exp4_acap}. The same acoustic models as in the previous experiment were tested. As these results show, $F_1$ measures improve for all configurations when post-processor duration modeling is employed. The effect is somewhat stronger for the \textit{DAMP} models than for the \textit{TIMIT} model. The best result rises from $0.52$ to $0.61$ with the \textit{DampB} model. Analysis of the detailed results shows that the precision can be improved significantly when detected occurrences with implausible phoneme durations are discarded. However, this often also decreases the recall, resulting in the shown $F_1$ results.\\

The approach was also tested on the \textit{DampTest} data sets for two acoustic models. $F_1$ measures on these data sets are generally somewhat blurry for the reasons described in section \ref{sec:kws_kwfhmm}. In this case, the results are just a little bit higher with post-processor duration modeling.


%TODO: clean up
\section{Conclusion}\label{sec:kws_conclusion}
%insgesamt besser
%huge DampB set best, but 3% DampBB almost as good
% even v small DampBB_small better than Timit
% context does not help
% gender models slightly better for kws, not better for phone rec --> variability?
% results phonerec
% results kws - esp. interesting because short
%---auto alignment error source


In this chapter, an approach for keyword spotting using the new acoustic models trained on singing was described. Keyword spotting is performed by extracting phoneme posteriorgrams generated with these new models from the audio, and then running them through a keyword-filler HMM to detect 15 keywords. On the \textit{DampTest} data sets, the resulting $F_1$ measure rises from $0.44$ for the models trained on speech (\textit{TIMIT}) to $0.47$ for the new models. In general, results on the \textit{DampTest} data sets are somewhat inconclusive because the effect of the different models is shadowed by issues with the test data itself - i.e. automatic and thus possibly inaccurate annotations, amateur singing, and a relatively low frequency of keywords because of the large size of the data sets. On the smaller, hand-annotated \textit{ACAP} test set, the results become clearer: The best $F_1$ measure for the models trained on \textit{TIMIT} is $0.48$, and $0.52$ with those trained on \textit{DampB}.\\
This result is especially interesting because most of the keywords have few phonemes. Gender-dependent models perform similar to mixed-gender models of the same size. Individual analysis of the keyword results shows that keywords that occur more frequently are detected more accurately. This probably happens because the approach is able to obtain high recall easily, but precision is an issue. The more frequent a keyword, the easier obtaining higher precisions becomes. Additionally, keywords with more phonemes are detected more accurately than short ones because there is more information to base detection on.\\

One idea to improve precision was using additional information to discard implausible detections. This idea was tested in a second approach by integrating knowledge about phoneme durations. Means and variances of phoneme durations are calculated from annotated data (in particular, the \textit{ACAP} singing data set), and then occurrences with phoneme durations outside of their gamma distributions were ignored. This approach improved $F_1$ measures by up to $9$ percent points on the \textit{ACAP} test data, with the best result being $0.61$. On the \textit{DampTest} data sets, the effect is still existent, but not very pronounced. This is probably because of the blurriness of the result described above.\\

This approach was only tested with MFCC features. As preliminary experiments suggest \cite{kruspe_kws1}, other features like TRAP or PLP may work better on singing. So-called log-mel filterbank features have also been used successfully with DNNs \cite{hinton}. Another interesting factor is the size and configuration of the classifiers, of which only one was tested (after a small grid search to validate this choice).\\
As in the phoneme recognition experiments, there is not much of a difference between the acoustic model trained on the more than 6,000 songs of the \textit{DAMP} data set and the one trained on only $4\%$ of this data. It would be interesting to find the exact point at which additional training data does not further improve the models. On the evaluation side, a keyword spotting approach that allows for pronunciation variants or sub-words may produce better results. Language modeling might also help to alleviate some of the errors made during phoneme recognition.\\
These models have not yet been applied to singing with background music, which would be interesting for practical applications. Since this would probably decrease the result when used on big, unlimited data sets, specialized systems would be more manageable, e.g. for specific music styles, sets of songs, keywords, or applications. Searching for whole phrases instead of short keywords could also make the results better usable in practice.\\
As shown in \cite{mesaros_alignment} and \cite{goto_alignment} and in the next chapter, alignment of textual lyrics and singing already works well. A combined approach that also employs textual information could be very practical.

