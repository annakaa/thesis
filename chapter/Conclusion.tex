\chapter{Conclusion} \label{chap:conclusion}
\section{Summary}
In this work, ASR algorithms for various tasks were applied and adapted to singing. Five such tasks were considered: Phoneme recognition, language identification, keyword spotting, lyrics-to-audio alignment, and lyrics retrieval.\\

Two strategies for improving speech recognition technologies for singing in general were identified: Training better-matching acoustic models for phoneme recognition, and making specific algorithms more robust to singing - either by balancing possible deficits that occur because of the singing-specific characteristics, or by exploiting knowledge about sung vocal production.\\

The main bottleneck in almost all tasks is the \textbf{recognition of phonemes} in singing. Three approaches for this were tested: Recognition with models trained on speech, recognition with models trained on speech that was made more ``song-like'' (``songified''), and recognition with models trained on singing. The main issue in this area of research is the lack of large training data sets of singing with phoneme annotations. For this reason, the ``songified'' approach was developed, which slightly improved results over the models trained on pure speech when applied to sung recordings. For training on actual singing data, the large \textit{DAMP} data set of unaccompanied amateur singing recordings was chosen. Phonemes obtained from the matching textual lyrics were automatically aligned to these recordings, and new acoustic models were trained on the resulting annotated data set. These models showed large improvements for phoneme recognition in singing.\\\\
This training strategy is the main contribution of this work, and forms the basis for all the subsequent tasks. As proposed in the introduction, various algorithms for the individual tasks were then tested and adapted by improving robustness when applied to singing instead of speech, or by taking useful knowledge about singing into account. Robustness was, for example, increased by using i-vector extraction for language identification, by employing keyword-filler HMMs to keyword spotting, and by adapting alignment approaches to enable varying phoneme durations. Singing characteristics are exploited, for example, by basing language identification methods on long recordings, by integrating phoneme duration knowledge into keyword spotting, and by including known phoneme confusions in singing into the alignment and retrieval algorithms.\\
The following paragraphs will sum up the individual adaptations to the tasks in detail:\\
%This training strategy is the main contribution of this work, and forms the basis for all the subsequent tasks. In addition, various singing-specific adaptations were performed for individual ASR problems in singing. These will be summed up in the following paragraphs:\\

For \textbf{language identification} in singing, the state-of-the-art i-vector approach from ASR was tested and produced good results. It is especially suited to singing because it implements a strategy for removing irrelevant and repetitive information, such as signal components caused by the channel. These results improve to a level that is usable in practical applications when long recordings are available, which is usually the case for singing. In addition to this method, a new approach based on phoneme statistics was developed. In this approach, phoneme posteriorgrams are first generated with the models trained in the phoneme recognition experiments, then statistics are calculated, and different models are trained on those. This approach does not perform quite as well as the i-vector method, but is easier to implement under certain circumstances (when phoneme recognition needs to be performed anyway). Long recordings are necessary to calculate meaningful statistics, but, once again, this is not a problem on singing data.\\

\textbf{Keyword spotting} for singing is a task that has not frequently been a subject of research, and there is still a lot of room for improvement. In this work, keyword-filler HMMs were tested. In contrast to other state-of-the-art methods, this approach does not rely on highly stable phoneme durations, which are very common in speech, but not in singing. The method detects keywords with high recall, but often poor precision. Knowledge about probable phoneme durations was obtained from analyzing sung recordings and added in a post-processing step. This improved results for frequently-occurring keywords. The performance may not be good enough for practical applications yet, but these approaches show promise for future research. The length of the keyword (number of phonemes) is a major deciding factor. Searching for long keywords or phrases is already possible in practice, and this is a probable usage scenario for music collections.\\

\textbf{Lyrics-to-audio alignment} is a relatively well-researched topic. In this work, the new acoustic models were also applied to this task, alongside a traditional HMM-based approach. Two methods for utilizing phoneme posteriorgrams for alignment were developed. The first one calculates a DTW between the posteriorgram and a binary representation of the expected lyrics. The resulting path is the alignment. Since no knowledge about the duration of the constituting phonemes is available in the text, and these durations vary widely in singing, the algorithm is modified to not punish such variations. The second approach first extracts a plausible phoneme sequence from the posteriorgram, taking known phoneme confusions in singing into account and compressing extended productions of similar phonemes. Then, the Levenshtein distance between this symbolic (phonetic) representation and the phonemes obtained from the textual lyrics is calculated, and the alignment path is the result.\\
The HMM and DTW approaches perform acceptably, while the Levenshtein approach produces much better results. These algorithms are suited for practical applications. One such example was described: The automatic detection and removal of expletives in songs.\\

\textbf{Lyrics retrieval} from a textual database based on sung queries is performed with the same algorithms as the alignment. Alignment is performed between the posteriorgram generated from the audio and all possible lyrics. Then, the ones with the best scores/lowest distances are selected as the result.\\
Once again, the DTW approach performs feasibly, while the Levenshtein approach produces very good results that are practically usable, even on short queries. Additionally, it allows for fast search and optimizations on large lyrics databases, which is a so-far underresearched use case.

\section{Contributions}


Major research contributions of this work include:

\paragraph{A new large data set of phonetic annotations for unaccompanied singing}
As described in sections \ref{sec:data_damp} and \ref{sec:phonerec_acap}, the pre-existing \textit{DAMP} data set of unaccompanied singing was used as a basis for this. The matching textual lyrics were scraped from the internet, and automatically aligned to the audio with an HMM model trained on speech. Various strategies were tested for this. The resulting phoneme annotations are not 100\% accurate, but the data set is very large and these inaccuracies balance out, as the experiments building on the data demonstrate. Various configurations for this data set were composed for different purposes, including female and male singing test sets.

\paragraph{Acoustic models trained on this data set}
New acoustic models (DNNs) were then trained on the generated training data sets (section \ref{sec:phonerec_acap}). The performance of these models was thoroughly compared to others, and they generally performed better when applied to singing phoneme recognition. They were also integrated into the other tasks (language identification, keyword spotting, lyrics-to-audio alignment and retrieval) and demonstrated improvements in almost all cases.
%+evaluation

\paragraph{A novel approach for language identification based on phoneme statistics}
In addition to an i-vector-based approach from ASR, a completely new method for language identification was developed on the basis of phoneme posteriorgrams. This approach is particularly suited to singing because long recordings are often available in music applications. As described in section \ref{sec:lid_stats}, phoneme statistics are calculated on these posteriorgrams over long time frames (e.g. whole songs), and then new models are trained on these statistics. Unseen recordings can then be subjected to the same process to determine their languages. The results of this approach were not quite as good as those of the i-vector algorithm, but they show promise. This method is easier to implement than the one using i-vectors when long audio sequences are available, and when phoneme posteriorgrams need to be extracted from them for other purposes.

\paragraph{A new method for flexibly integrating knowledge about phoneme durations into keyword spotting}
The presented approach for keyword spotting is based on keyword-filler HMMs. In order to improve its results, the algorithm was expanded to take a-priori knowledge about the plausible durations of individual phonemes into account (see section \ref{sec:kws_duration}). This was implemented via post-processor duration modeling: The model is tuned to return many results (resulting in a high recall), and then those with improbable phoneme durations are discarded to improve the precision. Influence of individual phoneme durations is easily turned on or off.  This form of duration modeling has not been applied to keyword-filler HMMs before. 

\paragraph{A novel method for extracting plausible phoneme sequences from posteriorgrams}
The described phoneme recognition approaches result in phoneme posteriorgrams, and many algorithms operate directly on them. However, there are also applications where a fixed phoneme sequence is required. In section \ref{sec:retrieval_phone}, a new method for extracting such sequences from posteriorgrams is presented. Traditionally, HMMs would be used for this task, but they have a number of disadvantages in the use case at hand. The results of this phoneme extraction method are further used in this work for alignment and retrieval.

\paragraph{Two new approaches for lyrics-to-audio alignment}
In addition to classic HMM-based lyrics-to-audio alignment, this thesis presents two novel approaches that operate on posteriorgrams: One based on DTW, and one based on Levenshtein distance calculation. This enables them to make use of the new acoustic models. The developed algorithms were submitted to the \textit{MIREX} 2017 challenge for lyrics-to-audio alignment, where the Levenshtein-based approach outperformed the other submissions.

\paragraph{Two first approaches to lyrics retrieval based purely on sung queries}
The same two approaches for lyrics-to-audio alignment can be applied to the task of lyrics retrieval. This is an application that has only rarely been the subject of research so far. This thesis describes the implementation of the presented methods for retrieval of textual song lyrics from a database with sung queries.


\section{Limitations and future work}
%applications - genre, mood, similarity
% robustness for asr tasks
%other languages
%polyphonic

Suggestions for the individual tasks and approaches are described in the corresponding chapters. This section focuses on the over-all field of ASR for singing.\\
In general, phoneme recognition is still the major bottleneck for many of the described tasks. This thesis shows possible ways to improve this recognition. Future research approaches may employ even larger data sets of singing for training, preferably with more reliable phonetic annotations (e.g. manual annotations). Alternatively, models could be trained in an unsupervised or semi-supervised way with large amounts of unannotated data and small sets of reliably annotated data. New machine learning techniques could be tested as well, such as CNNs, RNNs, or end-to-end models, or methods for unsupervised pre-training like autoencoders.\\
Unusual or unclear pronunciations, accents, and unusual voices (e.g. children's voices) also lead to unsatisfactory results in many tasks. New models trained on a larger variety of data would improve robustness. The same applies to problems with the audio quality or channels, and recording conditions not seen during training.\\

Some of the algorithms require exact annotations (e.g. alignment and retrieval). This is not always a given in a real-world scenario, where singers will perform different words, additional vocalizations, unexpected repetitions etc. The actual calculations could be made robust to such changes.\\

Most of the presented approaches have so far only been applied to unaccompanied singing. A major step forward would be the adaptation to polyphonic music. This was already done for alignment and retrieval, but could be improved significantly. There are several possible routes that future research could take. First, acoustic models could be trained on accompanied singing instead. This would require large amounts of realistic training data, and probably more sophisticated models in order to represent the more complex structures. Second, source separation could be integrated to extract the singing track from the audio, and only perform the analysis on this part. As a third alternative, Vocal Activity Detection could be applied beforehand to only analyze the segments where actual singing occurs. In the presented experiments, instrumental solos in particular frequently led to misalignments or recognition errors.\\

Finally, the developed algorithms could be employed for the applications described in section \ref{sec:intro_motivation}. It would be highly interesting to see them integrated into other MIR systems, e.g. for genre or regional classification or for mood detection. The other practical scenarios could also make good use of them, for example in karaoke systems, in audio identification, or in cover song detection. Search algorithms or similarity calculation based on the analyzed characteristics could be used in practical MIR systems.
