%\renewcommand{\abstractname}{Kurzfassung}
%\begin{otherlanguage}{english}
\begin{abstract}
The research field of Music Information Retrieval is concerned with the automatic analysis of musical characteristics. One aspect that has not received much attention so far is the automatic analysis of sung lyrics. On the other hand, the field of Automatic Speech Recognition has produced many methods for the automatic analysis of speech, but those have rarely been employed for singing so far. This thesis analyzes the feasibility of applying various speech recognition methods to singing, and suggests adaptations. In addition, the routes to practical applications for these systems are described. Five tasks are considered: Phoneme recognition, language identification, keyword spotting, lyrics-to-audio alignment, and retrieval of lyrics from sung queries.\\
For the task of phoneme recognition, two large data sets were created. The first one is generated by making speech recordings more ``song-like''. The second one is based on an existing singing data set with matching textual lyrics, which are automatically aligned to the audio and then used for training new acoustic models. The phoneme error rate is improved from $1.08$ to $0.77$ on singing.\\
Two methods for language identification are presented. The first one employs i-vector extraction and achieves an accuracy of $0.73$ on singing. The second method is a completely new approach based on the extraction of phoneme statistics with the new acoustic models. The accuracy is lower at $0.63$, but the approach is promising for systems in which phoneme extraction is performed.\\
For the keyword spotting task, the new acoustic models were integrated into a keyword-filler HMM system. Additionally, duration modeling is applied to the results. In an evaluation with 15 short keywords, an $F_1 $ measure of $0.61$ is obtained.\\
In both lyrics-to-singing alignment and retrieval of textual lyrics from sung queries, an alignment between the audio recording and the text lyrics is performed. For retrieval, the scores for all possible lyrics are compared. A first approach employs ``classic'' Viterbi alignment, and is used for the generation of the previously mentioned singing data set. Two new approaches perform alignment between the lyrics' phonemes and phoneme posteriorgrams extracted with the new acoustic models, either with Dynamic Time Warping or with Levenshtein alignment. The last approach performed best in the \textit{MIREX} 2017 lyrics-to-audio alignment challenge, and achieves accuracies of $0.54$ for short segments and $0.94$ for whole-song queries. Additionally, a practical application to expletive detection in singing is presented.
\end{abstract}
%\end{otherlanguage}
\begin{otherlanguage}{ngerman}
%\renewcommand{\abstractname}{Kurzfassung}
%\newcommand{\dtabstract}{\hyphenpenalty=10000}
%{\dtabstract
\begin{abstract}
Das Gebiet des Music Information Retrieval befasst sich mit der automatischen Analyse von musikalischen Charakteristika. Ein Aspekt, der bisher kaum erforscht wurde, ist dabei der gesungene Text. Auf der anderen Seite werden in der automatischen Spracherkennung viele Methoden für die automatische Analyse von Sprache entwickelt, jedoch selten für Gesang. Die vorliegende Arbeit untersucht die Anwendung von Methoden aus der Spracherkennung auf Gesang und beschreibt mögliche Anpassungen. Zudem werden Wege zur praktischen Anwendung dieser Ansätze aufgezeigt. Fünf Themen werden dabei betrachtet: Phonemerkennung, Sprachenidentifikation, Schlagwortsuche, Text-zu-Gesangs-Alignment und Suche von Texten anhand von gesungenen Anfragen.\\
Für die Phonemerkennung wurden zwei neue Datensätze erzeugt. Für den ersten wurden Sprachaufnahmen künstlich ähnlicher zu Gesang gemacht. Für den zweiten wurden Texte automatisch zu einem vorhandenen Gesangsdatensatz alignt, der dann zum Trainieren neuer akustischer Modelle genutzt wurde. Die Phoneme Error Rate sinkt dabei von $1.08$ auf $0.77$.\\
Zwei Ansätze für die Sprachenidentifikation werden vorgestellt. Im ersten kommt die i-vector-Extraktion zum Einsatz, und die Accuracy liegt bei $0.73$. Die zweite Methode ist ein völlig neuer Ansatz, bei dem mit Hilfe der neuen akustischen Modelle Phonemstatistiken berechnet werden. Die Accuracy ist hier $0.63$, aber der Ansatz ist vielversprechend für Systeme, in denen Phoneme erkannt werden.\\
Für die Schlagwortsuche wurden die neuen akustischen Modelle in ein Keyword-Filler-HMM-System integriert. Auch Duration Modeling wurde eingesetzt. Für 15 kurze Schlagworte wird ein $F_1$-Maß von $0.61$ erzielt.\\
Im Text-zu-Gesangs-Alignment und in der Textsuche wird jeweils ein Alignment zwischen der Audioaufnahme und den Texten berechnet. Für die Textsuche werden dann die Ergebnisse für alle möglichen Texte verglichen. In einem ersten Versuch wird ``klassisches'' Viterbi-Alignment eingesetzt, u.a. auch zur Erzeugung des erwähnten Datensatzes. In zwei neuen Ansätzen wird das Alignment zwischen den Phonemen des Textes und mit den neuen akustischen Modellen erzeugten Posteriorgrammen berechnet, entweder mit Dynamic Time Warping oder mit der Levenshtein-Distanz. Die letztgenannte Methode erzielte bei der \textit{MIREX} 2017 Lyrics-to-Audio Alignment Challenge die besten Ergebnisse, und erreicht Accuracies von $0.54$ für kurze Segmente und $0.94$ für ganze Songs. Außerdem wird eine praktische Anwendung für die automatische Schimpfwortsuche vorgestellt.
\end{abstract}

\end{otherlanguage}
