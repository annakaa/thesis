\chapter{Introduction} \label{chap:introduction}
\section{Motivation}\label{sec:intro_motivation}
%mir - no lyrics recognition
%asr - no singing
%similar
%applications

Ever since the widespread introduction of digital formats for music, professional and personal music collections have grown exponentially. Efficient search algorithms are necessary for managing these huge collections. In the past $\sim$15 years, many interesting technologies have been developed to make it easier for users to efficiently search these collections by certain semantic criteria, such as tempo, mood, genre, instruments, etc. This field of research is called Music Information Retrieval (MIR) \cite{incollection:mir}. One characteristic that has not received much research attention yet is the lyrical content of songs, even though this information is useful for many practical applications, and could aid other MIR tasks.\\

On the other hand, Automatic Speech Recognition (ASR) has been an active field of research for more than 60 years now \cite{juang_rabiner_history} and encompasses a large variety of research topics. However, speech recognition algorithms have so far only rarely been adapted to singing. One of the reasons for this seems to be that most of these tasks get harder when using singing because singing data has different characteristics, which are also often more varied than in pure speech \cite{loscos}. For example, the typical fundamental frequency for female speech lies between $165$ and 200Hz, while in singing it can reach more than 1000Hz. Other differences include harmonics, durations, pronunciation, and vibrato, as well as difficulties introduced with background music.\\

Generally, both fields of research are strongly related and utilize many of the same approaches and technologies. This overlap, however, has not been explored thoroughly. This work aims to look at this relation more closely, and to apply and adapt ASR technologies to singing.\\

The possibilities for practical use of such technologies are manifold. Such applications include, but are not limited to:
\begin{description}
\item[Direct search of songs based on their lyrical content] Potential users could search for songs by their language, lyrical phrases, or keywords. This is useful for such applications as language learning, finding songs on certain topics, advertisement etc.
\item[Improvement of similarity search and playlist generation] Similarity dimensions could include the sung language, keywords, or topics.
 \item[Improvement of regional classification] As described in \cite{kruspe11}, human subjects tend to rely on the language to determine the region of origin of a musical piece. This is not taken into account by current regional classification systems.
 \item[Improvement of genre classification] Similar to regional classification, certain musical genres are closely connected to a single singing language, or to certain keywords. Considering the ``glass ceiling'' of approximately 80\% accuracy for many classification tasks in MIR \cite{glass_ceiling}, new hybrid approaches are necessary to improve them.
 \item[Improvement of mood detection] Words can be indicatory of specific moods. By exploiting those, mood detection in music could be expanded with an additional dimension.
 \item[Lyrics alignment for karaoke] Automatically aligned lyrics could be used in karaoke systems to enable users to sing any song they want to.
\item[Lyrics retrieval from databases] Textual lyrics can be retrieved from a database with just a short sung recording as the input. This is, once again, useful for karaoke. 
\item[Lyrics identification] It is possible to compute compressed representations of detected lyrics. These could be used to aid audio identification (query-by-humming) technologies by utilizing lyrics information in addition to the melodic and harmonic characteristics used so far. 
\item[Cover song detection by lyrics] In the same vein, alternative or auxiliary technologies for cover song detection through lyrics analysis are possible.
\item[Lyrics transcription] Given an audio recording, it will eventually be possible to automatically transcribe the full lyrics for users.
\item[Singing generation] Similar to recent approaches for speech synthesis \cite{wavenet}, ASR technologies for singing could be used to automatically generate singing audio.
\end{description}


\section{Research objectives}
As described, speech recognition becomes more difficult when applied to singing. This thesis highlights strategies to improve various recognition tasks. There are two main starting points for this improvement:
\begin{enumerate}
\item Training better acoustic models for phoneme recognition, which forms the basis of many speech recognition tasks
\item Adapting subsequent algorithms for the various tasks to singing by either making them more robust to singing characteristics, or by exploiting knowledge about sung performances
\end{enumerate}

To show how these improvements can impact speech recognition in singing, five topics were researched for this thesis:

\paragraph{Phoneme recognition}
Phoneme recognition describes the task of determining the sung sounds (phonemes) occurring in an audio recording. This forms the basis for many other tasks; first and foremost, lyrics transcription, but also almost all other tasks in this work. Phoneme recognition tends to be the bottleneck component in systems for ASR in singing. Inaccurate results at this step will lead to inaccurate results in the subsequent ones.\\
As will be shown, phoneme recognition in singing has so far been performed with models trained on speech; these are, of course, not optimal. The reason why models have so far not been trained on singing is the lack of available training data for this task. This thesis presents ways around this problem. Specifically, acoustic models are trained on speech data that has been made more ``song-like'', and on singing data with automatically generated phoneme annotations. These new models lead to improvements on this and all the following tasks.

\paragraph{Language identification}
Language identification is the task of detecting the language in which a sung recording is performed. This has many practical applications, as described above: The results could be employed to directly search for music in certain languages (e.g. for language learning or for advertisements), to improve similarity search algorithms, or to support regional and genre classification.\\
There are very few publications dealing with sung language identification so far. In this thesis, a state-of-the-art approach from the field of Automatic Speech Recognition (ASR) is applied to the problem, and a completely new one based on phoneme statistics is presented.

\paragraph{Keyword spotting}
During keyword spotting, a set of singing recordings is searched for a specific keyword. Just like language identification, there are practical motivations for this. Keyword-based search systems are useful for finding songs on certain topics, for playlist generation, similarity search, genre classification, or for mood detection.\\
Once again, there are very few published approaches for this task. This thesis presents the first approach for English-language keyword spotting of arbitrary keywords without side information (like the musical score or sung samples of the keyword). Additionally, a new method for integrating knowledge about plausible phoneme durations is described.

\paragraph{Lyrics-to-audio alignment}
Using an audio recording and its known textual lyrics, lyrics-to-audio alignment methods are able to determine where each phrase, word, or phoneme occurs in time. In contrast to the other tasks, this topic is already relatively well-researched. It is a sought-after technology for karaoke applications, or for supporting other speech recognition tasks (for example, keyword spotting becomes much easier when the textual lyrics are available).\\
In this thesis, classic HMM-based alignment is first used as an auxiliary technology to create a new training data set for phoneme recognition. Then, two new methods are presented: One based on Dynamic Time Warping (DTW) on the results of the phoneme recognition, and one based on Levenshtein distance calculation on phoneme sequences.

\paragraph{Lyrics retrieval}
As another research topic that has not received much attention so far, lyrics retrieval is the task of finding the correct textual lyrics (and consequently the correct song) in a database given a sung query. This is, again, useful for karaoke systems or generally for voice-based search.\\
This work presents new approaches for this task that utilize the same technologies as the audio alignment. Compared to the state of the art, these are the first systems that do not require melody information in addition to the lyrics, and also work directly on the detected phonemes without a language modeling step required (which is, in effect, a text search on the detected phrases).\\

Generally, this thesis focuses on unaccompanied singing in order to determine how much the developed algorithms directly improve results on singing in comparison to speech. Only the alignment task was tested on polyphonic music. Expanding the resulting systems to full songs is an important future step.

\section{Thesis structure}
The thesis is structured into nine chapters:
\begin{description}
\item[1 Introduction] This chapter. Motivates the work and describes the research goals.
\item[2 Technical background] Describes the various algorithms for feature extraction, machine learning, and distance calculation used throughout this work. Also explains the general system structures of the developed approaches as well as their evaluation.
\item[3 State of the art] Summarizes existing methods for solving the mentioned research objectives.
\item[4 Data sets] An overview of the various data sets used for training and testing the developed approaches.
\item[5 Singing phoneme recognition] Describes the approaches developed for the phoneme recognition task and their evaluation results.
\item[6 Sung language identification] Presents the developed methods for language identification and their evaluation results.
\item[7 Sung keyword spotting] Explains the keyword spotting algorithms and their evaluation results.
\item[8 Lyrics retrieval and alignment] Describes the developed systems for lyrics alignment and retrieval and their evaluation results.
\item[9 Conclusion] Summarizes the work, points out the major contributions, and suggests future research directions.
\end{description}


\section{Publications}
The achieved research results have been published in the following conference papers:

\paragraph{Phoneme recognition}
\begin{itemize}
\item Anna M. Kruspe, ``Training phoneme models for singing with "songified" speech data'', in \textit{16th International Society for Music Information Retrieval Conference (ISMIR)}, Malaga, Spain, 2015.
\item Anna M. Kruspe, ``Bootstrapping a system for phoneme recognition and keyword spotting in unaccompanied singing'', in \textit{17th International Society for Music Information Retrieval Conference (ISMIR)}, New York, NY, USA, 2016.
\end{itemize}

\paragraph{Language identification}
\begin{itemize}
\item Anna M. Kruspe, ``Automatic Language Identification for Singing'', in \textit{Mid-Atlantic Student Colloquium on Speech, Language and Learning (MASC-SLL)}, Baltimore, MD, USA, 2013.
\item Anna M. Kruspe, Jakob Abesser, Christian Dittmar, ``A GMM approach to singing language identification'', in \textit{Proc. of the AES Conference on Semantic Audio}, London, UK, 2014.
\item Anna M. Kruspe, ``Improving singing language identification through i-vector extraction'', in \textit{Proc. of the 17th Int. Conference on Digital Audio Effects (DAFx-14)}, Erlangen, Germany, 2014.
\item Anna M. Kruspe, ``Phonotactic Language Identification for Singing'', in \textit{Interspeech}, San Francisco, CA, USA, 2016.
\end{itemize}

\paragraph{Keyword spotting}
\begin{itemize}
\item Anna M. Kruspe, ``Keyword spotting in a-capella singing'', in \textit{15th International Society for Music Information Retrieval Conference (ISMIR)}, Taipei, Taiwan, 2014.
\item Anna M. Kruspe, ``Keyword spotting in singing with duration-modeled HMMs'', in \textit{European Signal Processing Conference (EUSIPCO)}, Nice, France, 2015.
\item Anna M. Kruspe, ``Bootstrapping a system for phoneme recognition and keyword spotting in unaccompanied singing'', in \textit{17th International Society for Music Information Retrieval Conference (ISMIR)}, New York, NY, USA, 2016.
\end{itemize}

\paragraph{Lyrics alignment and retrieval}
\begin{itemize}
\item Anna M. Kruspe, ``Retrieval of textual song lyrics from sung inputs'', in \textit{Interspeech}, San Francisco, CA, USA, 2016.
\item Anna M. Kruspe, ``Automatic B**** Detection'', in \textit{17th International Society for Music Information Retrieval Conference (ISMIR)} (Late-breaking demo), New York, NY, USA, 2016.
\item Anna M. Kruspe, Jakob Abesser, ``Automatic lyrics alignment and retrieval from singing audio'', in \textit{Proc. of the AES Conference on Semantic Audio} (Late-breaking demo), Erlangen, Germany, 2017.
\item Anna M. Kruspe, ``Lyrics alignment using HMMs, posteriorgram-based DTW, and phoneme-based Levenshtein alignment'', in \textit{18th International Society for Music Information Retrieval Conference (ISMIR)} (\textit{MIREX} submission), Suzhou, China, 2017.
\item Anna M. Kruspe, Masataka Goto, ``Retrieval of song lyrics from sung queries'', \textit{IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, Calgary, Canada, 2018.
\item Eric J. Humphrey, Rachel M. Bittner, Andrew Demetriou, Sankalp Gulati, Andreas Jansson, Tristan Jehan, Bernhard Lehner, Anna Kruspe, Aparna Kumar, Sravana Reddy, Prem Seetharaman, Luwei Yang, ``Signal Processing for Singing Voice Analysis: Significance, Applications, and Methods''. For consideration in the \textit{IEEE Signal Processing Magazine}, 2018.
\end{itemize}

