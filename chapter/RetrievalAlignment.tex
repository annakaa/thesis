\chapter{Lyrics Retrieval and Alignment} \label{chap:retrieval_alignment}

Lyrics retrieval and alignment are related tasks: The retrieval task can be interpreted as an alignment of all possible lyrics sequences to the query audio, and a subsequent selection of the best-matching result. This is done in all described algorithms.\\
\begin{figure}
	\begin{center}
		\includegraphics[width=0.8\textwidth]{images/process_classification_alignment.png}
		\caption{Schematic of the procedure for lyrics-to-audio alignment.}
		\label{fig:process_alignment}
	\end{center}
\end{figure}
In this work, alignment is also performed on phoneme posteriorgrams. The lyrics with their phonetic pronunciation must be known in advance; then, an algorithm finds the best positions of each phoneme in the sequence in the posteriorgram. Traditionally, Viterbi decoding is used for this, with the transition matrix shaped such that only transitions through the expected phonemes are possible. The general process is shown in figure \ref{fig:process_alignment}.

\begin{figure}
	\begin{center}
		\includegraphics[width=0.9\textwidth]{images/process_retrieval.png}
		\caption{Schematic of the procedure for lyrics retrieval.}
		\label{fig:process_retrieval}
	\end{center}
\end{figure}
For retrieval, alignment is performed on a whole database of lyrics instead of just those for a single song as illusted in figure \ref{fig:process_retrieval}. Then, the scores for each alignment are compared, and the highest one determines the best match.\\

As a starting point, a traditional HMM-based algorithm for Forced Alignment was tested. Building on top of the posteriorgrams extracted with the new acoustic models as described in chapter \ref{chap:phonerec}, a new algorithm using Dynamic Time Warping (DTW) was developed and evaluated for both alignment and retrieval. Next, another processing step for explicitly extracting phonemes from the posteriorgrams was included; the resulting method was also tested for both alignment and retrieval. The general process is shown in figure \ref{fig:process_alignment}.\\

For alignment evaluation, the approaches were submitted to the \textit{MIREX} 2017 challenge for lyrics-to-audio alignment\footnote{\url{http://www.music-ir.org/mirex/wiki/2017:Automatic_Lyrics-to-Audio_Alignment}}, in which the algorithms were tested on Hansen's dataset, both in the singing-only and polyphonic conditions (\textit{ACAP} and \textit{ACAP\_Poly}, see sections \ref{subsec:data_hansen} and \ref{subsec:data_hansen_polyphonic}), and on Mauch's alignment data set (\textit{Mauch}, see section \ref{subsec:data_mauch_alignment}). In this challenge, the mean absolute error in seconds was used as the evaluation measure. For its calculation, all absolute deviations between the expected phoneme start and end timestamps and the automatically detected ones are calculated, averaged over the whole song, and once again over all songs. The measure was suggested in \cite{mesaros_alignment}. The full results can be viewed on \url{http://www.music-ir.org/mirex/wiki/2017:Automatic_Lyrics-to-Audio_Alignment_Results}.\\

For evaluation of the retrieval results, the accuracy when taking the first $N$ number of results into account (also called recognition rate in the state of the art) is calculated. For clarification: The first $N$ results are checked for occurrence of the expected result; then, the percentage of queries for which the correct result was detected is calculated. (In literature, this is sometimes called the retrieval rate or recognition rate.) An alternative measure for tasks like this is the Mean Reciprocal Rank (i.e. the average inverse of the position where the expected result occurs); once again, the previously described measure was used for comparability with the state of the art. Retrieval is tested on the \textit{AuthorRetrieval}, \textit{DampRetrieval}, and \textit{DampTest} data sets. Since these are dedicated test sets and model training is performed on much larger data sets, no cross validation is necessary once again. \\

This chapter also presents a practical application for lyrics alignment: Automatic expletive detection in rap songs.


\section{HMM-based lyrics-to-audio alignment}\label{sec:ret_hmm}
For this algorithm, monophone HMMs are trained on the \textit{TIMIT} speech data using the HTK framework \cite{htk}. Then, Viterbi alignment is run to align the phonemes to the singing audio. These are the same models used to align lyrics to the \textit{DAMP} audio data to generate the training data set described in section \ref{sec:phonerec_acap}. The results in the \textit{MIREX} challenge are shown in figure \ref{fig:results_align_hmm}. The detailed results for all tested songs are given in appendix \ref{app:mirex}.\\
On the unaccompanied data set, the mean error is 5.11s, while the median error is 0.67s. The large difference comes about because one of the tested songs (``Clocks'') produced high error values in every approach. Considering only the other songs, the highest error is 1.45s, and all others are lower than 1s. This is a good result, especially considering that the approach is relatively simple and trained on speech data only. A manual check suggests that the results are usable in practice.\\
On the accompanied version of the same data set, the average error is 13.96s and the median error is 7.64s. On the \textit{Mauch} data set, the mean error is at 17.7s and the median error is at 10.14s. Once again, the error varies widely across the songs. As expected, the result is worse since no measures were taken make the approach robust to background music. Interestingly, other submitted approaches that employ source separation did not fare much better on this data set either, suggesting that the core algorithm may be better. It would be interesting to see results for the HMM-based algorithm on polyphonic music with a source separation step (or at least a vocal activity detection step) included.\\
%TODO erklären
\begin{figure}
 \begin{center}
                \includegraphics[width=0.45\textwidth]{images/res_align_hmm.png}
               \caption{\textit{MIREX} results on all three data sets using the HMM-based approach.}
                \label{fig:results_align_hmm}
                 \end{center}
 \end{figure}


\section{Posteriorgram-based retrieval and alignment}\label{sec:ret_post}

\begin{figure}
 \begin{center}
                \includegraphics[width=1\textwidth]{images/process_alignment_dtw.png}
                \caption{Overview of the DTW-based lyrics alignment and retrieval method.}
                \label{fig:retrieval_dtw_process}
                 \end{center}
 \end{figure}

A new approach is based on the posteriorgrams generated with the DNN acoustic models described in section \ref{sec:phonerec_acap}; the procedure is shown in figure \ref{fig:retrieval_dtw_process}. In order to align lyrics to these posteriorgrams, binary templates are generated from the text lyrics on the phoneme scale. These can be seen as oracle posteriorgrams, but do not include any timing information.\\
Between this template and the query posteriogram, a similarity matrix is calculated using the cosine distance. On the resulting matrix, DTW is performed using the implementation from \cite{ellis_dtw} to obtain the alignment. An example is shown in figure \ref{fig:retrieval_dtw}.\\
Two optimizations were made to the algorithm. The first one is a sub-sampling of the phoneme posteriorgrams by the factor 10 (specifically, the mean for 10 consecutive frames is calculated). This increases the speed of the DTW for comparisons and also produces better results. Longer windows were also tested, but this had a negative impact on the result.\\
Secondly, squaring the posteriorgrams before the similarity calculation produces slightly better results. This makes the posteriorgrams more similar to the binary lyrics templates. Binarizing them was also tested, but this emphasized phoneme recognition errors too much.\\

In the retrieval case, the binary templates are generated for all possible lyrics in the database, and the DTW calculation is performed for each of them with the query posteriorgram. The DTW cost is used as the measure to obtain the best-matching lyrics. Since the phoneme durations in the actual recording and the lyrics templates have different lengths, the length of the warping path should not be a detrimental factor in cost calculation. 
Therefore, the accumulative cost of the best path is divided by the path length and then retained as a score for each possible lyrics document. In the end, the lyrics document with the lowest cost is chosen as a match (or, in some experiments, the N documents with the lowest costs).\\
As an additional experiment, both the textual lyrics corpus and the sung inputs were split into smaller segments roughly corresponding to one line in the lyrics each (around 12,000 lines). The retrieval process was then repeated for these inputs. This allowed an evaluation as to how well lyrics can be retrieved from just one single sung line of the song. (Sub-sequence DTW could also be used for this task instead of splitting both corpora.)\\


\begin{figure}
	\centering
	\begin{subfigure}[t]{0.5\textwidth}
		\includegraphics[width=\textwidth]{images/retrieval_posts.png}
		\caption{Phoneme posteriorgram}
		
	\end{subfigure}%
	%add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
	%(or a blank line to force the subfigure onto a new line)
	\begin{subfigure}[t]{0.5\textwidth}
		\includegraphics[width=\textwidth]{images/retrieval_template.png}
		\caption{Phoneme template}
	\end{subfigure}

	\begin{subfigure}[t]{0.5\textwidth}
		\includegraphics[width=\textwidth]{images/retrieval_similarity.png}
		\caption{Similarity matrix with cheapest path (blue)}
	\end{subfigure}
	\caption{Example of a similarity calculation: Phoneme posteriorgrams are calculated for the audio recordings \textbf{(a)}. Phoneme templates are generated for the textual lyrics \textbf{(b)}. Then, a similarity matrix is calculated using the cosine distance between the two, and DTW is performed on it \textbf{(c)}. The accumulated cost divided by the path length is the similarity measure.}\label{fig:retrieval_dtw}
\end{figure}

\subsection{Alignment experiments}
The results for this approach in the \textit{MIREX} challenge are displayed in figure \ref{fig:results_align_dtw}, with the detailed results given in appendix \ref{app:mirex}. Overall, errors are much higher than with the HMM-based approach: The mean error is 9.77s for the \textit{ACAP} data set, 27.94s on the \textit{ACAP\_Poly} data set, and 22.23s on the \textit{Mauch} data set. This presumably happens because in contrast to the HMM approach, the algorithm does not have any information about phoneme priors and transition probabilities, neither implicitly nor explicitly. DTW on the posteriorgram is a relatively rudimentary method for performing alignments. Nevertheless, a manual check of the results suggests that the algorithm is still able to produce usable results in many cases, with many song-wise alignment errors for the \textit{ACAP} data set still being below 1s. In particular, it works acceptably when the posteriorgram is not too noisy. This is also corroborated by the retrieval results for unaccompanied queries presented in the following.\\
As a future step, incorporating phoneme transition information (e.g. in the form of DNN-HMMs) could help mitigate some of the noise in the posteriorgram caused by model inaccuracies or by background music.

\begin{figure}
 \begin{center}
                \includegraphics[width=0.5\textwidth]{images/res_align_dtw.png}
               \caption{\textit{MIREX} results on all three data sets using the DTW-based approach.}
                \label{fig:results_align_dtw}
                 \end{center}
 \end{figure}



\subsection{Retrieval experiments on whole-song inputs}
In the first retrieval experiment, similarity measures were calculated between the lyrics and recordings of whole songs using the described process. This was tested with phoneme posteriorgrams obtained with all five acoustic models on the female and the male test sets (\textit{DampTestF} and \textit{DampTestM}). The accuracy was then calculated on the Top-1, -3, and -10 results for each song (i.e., how many lyrics are correctly detected when taking into account the 1, 3, and 10 lowest distances?). The results on the female test set are shown in figure \ref{fig:retrieval_dtwres_testF}, the ones for the male test set in figure \ref{fig:retrieval_dtwres_testM}.

\begin{figure}
       \centering
        \begin{subfigure}[c]{0.4\textwidth}
	    \includegraphics[width=\textwidth]{images/retrieval_dtwres_testF.png}
                \caption{DampTestF}
                \label{fig:retrieval_dtwres_testF}
        \end{subfigure}%
         %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
          %(or a blank line to force the subfigure onto a new line)
         \vspace{10px}
        \begin{subfigure}[c]{0.4\textwidth}
	  \includegraphics[width=\textwidth]{images/retrieval_dtwres_testM.png}
                \caption{DampTestM}
                \label{fig:retrieval_dtwres_testM}
        \end{subfigure}

        \caption{Accuracies of the results for lyrics detection on the whole song for the \textit{DampTest} sets using the DTW-based approach with five different acoustic models, and evaluated on the Top-1, -3, and -10 results.}\label{fig:retrieval_dtwres}
\end{figure}

 These results show that phoneme posteriorgrams obtained with models trained on speech data (\textit{TIMIT}) generally produce the lowest results in lyrics retrieval. The difference between the two test sets is especially interesting here: On the male test set, the accuracy for the single best result is $58\%$, while on the female set it is only $39\%$. Previous experiments showed that the phoneme recognition itself performs somewhat worse for female singing inputs. This effect is compounded in the lyrics retrieval results. This may happen because the frequency range of female singing is even further removed from that of speech than the frequency range of male singing is \cite{sundberg}. Even female speech is often performed at the lower end of the female singing frequency range. The frequency range of male singing is better covered when training models on speech recordings (especially when speech recordings of both genders are used).\\
 This effect is still visible for the \textit{TimitM} models, which is the variant of \textit{TIMIT} that was artificially made more ``song-like''. However, the pitch range was not expanded too far in order to keep the sound natural.\\
The results improve massively when acoustic models trained on any of the \textit{DAMP} singing corpora are used. The difference between the male and female results disappears, which supports the idea that the female pitch range was not covered well by the models trained on speech. Using the models trained on the smallest singing data set (\textit{DampBB\_small}), which is slightly smaller than \textit{TIMIT}, the results increase to $81\%$ and $83\%$ for the single best result on the female and the male test set respectively. With the models trained on the \textit{DampBB} corpus, which is about twice as big, they rise slightly more to $85\%$ on the female test set. Gender-specific models of the same size do not improve the results.\\
Finally, the results obtained with the acoustic models trained on the largest singing corpus (\textit{DampB}) provide the very best results at accuracies of $87\%$ and $85\%$ (female/male).\\
For some applications, working with the best N instead of just the very best result can be useful (e.g. for presenting a selection of possible lyrics to a user). When the best 3 results can be taken into account, the accuracies on the best posteriorgrams rise to $89\%$ and $88\%$ on the female and male test sets respectively. When the best 10 results are used, they reach $92\%$ and $89\%$.

\subsection{Lyrics retrieval experiments on line-wise inputs}
\vspace{-2px}
In the second retrieval experiment, the same process was performed on single lines of sung lyrics as inputs (usually a few seconds in duration). Costs are calculated between the posteriorgrams of these recordings and all 12,000 available lines of lyrics. Lines with fewer than 10 phonemes are not taken into account.\\
Then, evaluation was performed as to whether a line from the correct song was retrieved in the Top-N results. In this way, confusions between repetitions of a line in the same song do not have an impact on the result. However, repetitions of lyrical lines across multiple songs are a possible source of confusion. The results for the female test set are shown in figure \ref{fig:retrieval_dtwres_testF_line}, the ones for the male test set in figure \ref{fig:retrieval_dtwres_testM_line}.
\begin{figure}
        \centering
        \begin{subfigure}[c]{0.4\textwidth}
	    \includegraphics[width=\textwidth]{images/retrieval_dtwres_testF_line.png}
                \caption{DampTestF}
                \label{fig:retrieval_dtwres_testF_line}
        \end{subfigure}%
        %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
          %(or a blank line to force the subfigure onto a new line)
          \vspace{10px}
       \begin{subfigure}[c]{0.4\textwidth}
	  \includegraphics[width=\textwidth]{images/retrieval_dtwres_testM_line.png}
                \caption{DampTestM}
                \label{fig:retrieval_dtwres_testM_line}
       \end{subfigure}
        \caption{Accuracies of the results for lyrics detection on separate lines of sung lyrics for the \textit{DampTest} sets using the DTW-based approach with five different acoustic models, and evaluated on the Top-1, -3, -10, -50, and -100 results.}\label{fig:retrieval_dtwres_line}
\end{figure}

Again, a difference between both test sets is visible when generating posteriograms with the \textit{TIMIT} models. The accuracy on the best result is $14\%$ for the male test set, but just $7\%$ for the female test.\\
The results for the \textit{DAMP} models show the same basic tendencies as before, although naturally much lower. For the single best result, the accuracies when using the \textit{DampB} model are $38\%$ and $36\%$ on the female and male test sets respectively. For this task, gender-dependent models produce slightly higher results than the mixed-gender ones of the same size.\\



\section{Phoneme-based retrieval and alignment}\label{sec:retrieval_phone}
Next, another step was introduced to improve the previous approach and make it more flexible. This step serves to compress the posteriorgrams down to a plausible sequence of phonemes, which can then be used to search directly on a textual lyrics database. Text comparison is much cheaper than the previous comparison strategy, and enables quick expansion of the lyrics database.\\
In parallel, the lyrics database is prepared by converting it into phoneme sequences as described in section \ref{subsec:phonerec_corpus}. The key algorithms are (a) how to generate plausible phoneme sequences from the posteriorgrams, and (b) how to compare these against the sequences in the database. These parts will be described in more detail in the following. An overview is given in figure \ref{fig:retrieval_phone_overview}.

%TODO: phoneme seqs erklären
\begin{figure}
 \begin{center}
                \includegraphics[width=.9\textwidth]{images/process_alignment_lev.png}
                \caption{Overview of the phoneme-based lyrics retrieval process.}
                \label{fig:retrieval_phone_overview}
                 \end{center}
 \end{figure}

\vspace{-5pt}
\paragraph{Symbolic mapping} \label{par:symbolic_mapping}
As described before, starting from the sung recording used as a query, MFCC features are extracted and run through the acoustic model trained on singing to recognize the contained phonemes. This produces a phoneme posteriorgram, such as the one shown in figure \ref{fig:posteriorgram}; i.e., probabilities of each phone over each time frame. These probabilities contain some noise, both due to inaccuracies in the model and due to ambiguities or actual noise in the performance.\\
(In ASR, HMMs are commonly used for obtaining phoneme sequences from the output of an acoustic model. This was not done in this work for various reasons. One of them is the lack of reliable training data; the same data as for the acoustic models could be used, but may lead to errors in the model due to the relatively small number of individual songs, and this could also amplify errors in the automatic annotation. In addition to this, the presented approach is more flexible and allows taking knowledge about phoneme performances in singing into account, such as occurrences of long phonemes and the most frequent confusions between phonemes. Nevertheless, a HMM approach could also be tested in the future).\\

The following steps are undertaken to obtain a plausible phoneme sequence from the posteriorgram:
\begin{description}
	\item[\textit{1. Smoothing}] First, the posteriorgram is smoothed along the time axis with a window of length 3 in order to remove small blips in the phoneme probabilities. 
	\item[\textit{2. Maximum selection and grouping}] Then, the maximum bin (i.e. phoneme) per frame is selected, and consecutive results are grouped. An example is given in figure \ref{fig:retrieval_phone_block1}.
	\item[\textit{3. Filtering by probability and duration}] These results can then be pre-filtered to discard those that are too short or to improbable. This is done with different parameterizations for vowels and consonants since vowels are usually longer in duration. This yields a first sequence of phonemes, each with duration and sum probability information, which is usually too long and noisy. In particular, a lot of fluctuations between similar phonemes occur. %; an example is shown in figure \ref{fig:}.
	\item[\textit{4. Grouping by blocks and filtering through confusion matrix}] This problem is solved by first grouping the detected phonemes into blocks, in this case vowel and consonant blocks (shown in figure \ref{fig:retrieval_phone_block2}). Then, a decision needs to be made as to which elements of these blocks are the ``true'' phonemes and which ones are noise. This is done by taking each phoneme's probability as well as the confusion between phonemes into account. The confusion is calculated in advance by evaluating the classifier on an annotated test set; the result covers both the confusion by inaccuracies in the classifier as well as perceptual or performance-based confusions (e.g. transforming a long \texttt{/ay/} sound into \texttt{/aa/ - /ay/ - /iy/} during singing). An example of such a confusion matrix is shown in figure \ref{fig:retrieval_phone_confusion}. The product of the probabilities and the confusions are calculated for the highest combinations up to a certain threshold, and all other detected phonemes are discarded. This results in a shorter, more plausible phoneme sequence (figure \ref{fig:retrieval_phone_block3}).% such as the example shown in figure \ref{fig:}.
\end{description}
%listing?

  \begin{figure}

	\centering
	\begin{subfigure}[t]{0.5\textwidth}
		\includegraphics[width=.8\textwidth]{images/retrieval_phone_block1.png}
		\caption{Original phoneme sequence}\label{fig:retrieval_phone_block1}
	\end{subfigure}%
	\begin{subfigure}[t]{0.5\textwidth}
		\includegraphics[width=.8\textwidth]{images/retrieval_phone_block2.png}
		\caption{Grouped into blocks (with probabilities in italics)}\label{fig:retrieval_phone_block2}
	\end{subfigure}
	\begin{subfigure}[t]{0.4\textwidth}
		\includegraphics[width=.8\textwidth]{images/retrieval_phone_block3.png}
		\caption{Filtered result}\label{fig:retrieval_phone_block3}
	\end{subfigure}

	\caption{Example of the block grouping of the phoneme sequence and subsequent filtering by probabilities and confusions.}\label{fig:retrieval_phone_blocking}
\end{figure}

\begin{figure}
	\begin{center}
		\includegraphics[width=1\textwidth]{images/retrieval_phone_confusion.png}
		\caption{Example of a confusion matrix for an acoustic model. (Note that \texttt{/pau/} confusions are set to 0).}
		\label{fig:retrieval_phone_confusion}
	\end{center}
\end{figure}

%\begin{table}[h!tp]
%\footnotesize
%\begin{center}
%\begin{tabular}{|c||c|c|c||c|c|c||c|c|c|}
%\hline
% & \multicolumn{3}{|c|}{Female} & \multicolumn{3}{|c|}{Male} & \multicolumn{3}{|c|}{Author} \\
% & Top1 & Top3 & Top10 & Top1 & Top3 & Top10 & Top1 & Top3 & Top10 \\
%\hline \hline
%Baseline & .25 & .25 & .35 & .1 & .1 & .2 & .1 &.23 &.36 \\
%\hline
%Posteriorgram smoothing & .65 & .7 & .90 & .5 & .65 &.65 &.61 &.67 &.76 \\
%\hline
%Filtering blocks by probabilities and confusions & .8 & .85 &.96 & .75 &.8 &.85 &.74 &.78 &.82 \\
%\hline
%Substitution weights & .9 &.95 &.95 &.65 &.8 &.9 &.76 &.78 &.83 \\
%\hline
%Substitution + Insertion weights & 1 & 1 & 1 & .75 & .85 &.9 &.81 &.84 &.9 \\
%\hline
%\end{tabular}
%\end{center}
%\caption{Results of the retrieval algorithm for three test data sets with various improvement steps.}
%\label{tab:results}
%\end{table}%




\vspace{-5pt}
\paragraph{Distance calculation} \label{par:lev_distance}
Then, the distances between the extracted phoneme sequence and the ones provided in the lyrics database are calculated.\\
First, an optional step to speed up the process is introduced. Each sequence's number of vowels is counted in advance, and the same is done for the query sequence. Then, only sequences with roughly the same amount of vowels are compared (with some tolerance). This slightly decreases accuracies, but drastically speeds up the calculation time.\\
The similarity calculation itself is implemented with a modified Levenshtein distance. Again, the classifier's confusion between phonemes is taken into account. These confusions are used as the Levenshtein weights for substitutions. Surprisingly, using them for insertion weights improves the results as well. This probably happens because of the effect described above: A singer will in many cases vocalize an phoneme as multiple different ones, particularly for vowels with long durations. This will result in an insertion in the detected phoneme sequence, which is not necessarily a ``wrong'' classification by the acoustic model, but does not correspond to the expected sequence for this line of lyrics. For this reason, such insertions should not be harshly penalized. For deletions, the weight is set to $0.5$ to balance out the lower insertion and substitution weights.



\subsection{Alignment experiments}
\textit{MIREX} results for this approach are shown in figure \ref{fig:results_align_phone}; the detailed results can be found in appendix \ref{app:mirex}. Over-all, this approach produces much lower error values than the other two, and was in fact the winning algorithm in the competition. This confirms that the phoneme detection strategy is a feasible alternative to the HMM-based approach. On the \textit{ACAP} data set, the mean error is at $2.87$s and the median is $0.26$s. On \textit{ACAP\_Poly}, these values are $7.34$s and $4.55$s respectively, and on \textit{Mauch}, they are $9.03$s and $7.52$s. Once again, the results are much higher on polyphonic data than on unaccompanied singing, for which the models were trained. However, the error is still lower than with the submitted methods that include source separation. Therefore, future experiments that also employ pre-processing of this kind would be very interesting. Looking at the song-wise results, songs with heavier and noisier accompaniment generally receive higher errors than others.
\begin{figure}
 \begin{center}
                \includegraphics[width=0.45\textwidth]{images/res_align_phone.png}
               \caption{\textit{MIREX} results on all three data sets using the phoneme-based approach.}
                \label{fig:results_align_phone}
                 \end{center}
 \end{figure}
 

\subsection{Retrieval experiments: Calibration}
For calibrating the algorithm's parameters quickly, two smaller data sets were used: \textit{DampRetrieval}, which consists of 20 female and 20 male hand-selected sung segments with clear pronunciation and good audio quality, and \textit{AuthorRetrieval}, which consists of 90 small performances of phrases in the \textit{DAMP} data set by the author. Both are described in sections \ref{sec:data_damp} and \ref{sec:data_retrieval}. This was done to ensure that the results were not influenced by flaws in the data set (such as unclear or erroneous pronunciation or bad quality). Lyrics can still come from the whole \textit{DAMP} lyrics set, resulting in 12,000 lines of lyrics from 300 songs.\\
Figure \ref{fig:retrieval_cal} shows an overview over the experimental results. Accuracies are reported for the Top-1 result (i.e. the one with the lowest Levenshtein distance), the Top-3, and the Top-10 results. Queries were allowed to be one to three consecutive lines of songs, increasing the number of ``virtual'' database entries to around 36,000 (12,000 lines as 1-, 2-, and 3-grams). It should be noted that a result counts as correct when the correct song (out of 300) was detected; this was done as a simplification because the same line of lyrics is often repeated in songs. For possible applications, users are most probably interested in obtaining the correct full song lyrics, rather than a specific line. Picking random results would therefore result in an accuracy of $0.3\%$.\\
The various improvement steps of the algorithm were tested as follows:
\begin{description}
\item[Baseline] This is the most straightforward approach: Directly pick the phonemes with the highest probabilities for each frame from the posteriorgram, group them by consecutive phonemes, and use the result of that for searching the lyrics database with a standard Levenshtein implementation. This results in accuracies of $25\%$, $10\%$, and $23\%$ for the Female, Male, and Author test sets respectively.
\item[Posteriorgram smoothing] This is the same as the baseline approach, but the posteriorgram is smoothed along the time axis as described in paragraph \ref{par:symbolic_mapping}. This already improves the result by around $40$ percent points for each test set.
\item[Filtering blocks by probabilities and confusions] This includes the last step described in paragraph \ref{par:symbolic_mapping}. The result is improved further by $13\%$ to $25\%$ accuracy.
\item[Substitution and insertion weights] Finally, the modified Levenshtein distance as described in paragraph \ref{par:lev_distance} is calculated. When using the confusion weights for phoneme substitutions only, the result increases further, and even more so when they are also used for the insertion weights. The final Top-1 accuracies are $100\%$, $75\%$, and $81\%$ for the three test sets respectively.
\end{description}

  \begin{figure}

	\centering
	\begin{subfigure}[t]{0.5\textwidth}
		\includegraphics[width=.8\textwidth]{images/retrieval_calF.png}
		\caption{DampRetrievalF}\label{fig:retrieval_calF}
	\end{subfigure}%
	\begin{subfigure}[t]{0.5\textwidth}
		\includegraphics[width=.8\textwidth]{images/retrieval_calM.png}
		\caption{DampRetrievalM}\label{fig:retrieval_calM}
	\end{subfigure}\\
	\begin{subfigure}[t]{0.5\textwidth}
		\includegraphics[width=.8\textwidth]{images/retrieval_calA.png}
		\caption{AuthorRetrieval}\label{fig:retrieval_calA}
	\end{subfigure}%
	
	\caption{Results of the phoneme-based retrieval algorithm with various improvement steps for three small calibration data sets.}\label{fig:retrieval_cal}
\end{figure}

\subsection{Retrieval experiments: Full data set}
The full algorithm was also tested on the full \textit{DampTest} data sets in the same way as the DTW-based approach.\\
The results for retrieval using whole songs as queries are shown in figure \ref{fig:retrieval_levres}. All the steps presented in the previous section are included in the calculation of these results. As in the DTW-based approach, the material on which the acoustic model is trained plays a huge role. When using models trained on \textit{TIMIT}, the accuracy of the Top-1 result is just $58\%$ on the female test set, and $75\%$ on the male one (this mirrors the discrepancy between female and male results already seen in the previous approach).
Interestingly, the model trained on \textit{TimitM} actually performs worse for this approach with accuracies of $44\%$ and $62\%$ for the female and male test sets respectively. Since this approach is based on extracting salient phonemes from the posteriorgrams, it is possible that the time-stretched and pitch-shifted training data causes too much noise in the model's results, or a stronger bias towards vowels.\\
As previously seen, the models trained on the \textit{DAMP} data sets perform much better at this task. On both the female and male test sets, the best result is an accuracy of $94\%$ with the model trained on \textit{DampB}. The models trained on less data gradually perform a few percent points worse. Interestingly, training on gender-specific data once again does not improve the result. As suggested before, this could be due to the higher variety in timbre and pitch across songs and singers compared to variation between genders.\\
The accuracies for the Top-3 and Top-10 results follow a similar pattern. For the \textit{DAMP}-based models, the increase is not very high because the Top-1 result is already close to an upper bound. Analysis of the non-retrieved songs mainly shows discrepancies between the expected lyrics and the actual performance, such as those described in section \ref{subsec:error_sources}. This means that the most effective way to improve results would lie in making the algorithm more robust to such variances (versus improving the detection itself). Over-all, the results are significantly higher than with the DTW approach.\\


\begin{figure}
	\centering
	\begin{subfigure}[c]{0.4\textwidth}
		\includegraphics[width=\textwidth]{images/retrieval_levres_testF.png}
		\caption{DampTestF}
		\label{fig:retrieval_levres_testF}
	\end{subfigure}%
	%add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
	%(or a blank line to force the subfigure onto a new line)
	\vspace{10px}
	\begin{subfigure}[c]{0.4\textwidth}
		\includegraphics[width=\textwidth]{images/retrieval_levres_testM.png}
		\caption{DampTestM}
		\label{fig:retrieval_levres_testM}
	\end{subfigure}
	
	\caption{Accuracies of the results for lyrics detection on the whole song for the \textit{DampTest} sets using the phoneme-based approach with five different acoustic models, and evaluated on the Top-1, -3, and -10 results.}\label{fig:retrieval_levres}. 
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}[c]{0.4\textwidth}
		\includegraphics[width=\textwidth]{images/retrieval_levres_testF_line.png}
		\caption{DampTestF}
		\label{fig:retrieval_levres_testF_line}
	\end{subfigure}%
	%add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
	%(or a blank line to force the subfigure onto a new line)
	\vspace{10px}
	\begin{subfigure}[c]{0.4\textwidth}
		\includegraphics[width=\textwidth]{images/retrieval_levres_testM_line.png}
		\caption{DampTestM}
		\label{fig:retrieval_levres_testM_line}
	\end{subfigure}
	\caption{Accuracies of the results for lyrics detection on separate lines of sung lyrics for the \textit{DampTest} sets using the phoneme-based approach with five different acoustic models, and evaluated on the Top-1, -3, -10, -50, and -100 results.}\label{fig:retrieval_levres_line}
\end{figure}

Figure \ref{fig:retrieval_levres_line} shows the analogous results when using single line queries for retrieval. The trend across the different classifiers runs parallel to the whole song results, with the \textit{TimitM} models performing worse than the \textit{TIMIT} models, and both being outperformed by the \textit{DAMP}-based models. The best Top-1 result is $55\%$ for the female test set, and $52\%$ for the male one. For the Top-10 results, the retrieval results are $7\%$ and $67\%$ respectively. Once again, lines with fewer than 10 phonemes were excluded; however, lines can still occur in more than one song. Considering this fact and the previously mentioned interfering factors, this result is already salient. When comparing these results to the ones for the calibration data set, it becomes clear that performance is highly dependent on the quality of the queries. In a practically usable system, users could be asked to enunciate clearly or to perform long segments.


\section{Application: Expletive detection}

Lots of song lyrics contain expletives. There are many scenarios in which it is necessary to know when these words occur, e.g. for airplay and for the protection of minors. During broadcasting, they are commonly ``bleeped" or acoustically removed. The alignment strategies described previously are employed for the practical scenario of finding such expletives automatically.\\
The test data set is the one compiled by Queen Mary University, described in section \ref{subsec:data_qmul}.\\

A direct keyword spotting approach was also considered, but this did not generate sufficient results since most of the expletives only consist of 2 or 3 phonemes. Keyword spotting becomes notoriously hard for such short keywords as described in chapter \ref{chap:kws}.\\
Since textual lyrics are usually easily available on the internet, a new approach utilizing those was developed:
\begin{enumerate}
\item Automatically align textual lyrics to audio (as described above)
\item Search for pre-defined expletives in the result
\item If necessary, remove those expletives. A stereo subtraction approach was used, which works adequately for this case since the removed timespans are short. Alternatively, keywords can be masked with a bleep or similar.
\end{enumerate}
The data flow is shown in figure \ref{fig:expletive_dataflow}.
\begin{figure}
\begin{center}
                \includegraphics[width=1\textwidth]{images/process_expletives.png}
                \caption{Data flow in the expletive detection approach.}
                \label{fig:expletive_dataflow}
                 \end{center}
 \end{figure}

The alignment is performed using the HMM-based alignment described in section \ref{sec:ret_hmm}, and the DTW alignment from section \ref{sec:ret_post} using the DNN acoustic models trained on \textit{TIMIT} and on \textit{DampB}. Accuracies are then calculated by evaluating how many of the annotated expletives were recognized at their correct timestamps (with various tolerances). The results are shown in figure \ref{fig:expletive_results}.
\begin{figure}
 \begin{center}
                \includegraphics[width=0.8\textwidth]{images/expletive_results.png}
               \caption{Results for the expletive detection approach at various tolerances.}
                \label{fig:expletive_results}
                 \end{center}
 \end{figure}
 %analyze?

In this small practical example, only two alignment strategies were tested, but there are others that could provide better results. Whenever the alignment failed, it was mostly due to solo instruments. In order to remedy this, vocal detection (and possibly source separation) could be employed prior to alignment. Additionally, a more sophisticated removal approach could be implemented to remove the expletives.\\
Lyrics collected from the internet are often incorrect, e.g. because repetitions are not spelled out, because of spelling errors, or because they refer to different versions of a song. This approach could be expanded to allow for some flexibility in this respect. At the moment, these lyrics need to be provided manually. In the future, those could be retrieved automatically (for example by using the approaches described above).\\


%TODO: Clean up!
\section{Conclusion}
%error sources: same as mentioned in phonerec because same data!!
%error source: amateur recordings
In this section, three approaches to lyrics-to-audio alignment and retrieval and one practical application were presented. The first one uses HMMs trained on speech for alignment. The mean alignment error in the 2017 \textit{MIREX} challenge was $5.11$s on unaccompanied singing, and $13.96$s and $17.7$s on two polyphonic data sets. A manual check on some examples suggests that the algorithm is already usable in practice in many cases, even for polyphonic music. It was also used to generate the annotations for the \textit{DAMP} training data sets as described in section \ref{sec:phonerec_acap}.\\

The second algorithm is based on Dynamic Time Warping. In its first step, phoneme posteriorgrams are extracted with the various acoustic models described in chapter \ref{chap:phonerec}, in particular those trained on \textit{TIMIT} speech data, the time-stretched and pitch-shifted version \textit{TimitM}, and several of the \textit{DAMP}-based data sets whose annotations were generated with the previous approach. Then, a one-hot binary template is generated from the lyrics to be aligned, and an optimal alignment between this and the posteriorgram is computed via DTW. In the \textit{MIREX} challenge, this approach achieved a mean alignment error of $9.77$s on the unaccompanied singing data, and of $27.94$s and $22.23$s on the two polyphonic data sets.\\
The same approach can also be used for retrieving lyrics from a text database with a sung query. To this end, binary templates are generated for all possible lyrics, and the alignment is performed for all of them. Then, the result with the lowest DTW cost is selected as the winner. When the whole song is used as the input, an accuracy of $86\%$ for the single best result is obtained. If the 10 best results are taken into account, this rises to $91\%$. When using only short sung lines as input, the mean Top-1 accuracy for retrieving the correct song lyrics of the whole song is $37\%$. For the best 100 results, the accuracy is $67\%$. An interesting result was the difference between the female and the male test sets: On the female test set, retrieval with models trained on speech was significantly lower than on the male set ($39\%$ vs. $58\%$ on the song-wise task). This may happen because the frequency range of female singing is not covered well with speech data only. When using acoustic models trained on singing, this difference disappears and the results become significantly higher in general. Even for a model trained on less data than that contained in \textit{TIMIT}, the average accuracy is $82\%$.\\

The third approach is also based on posteriorgrams generated with the models from chapter \ref{chap:phonerec}. However, instead of operating directly on them, phoneme sequences are extracted. This is done by selecting the phoneme with the highest probability in each frame and compressing this sequence down to discard unlikely phoneme occurrences. This process takes the known phoneme confusions of the classifier into account. Several improvement steps were added.  The extracted phoneme sequence is then aligned to the expected one (from the known lyrics) using the Levenshtein distance. In the \textit{MIREX} challenge, this algorithm produced the best results with a mean error of $2.87$s on unaccompanied singing, and $7.34$s and $4.55$s on polyphonic music.\\
For lyrics retrieval, the extracted phoneme sequence is compared to all the sequences in the lyrics database using the Levenshtein distance, and the result with the lowest distance is selected. On the same test data as above, the approach achieves an accuracy of $94\%$ for whole-song inputs. With line-wise inputs, the accuracy is $54\%$ for the Top-1 result, and $88\%$ for the Top-100. Once again, there is a significant difference between the female and male results with models trained on speech, which disappears with singing-based models.\\
Tests on a smaller database of hand-selected clean audio samples resulted in much higher accuracies for single-line queries. This suggests that the system can perform much better when provided with clean inputs; this would be feasible in a real-world application.\\

Finally, the HMM and DTW alignments were tested in an application scenario: The extraction of expletives from popular music. To this end, known lyrics were aligned to polyphonic song recordings containing such words. Then, the found time segments were masked with other sounds, or stereo subtraction was performed to remove the singing voice. At a tolerance of $1$s, $92\%$ of the expletives were detected in their correct positions, making the system usable for practical applications.\\

In all experiments, analysis of the errors showed the same possible sources as described in section \ref{subsec:error_sources}. Many of them had to do with enunciation issues (clarity, accents, or children's voices) or issues with the recording itself (background music, clipping, extraneous speaking). These problems would not be as prevalent in professional recordings. However, some of them could be fixed with adaptations to the algorithm. In polyphonic alignment experiments, errors also occurred due to prominent instruments, in particular during instrumental solos. As described in section \ref{sec:sota_speechtosinging}, this is also an issue in other MIR tasks related to singing, such as Vocal Activity Detection.\\

As described, the developed algorithms are in many cases ready for practical applications. In the future, it would be interesting to see them integrated into existing systems. There are also ways to make them more robust, e.g. to errors in the lyrics transcriptions or to mistakes or variations by the singers. No special steps have been taken so far to adapt them to polyphonic music; Vocal Activity Detection or source separation could be performed in the pre-processing.\\
So far, the retrieval approaches were only tested on a relatively small lyrics database containing 12,000 lines of lyrics across 300 songs. For larger databases, scalability can become an issue. One partial solution was already integrated into the phoneme-based method. To take this one step further, both retrieval algorithms could first perform a rough search with smaller lyrical ``hashes'' to find possible matches, and then perform a refinement as presented. This is similar to techniques that are already used in audio fingerprinting \cite{shazam}.\\
Alternatively, the phoneme-based approach could be expanded to retrieve lyrics from the internet instead of from a fixed database, e.g. with Semantic Web techniques. 



%In this paper, we presented an approach to retrieving the matching lyrics for a singing recording from a fixed database of 300 textual lyrics. To do this, we first extract phoneme posteriorgrams from the audio and generate phoneme templates from all possible lyrics. We then perform Dynamic Time Warping on all combinations to obtain distance measures.\\
%When the whole song is used as the input, we obtain an accuracy of $86\%$ for the single best result. If the 10 best results are taken into account, this rises to $91\%$. When using only short sung lines as input, the mean 1-best accuracy for retrieving the correct song lyrics of the whole song is $37\%$. For the best 100 results, the accuracy is $67\%$.\\
%An interesting result was the difference between the female and the male test sets: On the female test set, retrieval with models trained on speech was significantly lower than on the male set ($39\%$ vs. $58\%$ on the song-wise task). We believe this happens because the frequency range of female singing is not covered well with speech data only. When using acoustic models trained on singing, this difference disappears and the results become significantly higher in general. Even for a model trained on less data than that contained in \textit{Timit}, the average accuracy is $82\%$.\\
%When looking at possible sources of error, many of them had to do with enunciation issues (clarity, accents, or children's voices) or issues with the recording itself (background music, clipping, extraneous speaking). These problems would not be as prevalent in professional recordings. However, some of them could be fixed with adaptations to the algorithm.
%
%As mentioned before, we would like to improve our algorithm to be more robust to the detected error sources. Other possible points of improvement include the choice of the distance metric or of the acoustic models. Preliminary tests suggest that combining results of different phoneme recognizers could improve the over-all result.\\
%We have not tested this approach on singing with background music yet, which could be an interesting next step. So far, only a fixed corpus of possible lyrics was taken into account. Opening the approach up to larger databases would make it more flexible. This could be combined with Semantic Web technologies to automatically find lyrics on the internet.\\
%When the space of possible lyrics becomes larger, techniques for scalability will be necessary. One such idea could be a rough search with smaller lyrical ``hashes'' to find possible matches, and then a refinement with our current approach. This is similar to techniques that are already used in audio fingerprinting \cite{shazam}.
%
%
%In this paper, we presented a new approach for retrieving textual lyrics from single lines of sung queries. To this end, we first extract phoneme posteriorgrams from the audio recording with acoustic models trained on a large singing database. Then, we employ a new algorithm to map the result to a symbolic representation (i.e. a phoneme sequence). This sequence is then compared to the ones in the lyrics database using a modified Levenshtein distance.\\
%We found several improvements to this basic algorithm. First, we noticed that smoothing the posteriorgram along the time axis with a short window removes spurious probability fluctuations. The mapping from the posteriorgram to the symbolic representation can be improved by filtering the phonemes with the highest probabilities with the known phoneme confusions. Finally, we modified the Levenshtein distance by also utilizing these confusions for substitution and insertion weights. The final results range between $.75$ and $1$ for the Top 1 result, and between $.9$ and $1$ for the Top 10.
%
%In the next step, we would like to perform a more extensive evaluation of the various components of the algorithm. In particular, it would be interesting to research the relationship between retrieval rate and the number of phonemes in the query in more detail. Related to this, we would like to evaluate the approach for speeding up the search by filtering by number of vowels more closely.\ As mentioned in section \ref{sec:approach}, HMMs are an alternative for obtaining the phoneme sequence from the posteriorgram which would be interesting to test.\\
%The suggested approach can be integrated into various systems as described in section \ref{sec:intro}; one particularly interesting application is the search of lyrics on the internet without a prepared database. Our method could be expanded to enable this. Additionally, the described algorithm can also be employed for lyrics-to-singing alignment by calculating the Levenshtein alignment between the detected and the expected phoneme sequence. This algorithm performed best at 2017's \textit{MIREX} challenge for lyrics-to-audio alignment\footnote{\url{http://www.music-ir.org/mirex/wiki/2017:Automatic_Lyrics-to-Audio_Alignment_Results}}.
