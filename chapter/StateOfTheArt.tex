\chapter{State of the art}	\label{chap:sota}
\section{From speech to singing} \label{sec:speech_to_singing}
Singing presents a number of challenges for speech recognition when compared to pure speech \cite{loscos} \cite{goto_alignment} \cite{kruspe_kws1}. The following factors make speech recognition on singing more difficult than on speech, and make it necessary to adapt existing algorithms.
\begin{description}
 \item[Larger pitch fluctuations] A singing voice varies its pitch to a much higher degree than a speaking voice. It often also has very different spectral properties.
 \item[Larger changes in loudness] In addition to pitch, loudness also fluctuates much more in singing than in speech.
 \item[Higher pronunciation variation] Singers are often forced by the music to pronounce certain sounds and words differently than if they were speaking them.
 \item[Larger time variations] In singing, sounds are often prolonged for a certain amount of time to fit them to the music. Conversely, they can also be shortened or left out completely.
 \item[Different vocabulary] In musical lyrics, words and phrases often differ from normal conversation texts. Certain words and phrases have different probabilities (e.g. higher focus on emotional topics in singing).
 \item[Background music] This is the biggest interfering factor when considering polyphonic recordings. Harmonic and percussive instruments add a huge amount of spectral components to the signal, which lead to confusion in speech recognition algorithms. Ideally, these components should be removed or suppressed in a precursory step. This could be achieved, for example, by employing source separation algorithms. However, such algorithms add additional artifacts to the signal, and may not even be sufficient for this purpose at the current state of research.\\
 Voice activity detection (VAD) could be used as a non-invasive first step to at the very least discard segments of songs that do not contain voice. However, such algorithms often make mistakes in the same cases that are problematic for speech recognition algorithms (e.g. instrumental solos)\cite{}.\\
 For these reasons, most of the experiments in this work were performed on unaccompanied singing. The integration of the mentioned pre-processing algorithms would be a very interesting next step of research.\\
 The exception are the lyrics-to-singing alignment algorithms presented in Chapter \ref{chap:retrieval_alignment}. Those were also tested on polyphonic music, and the algorithms appear to be largely robust to these influences.
 \end{description}
 %Mehr Beispiele aus Goto-Kapitel
 %Sundberg!
 % Vowels vs. Consonants (1. Mesaros-Paper + recognition of phonemes and words...)

%describe tasks here??

\section{Phoneme recognition} \label{sec:sota_phone}
%\subsection{Phoneme recognition in speech}
%\subsection{Phoneme recognition in singing}
Due to the factors mentioned above in section \ref{sec:speech_to_singing}, phoneme recognition on singing is more difficult than on clean speech. It has only been a topic of research for a few years, and there are few publications.\\
In 2007, Gruhne et al. presented a classical approach that employs feature extraction and various machine learning algorithms to classify singing into 15 phoneme classes \cite{Gruhne2007} \cite{Gruhne2007a}. The specialty of this approach lies in the pre-processing: At first, fundamental frequency estimation is performed on the audio input, using a Multi-Resolution Fast Fourier Transform \cite{inproceedings:dressler}. Based on the estimated fundamental frequency, the harmonic partials are retrieved from the spectrogram. Then, a sinusoidal re-synthesis is performed, using only the detected fundamental frequency and partials. Feature extraction is then performed on this re-synthesis instead of the original audio. Extracted features include MFCCs, PLPs \cite{plp2}, LPCs, and WLPCs \cite{lpc}. MLP, GMM, and SVM models are trained on the resulting feature vectors. The re-synthesis idea comes from a singer identification approach by Fujihara \cite{fujihara_identification}.\\
The approach is tested on more than 2000 separate, manually annotated phoneme instances from polyphonic recordings. Only one feature vector per phoneme instance is calculated. The phoneme set is reduced down to 15 classes. Using SVM models, $56\%$ of the tested instances were classified correctly. This is significantly better than the best result without the re-synthesis step ($34\%$).\\
In \cite{szepannek} (2010), the approach is expanded by testing a larger set of perceptually motivated features, and more classifiers. No significant improvements are found when using more intricate features, and the best-performing classifier remains SVM.\\
\medskip
Fujihara et al. described an approach based on spectral analysis in 2009 \cite{fujihara_phonemes}. The underlying idea is that spectra of polyphonic music can be viewed as the weighted sum of two types of spectra: One for the singing voice, and one for the background music. This approach then models these two spectra as probabilistic spectral templates. The singing voice is modeled by multiplying a vocal envelope template, which represents the spectral structure of the singing voice, with a harmonic filter, which represents the harmonic structure of the produced sound itself. This is analogous to the source-filter model of speech production \cite{}. For recognizing vowels, five such harmonic filters are prepared (a - e - i - o - u). Vocal envelope templates are trained on voice-only recordings, separated by gender. Templates for background music are trained on instrumental tracks. In order to recognize vowels, the probabilities for each of the five harmonic templates are estimated. As a side product, the algorithm also estimates the fundamental frequency of the singing voice.\\
As described, the phoneme models are gender-specific and only model five vowels, but also work for singing with instrumental accompaniment. The approach is tested on 10 Japanese-language songs. The best result is $65\%$ correctly classified frames, compared to the $56\%$ with the previous approach by this team, based on GMMs. \\
\medskip
Also in 2009, Mesaros et al. presented another classical approach to phoneme recognition in singing which is based on MFCC features and GMM-HMMs for acoustic modeling \cite{Mesaros2009}. The models are trained on the CMU ARCTIC speech corpus \footnote{\url{http://festvox.org/cmu arctic/}}. Then, different Maximum Likelihood Linear Regression (MLLR) techniques for adapting the models to singing voices are tested \cite{mllr}.\\
The adaptation and test corpus consists of 49 voice-only fragments from 12 pop songs with durations between 20 and 30 seconds. The best results are achieved when both the means and variances of the Gaussians are transformed with MLLR. The results improved slightly when not just a single transform was used for all phonemes, but when they were grouped into base classes beforehand, each receiving individual transformation parameters. The best result is around $.79$ Phoneme Error Rate on the test set.\\
In \cite{Mesaros2010} and \cite{Mesaros2011}, language modeling is added to the presented approach. Phoneme-level language models are trained on the CMU ARCTIC corpus as unigrams, bigrams, and trigrams, while word-level bigram and trigram models are trained on actual song lyrics in order to match the application case. The output from the acoustic models is then refined using these language models. The approach is tested on the clean singing corpus mentioned above, and on 100 manually selected fragments of 17 polyphonic pop songs. To facilitate recognition on polyphonic music, a vocal separation algorithm is introduced \cite{virtanen_separation}.\\
Using phoneme-level language modeling, the phoneme error rate on clean singing is reduced to $.7$. On polyphonic music, it is $.81$. For the word recognition approach, the word error rate is $.88$ on clean singing, and $.94$ on the polyphonic tracks.\\
A more detailed voice adaptation strategy is tested in \cite{Mesaros2010a}. Instead of adapting the acoustic models with mixed singing data, they are adapted gender-wise, or to specific singers. With the gender-specific adaptations, the average phoneme error rate on clean singing is lowered to $.81$ without language modeling, and $.67$ with language modeling. Singer-specific adaptation does not improve the results, probably because of the very small amount of adaptation data in this case.\\
%mvcivar, hosoya
In \cite{mcvicar_repetition} (2014), McVicar et al. build on a very similar baseline system, but also exploit repetitions of choruses to improve transcription accuracy. This has been done for other MIR tasks, such as chord recognition, beat tracking, and source separation. They propose three different strategies for combining individual results: Feature averaging, selection of the chorus instance with the highest likelihood, and combination using the Recogniser Output Voting Error Reduction (ROVER) algorithm \cite{rover}. They also employ three different language models, two of which were matched to the test songs (and therefore not representative for general application). 20 unaccompanied, English-language songs from the RWC database \cite{rwc} were used for testing; chorus sections were selected manually. The best-instance selection and the ROVER strategies improve results significantly; with the ROVER approach and a general-purpose language model, the Phoneme Error Rate is at $.74$ (versus $.76$ in the baseline experiment), while the Word Error Rate is improved from $.97$ to $.9$. Interestingly, cases with a low baseline result benefit the most from exploiting repetition information.\\
\medskip
The final system was proposed by Hansen in 2012 \cite{jens}. It also employs a classical approach consisting of a feature extraction step and a model training step. Extracted features are MFCCs and TRAP (TempoRAl Pattern) features. Then, Multilayer Perceptrons (MLPs) are trained separately on both feature sets. The assumption is that each feature models different properties of the considered phonemes: Short-term MFCCs are good at modeling the pitch-independent properties of stationary sounds, such as sonorants and fricatives. On the flip-side, TRAP features are able to model temporal developments in the spectrum, forming better representations for sounds like plosives or affricates.\\
The results of both MLP classifiers are combined via a fusion classifier, also an MLP. Then, Viterbi decoding is performed on its output.\\
The approach is trained and tested on a data set of 12 vocal tracks of pop songs, which were manually annotated with a set of 27 phonemes. The combined system achieves a recall of $.48$, compared to $.45$ and $.42$ for the individual MFCC and TRAP classifiers respectively. This confirms the assumption that the two features complement each other. The phoneme-wise results further corroborate this.

\section{Lyrics-to-audio alignment}
%\subsection{Forced alignment in speech}
%\subsection{Forced alignment in singing}
%mesaros
%Three techniques for im- proving automatic synchronization between music and lyrics: Fricative detection, filler model, and novel feature vectors for vocal activity detection
In contrast to the other tasks discussed in this chapter, the task of lyrics-to-audio alignment has been the focus of many more publications. A comprehensive overview until 2012 is given in \cite{goto_alignment}.\\
A first approach was presented in 1999 by Loscos et al. \cite{Loscos1999}. The standard forced alignment approach from speech recognition is adapted for singing. MFCCs are extracted first, and then a left-to-right HMM is employed to perform alignment via Viterbi decoding. Some modifications are made to the Viterbi algorithm to allow for low-delay alignment. The approach is trained and tested on a very small (22 minutes) database of unaccompanied singing, but no quantitative results are given.\\
The first attempt to synchronize lyrics to polyphonic recordings was made by Wang et al. in 2004 \cite{Wang2004}. They propose a system, named ``LyricAlly'', to provide line-level alignments for karaoke applications. Their approach is heavily based on musical structure analysis. First, the hierarchical rhythm structure of the song is estimated. The result is combined with an analysis of the chords and then used to split the song into sections, based on a chorus detection algorithm. Second, Vocal Activity Detection (VAD) using HMMs is performed on each section. Then, sections of the text lyrics are assigned to the detected sections (e.g. verses, choruses). In the next step, the algorithm determines whether the individual lines of the lyrics match up with the vocal sections detected by the VAD step. If they do not, grouping or partitioning is performed. This is based on the assumption that lyrics match up to rhythmic bars as determined by the hierarchical rhythm analysis. The expected duration of each section and line is estimated using a Gaussian distributions of phoneme durations from a singing data set. In this manner, lines of text are aligned to the detected vocal segments. The approach is tested on 20 manually annotated pop songs. On the line level, the average error is $.58$ seconds for the starting points and $-.48$ seconds for the durations. The system components are analyzed in more detail in \cite{lyrically}.\\
In 2006, the same team also presented an approach that also performs rhythm and bar analysis to facilitate syllable-level alignment \cite{Iskandar:2006}. For the phoneme recognition step, an acoustic model is trained on speech data and adapted to singing using the previously mentioned 20 songs. The possible syllable positions in the alignment step are constrained to the note segments detected in the rhythm analysis step. Due to annotator disagreement on the syllable level, the evaluation is performed on the word level. On three example songs, the average synchronization error rate is $.19$ when allowing for a tolerance of 1/4 bar.\\  
\medskip
An approach which does not require in-depth music analysis was presented by Fujihara et al. in 2006 \cite{fujihara_alignment}. A straightforward Viterbi alignment method from speech recognition is refined by introducing three singing-specific pre-processing steps: Accompaniment sound reduction, Vocal Activity Detection, and phoneme model adaptation.\\
For accompaniment reduction, the previously mentioned harmonic re-synthesis algorithm from \cite{fujihara_identification} is used again. For Vocal Activity Detection, a HMM is trained on a small set of unaccompanied singing using LPC-derived MFCCs and $F_0$ differences as features. The HMM can be parameterized to control the rejection rate. For the phoneme model adaptation, three consecutive steps are tested: Adaptation to a clean singing voice, adaptation to a singing voice segregated with the accompaniment reduction method, and on-the-fly adaptation to a specific singer. MFCC features are used for the Viterbi alignment.\\
Ten Japanese pop songs are used for testing

\cite{fujihara} \cite{goto_alignment} \\
\medskip
\cite{WongSW07} \\ %cantonese
\medskip
\cite{LeeC08} \\ %segmentation-based
\medskip
\cite{mauch_alignment2010} \cite{mauch_alignment2} \\
\medskip
\cite{mesaros_alignment} \\ %mention other mesaros papers
\medskip
\cite{gong_alignment} \\
\medskip
\cite{dzhambazov_alignment}



\section{Lyrics retrieval}
%HOSOYA???
%WangLC2003
%RETRIEVAL: Mesaros!!!



\section{Language identification}
\subsection{Language identification in speech}
Language identification has been extensively researched in the field of Automatic Speech Recognition since the 1980's. A number of successful algorithms have been developed over the years. An overview over the fundamental techniques is given by Zissman in \cite{zissman}.\\
Fundamentally, four properties of languages can be used to discriminate between them:
\begin{description}
 \item[Phonetics] The unique sounds that are used in a given language.
 \item[Phonotactics] The probabilities of certain phonemes and phoneme sequences.
 \item[Prosody] The ``melody'' of the spoken language.
 \item[Vocabulary] The possible words made up by the phonemes and the probabilities of certain combinations of words.
\end{description}
Even modern system mostly focus on phonetics and phonotactics as the distinguishing factors between languages. Vocabulary is sometimes exploited in the shape of language models.\\
Zissman mentions Parallel Phone Recognition followed by Language Modeling (PPRLM) as one of the basic techniques. It requires audio data, language annotations, and phoneme annotations for each utterance. In order to make use of vocabulary characteristics, full sentence annotations and word-to-phoneme dictionaries are also necessary.\\
Using the audio and phoneme data, acoustic models are trained. They describe the probabilities of certain sound and sound sequences occurring. This is done separately for each considered language. Similarly, language models are generated using the sentence annotations and the dictionary. These models describe the probabilities of certain words and phrases. Again, this is done for each language.\\
New audio examples are then run through all pairs of acoustic and language models, and the likelihoods produced by each model are retained. The highest acoustic likelihood, the highest language likelihood, or the highest combined likelihood are then considered to determine the language. This approach achieves up to $79\%$ accuracy for ten languages \cite{muthusamy}.\\
Another approach uses the idea to train Gaussian Mixture Models for each language. This technique can be considered a ``bag of frames'' approach, i.e. the single data frames are considered to be statistically independent of each other. The generated GMMs then describe probability densities for certain characteristics of each language. Using these, the language of new audio examples can be easily determined.\\
GMM approaches used to perform worse than their PPRLM counterparts, but the development of new features has made the difference negligible \cite{singer}. They are in general easier to implement since only audio examples and their language annotations are required. Allen et al. \cite{allen} report results of up to $76.4\%$ accuracy for ten languages. Different backend classifiers, such as Multi-Layer Perceptrons (MLPs) and Support Vector Machines (SVMs) \cite{campbell} have also been used successfully instead of GMMs.


\subsection{Language identification in singing}
A first approach for language identification in singing was proposed by Tsai and Wang in 2004 \cite{tsai_wang}. At its core, the algorithm is similar to Parallel Phoneme Recognition (PPR). However, instead of full phoneme modeling, they employ an unsupervised clustering algorithm to the input feature data and tokenize the results to form language-specific codebooks (plus one for background music). Following this, the results from each codebook are run through a matching language models to determine the likelihood that the segment was performed in this language. Prior to the whole process, vocal/non-vocal segmentation is performed. This is done by training GMMs on segments of each language, and on non-vocal segments. MFCCs are used as features.\\
The approach is tested on 112 English- and Mandarin-language polyphonic songs each, with 32 songs performed in both languages. A classification accuracy of $.8$ is achieved on the non-overlapping songs. On the overlapping songs, it is only at $.7$, suggesting some influence of the musical material (as opposed to the actual language characteristics). Misclassifications occur more frequently on the English-language songs, possibly because of accents of Chinese singers performing in English, and because of louder background music.\\
\medskip
A second, simpler approach was presented by Schwenninger et al. in 2006 \cite{schwenninger}. They also extract MFCC features, and then use these to directly train statistical models for each language. Three different pre-processing strategies are also tested: Automatic vocal/non-vocal segmentation, distortion reduction, and azimuth discrimination. Vocal/non-vocal segmentation is performed by thresholding the energy in high-frequency bands as an indicator for voice presence over 1 second windows. This leaves a relatively small amount of material per song. Distortion reduction is employed to discard strong drum and bass frames where the vocal spectrum is masked by using a mel-based approach. Finally, azimuth discrimination attempts to detect and isolate singing voice panned to the center of the stereo scene.\\
The approach is tested on three small data sets of speech, unaccompanied singing, and polyphonic music. Without pre-processing steps, the accuracy is $.84$, $.68$, and $.64$ respectively, highlighting the increased difficulty of language identification on singing versus speech, and on polyphonic music versus pure vocals. On the polyphonic corpus, the pre-processing steps do not improve the result.\\
\medskip
In 2011, Mehrabani and Hansen presented a full Parallel Phoneme Recognition followed by Language Modeling (PPRLM) approach for singing language identification. MFCC features are run through phoneme recognizers for Hindi, German, and Mandarin; then, the results are scored by individual language models for each considered language. In addition, a second system is employed which uses prosodic instead of phonetic tokenization. This is done by modeling pitch contours with Legendre polynomials, and then quantizing these vectors with previously trained GMMs. The results are then again used as inputs to language models.\\
The approach is trained and tested on a corpus containing 12 hours of unaccompanied singing and speech in Mandarin, Hindi, and Farsi. The average accuracy for singing is $.78$ and $.43$ for the phoneme- and prosody-based systems respectively, and $.83$ for a combination of both.\\
\medskip
Also in 2011, Chandrasekhar et al. presented a very interesting approach for language identification on music videos, taking into account both audio and video features \cite {chandrasekhar}. On the audio side, the spectrogram, volume, MFCCs, and perceptually motivated Stabilized Auditory Images (SAI) are used as inputs. One-vs-all SVMs are trained for each language. The approach is trained and tested on 25,000 music videos, taking 25 languages into consideration. Using audio features only, the accuracy is $.45$; combined with video features, it rises to $.48$. It is interesting to note that European languages seem to achieve much lower accuracies than Asian and Arabic ones. English, French, German, Spanish and Italian rank below $.4$, while languages like Nepali, Arabic, and Pashto achieve accuracies above $.6$. It is possible that the language characteristics of European languages make them harder to discriminate (especially against each other) than others.

\section{Keyword spotting}
%\subsection{Keyword spotting in speech}
%Igor Sz? oke, Petr Schwarz, Pavel Matejka, Luk?as Bur-get, Martin Karafi? at, Michal Fapso, and Jan Cernock`y.Comparison of keyword spotting approaches for infor- mal continuous speech. In Interspeech, pages 633?636, 2005.
%\subsection{Keyword spotting in singing}
%SPEECH HERE
%pedro!!
Keyword spotting in singing was first attempted in 2008 by Fujihara et al. \cite{hyperlinking_lyrics}. Their approach employs a phoneme recognition step first, which is again based on the vocal re-synthesis method first described in \cite{fujihara_identification}. MFCCs and power features are extracted from the re-synthesized singing and used as inputs to a phoneme model, similar to Gruhne's phoneme recognition approach mentioned above in \ref{sec:sota_phone}.  Three phoneme models are compared: One trained on pure speech and adapted with a small set of singing recordings, one adapted with all recordings, and one trained directly on singing. Viterbi decoding is then performed using keyword-filler HMMs (see \ref{sec:tech_kwfhmm}) to detect candidate segments where keywords may occur. These segments are then re-scored through the filler HMM to verify the occurrence.\\
The method is tested on 79 unaccompanied Japanese-language songs from the RWC database \cite{rwc}. The Phoneme Error Rate is $.73$ for the acoustic models trained on speech, $.67$ for the adapted models, and $.49$ for the models trained on singing (it should be mentioned that the same songs were used for training and testing, although a cross-validation experiment appears to show that the effect is negligible). The employed evaluation measure is ``link success rate'', describing the percentage of detected phrases that were linked correctly to other occurrences of the phrase in the data set. In that sense, it is a sort of accuracy measure. The link success rate for detecting the keywords is $.3$. The authors show that the result depends highly on the number of phonemes in the considered keyword, with longer keywords being easier to detect.\\
\medskip
In 2012, Mercado et al. presented an approach to keyword spotting in singing based on a different principle: Dynamic Time Warping (DTW) between a sung query and the requested phrase in the song recording. In particular, Statistical Sub-sequence DTW is the algorithm employed for this purpose. MFCCs are used as feature inputs, then the costs of the warping paths are calculated from all possible starting points to obtain candidate segments, which are then further refined to find the most likely position.\\
The approach is tested on a set of vocal tracks of 19 pop songs (see Section \ref{sec:data_hansen}) as the references, and phrase recordings by amateur singers as the queries, but no quantitative results are given. The disadvantage of this approach lies in the necessity for audio recordings of the key phrases, which need to have at least similar timing and pitch as the reference phrases.\\
\medskip
Finally, Dzhambazov et al. developed a score-aided approach to keyword spotting in 2015 \cite{dzhambazov_ismir}. A user needs to select a keyword phrase and a single recording in which this phrase occurs. The keyword is then modeled acoustically by concatenating recordings of the constituent phonemes (so-called acoustic keyword spotting). Similar to Mercado's approach, Sub-Sequence DTW is then performed between the acoustic template and all starting positions in the reference recording to obtain candidate segments. These segments are then refined by aligning the phonemes to the score in these positions to model their durations. This is done by using Dynamic Bayesian Network HMMs. Then, Viterbi decoding is performed to re-score the candidate segments and obtain the best match.\\
The approach is tested on a small set of unaccompanied Turkish-language recordings of traditional Makam music. The Mean Average Precision (MAP) for the best match is $.08$ for the DTW approach only, and $.05$ for the combined approach. For the top-6 results, the MAP is $.26$ and $.38$ respectively.


