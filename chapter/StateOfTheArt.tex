\chapter{State of the art}	\label{chap:sota}
\section{From speech to singing}
Singing presents a number of challenges for language identification when compared to pure speech \cite{goto_alignment}. To mention a few examples:
\begin{description}
 \item[Larger pitch fluctuations] A singing voice varies its pitch to a much higher degree than a speaking voice. It often also has very different spectral properties.
 \item[Higher pronunciation variation] Singers are often forced by the music to pronounce certain sounds and words differently than if they were speaking them.
 \item[Larger time variations] In singing, sounds are often prolonged for a certain amount of time to fit them to the music. Conversely, they can also be shortened or left out completely.
 \item[Different vocabulary] In musical lyrics, words and phrases often differ from normal conversation texts. Certain words and phrases have different probabilities (e.g. higher focus on emotional topics in singing).
 \item[Background music] adds irrelevant data (for language identification) to the signal, which acts as an interfering factor to the algorithms. 
 %It therefore should be removed or suppressed prior to the language identification, e.g. by source separation algorithms.
 \end{description}
 %Mehr Beispiele aus Goto-Kapitel
Most of the experiments in this work were performed on unaccompanied singing in order to remove this last difficulty for the moment.

\section{Phoneme recognition}
\subsection{Phoneme recognition in speech}
\subsection{Phoneme recognition in singing}
As described in \cite{loscos}, \cite{goto_alignment}, and \cite{kruspe_kws1}, there are significant differences between speech and singing audio, such as pitch and harmonics, vibrato, phoneme durations and pronunciation. These factors make phoneme recognition on singing more difficult than on speech. It has only been a topic of research for the past few years. \\
Fujihara et al. first presented an approach using Probabilistic Spectral Templates to model phonemes in \cite{fujihara_phonemes}. The phoneme models are gender-specific and only model five vowels, but also work for singing with instrumental accompaniment. The best result is $65\%$ correctly classified frames. \\
In \cite{gruhne}, Gruhne et al. describe a classical approach that employs feature extraction and various machine learning algorithms to classify singing into 15 phoneme classes. It also includes a step that removes non-harmonic components from the signal. The best result of $58\%$ correctly classified frames is achieved with Support Vector Machine (SVM) classifiers. The approach is expanded upon in \cite{szepannek}. \\
Mesaros presented a complex approach that is based on Hidden Markov Models which are trained on Mel-Frequency Cepstral Coefficients (MFCCs) and then adapted to singing using three phoneme classes separately \cite{mesaros1}\cite{mesaros2}. The approach also employs language modeling and has options for vocal separation and gender and voice adaptation. The achieved phoneme error rate on unaccompanied singing is $1.06$ without adaptation and $0.8$ with singing adaptation using 40 phonemes (the error rate greater than one means that there were more insertion, deletion, or substitution errors than phoneme instances). The results also improve when using gender-specific adaptation (to an average of $0.81\%$) and even more when language modeling is included (to $0.67\%$). \\
Hansen presents a system in \cite{jens} which combines the results of two Multilayer Perceptrons (MLPs), one using MFCC features and one using TRAP (Temporal Pattern) features. Training is done with a small amount of singing data. Viterbi decoding is then performed on the resulting posterior probabilities. On a set of 27 phonemes, this approach achieves a recall of up to $48\%$.

\section{Forced alignment}
\subsection{Forced alignment in speech}
\subsection{Forced alignment in singing}


\section{Language identification}
\subsection{Language identification in speech}
Language identification has been extensively researched in the field of Automatic Speech Recognition since the 1980's. A number of successful algorithms have been developed over the years. An overview over the fundamental techniques is given by Zissman in \cite{zissman}.\\
Fundamentally, four properties of languages can be used to discriminate between them:
\begin{description}
 \item[Phonetics] The unique sounds that are used in a given language.
 \item[Phonotactics] The probabilities of certain phonemes and phoneme sequences.
 \item[Prosody] The ``melody'' of the spoken language.
 \item[Vocabulary] The possible words made up by the phonemes and the probabilities of certain combinations of words.
\end{description}
Even modern system mostly focus on phonetics and phonotactics as the distinguishing factors between languages. Vocabulary is sometimes exploited in the shape of language models.\\
Zissman mentions Parallel Phone Recognition followed by Language Modeling (PPRLM) as one of the basic techniques. It requires audio data, language annotations, and phoneme annotations for each utterance. In order to make use of vocabulary characteristics, full sentence annotations and word-to-phoneme dictionaries are also necessary.\\
Using the audio and phoneme data, acoustic models are trained. They describe the probabilities of certain sound and sound sequences occurring. This is done separately for each considered language. Similarly, language models are generated using the sentence annotations and the dictionary. These models describe the probabilities of certain words and phrases. Again, this is done for each language.\\
New audio examples are then run through all pairs of acoustic and language models, and the likelihoods produced by each model are retained. The highest acoustic likelihood, the highest language likelihood, or the highest combined likelihood are then considered to determine the language. This approach achieves up to $79\%$ accuracy for ten languages \cite{muthusamy}.\\
Another approach uses the idea to train Gaussian Mixture Models for each language. This technique can be considered a ``bag of frames'' approach, i.e. the single data frames are considered to be statistically independent of each other. The generated GMMs then describe probability densities for certain characteristics of each language. Using these, the language of new audio examples can be easily determined.\\
GMM approaches used to perform worse than their PPRLM counterparts, but the development of new features has made the difference negligible \cite{singer}. They are in general easier to implement since only audio examples and their language annotations are required. Allen et al. \cite{allen} report results of up to $76.4\%$ accuracy for ten languages. Different backend classifiers, such as Multi-Layer Perceptrons (MLPs) and Support Vector Machines (SVMs) \cite{campbell} have also been used succesfully instead of GMMs.


\subsection{Language identification in singing}
So far, only a few approaches to perform language identification on singing have been proposed.\\
Schwenninger et al. \cite{schwenninger} use MFCC features, but do not mention how they perform their actual model training. They test different pre-processing techniques, such as vocal/non-vocal segmentation, distortion reduction, and azimuth discrimination. None of these techniques seem to improve the over-all results. They achieve an accuracy of 68\% on a-capella music for two languages (English and German).\\
The approach of Tsai and Wang \cite{tsai_wang} follows a traditional PPRLM flow. After vocal/non-vocal segmentation using GMMs, they run their data through acoustic models using vector tokenization. One acoustic model for each language is used. The results are then processed by bigram language models, again for each language. The language model score is used for a maximum likelihood decision to determine the language. They achieve results of 70\% accuracy for two languages (English and Mandarin) on pop music.\\
Mehrabani and Hansen \cite{mehrabani} also use a PPRLM system, with the difference that all combinations of acoustic and language models are tested. Their scores are combined by a classifier to determine the final language. This results in a score of 78\% for a-capella music in three languages (English, Hindi, and Mandarin). Combining this technique with prosodic data improved the result even further.\\
Finally, Chandrasekhar et al.\cite {chandrasekhar} try to determine the language for music videos using both audio and video features. They achieve accuracies of close to 50\% for 25 languages. It is interesting to note that European languages seem to achieve much lower accuracies than Asian and Arabic ones. English, French, German, Spanish and Italian rank below 40\%, while languages like Nepali, Arabic, and Pashto achieve accuracies above 60\%.


\section{Keyword spotting}
%\subsection{Keyword spotting in speech}
%\subsection{Keyword spotting in singing}
%SPEECH HERE
To the best of the author's knowledge, no keyword spotting systems for singing existed prior to this work.

