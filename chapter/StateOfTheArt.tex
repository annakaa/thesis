\chapter{State of the art}	\label{chap:sota}
\section{From speech to singing} \label{sec:speech_to_singing}
Singing presents a number of challenges for speech recognition when compared to pure speech \cite{loscos} \cite{goto_alignment} \cite{kruspe_kws1}. The following factors make speech recognition on singing more difficult than on speech, and make it necessary to adapt existing algorithms.
\begin{description}
 \item[Larger pitch fluctuations] A singing voice varies its pitch to a much higher degree than a speaking voice. It often also has very different spectral properties.
 \item[Larger changes in loudness] In addition to pitch, loudness also fluctuates much more in singing than in speech.
 \item[Higher pronunciation variation] Singers are often forced by the music to pronounce certain sounds and words differently than if they were speaking them.
 \item[Larger time variations] In singing, sounds are often prolonged for a certain amount of time to fit them to the music. Conversely, they can also be shortened or left out completely.
 \item[Different vocabulary] In musical lyrics, words and phrases often differ from normal conversation texts. Certain words and phrases have different probabilities (e.g. higher focus on emotional topics in singing).
 \item[Background music] This is the biggest interfering factor when considering polyphonic recordings. Harmonic and percussive instruments add a huge amount of spectral components to the signal, which lead to confusion in speech recognition algorithms. Ideally, these components should be removed or suppressed in a precursory step. This could be achieved, for example, by employing source separation algorithms. However, such algorithms add additional artifacts to the signal, and may not even be sufficient for this purpose at the current state of research.\\
 Voice activity detection (VAD) could be used as a non-invasive first step to at the very least discard segments of songs that do not contain voice. However, such algorithms often make mistakes in the same cases that are problematic for speech recognition algorithms (e.g. instrumental solos)\cite{}.\\
 For these reasons, most of the experiments in this work were performed on unaccompanied singing. The integration of the mentioned pre-processing algorithms would be a very interesting next step of research.\\
 The exception are the lyrics-to-singing alignment algorithms presented in Chapter \ref{chap:retrieval_alignment}. Those were also tested on polyphonic music, and the algorithms appear to be largely robust to these influences.
 \end{description}
 %Mehr Beispiele aus Goto-Kapitel
 %Sundberg!
 % Vowels vs. Consonants (1. Mesaros-Paper + recognition of phonemes and words...)

%describe tasks here??

\section{Phoneme recognition} \label{sec:sota_phone}
%\subsection{Phoneme recognition in speech}
%\subsection{Phoneme recognition in singing}
Due to the factors mentioned above in section \ref{sec:speech_to_singing}, phoneme recognition on singing is more difficult than on clean speech. It has only been a topic of research for a few years, and there are few publications.\\
In 2007, Gruhne et al. presented a classical approach that employs feature extraction and various machine learning algorithms to classify singing into 15 phoneme classes \cite{Gruhne2007} \cite{Gruhne2007a}. The specialty of this approach lies in the pre-processing: At first, fundamental frequency estimation is performed on the audio input, using a Multi-Resolution Fast Fourier Transform \cite{inproceedings:dressler}. Based on the estimated fundamental frequency, the harmonic partials are retrieved from the spectrogram. Then, a sinusoidal re-synthesis is performed, using only the detected fundamental frequency and partials. Feature extraction is then performed on this re-synthesis instead of the original audio. Extracted features include MFCCs, PLPs \cite{plp2}, LPCs, and WLPCs \cite{lpc}. MLP, GMM, and SVM models are trained on the resulting feature vectors. The re-synthesis idea comes from a singer identification approach by Fujihara \cite{fujihara_identification}.\\
The approach is tested on more than 2000 separate, manually annotated phoneme instances from polyphonic recordings. Only one feature vector per phoneme instance is calculated. The phoneme set is reduced down to 15 classes. Using SVM models, $56\%$ of the tested instances were classified correctly. This is significantly better than the best result without the re-synthesis step ($34\%$).\\
In \cite{szepannek} (2010), the approach is expanded by testing a larger set of perceptually motivated features, and more classifiers. No significant improvements are found when using more intricate features, and the best-performing classifier remains SVM.\\
\medskip
Fujihara et al. described an approach based on spectral analysis in 2009 \cite{fujihara_phonemes}. The underlying idea is that spectra of polyphonic music can be viewed as the weighted sum of two types of spectra: One for the singing voice, and one for the background music. This approach then models these two spectra as probabilistic spectral templates. The singing voice is modeled by multiplying a vocal envelope template, which represents the spectral structure of the singing voice, with a harmonic filter, which represents the harmonic structure of the produced sound itself. This is analogous to the source-filter model of speech production \cite{}. For recognizing vowels, five such harmonic filters are prepared (a - e - i - o - u). Vocal envelope templates are trained on voice-only recordings, separated by gender. Templates for background music are trained on instrumental tracks. In order to recognize vowels, the probabilities for each of the five harmonic templates are estimated. As a side product, the algorithm also estimates the fundamental frequency of the singing voice.\\
As described, the phoneme models are gender-specific and only model five vowels, but also work for singing with instrumental accompaniment. The approach is tested on 10 Japanese-language songs. The best result is $65\%$ correctly classified frames, compared to the $56\%$ with the previous approach by this team, based on GMMs. \\
\medskip
Also in 2009, Mesaros et al. presented another classical approach to phoneme recognition in singing which is based on MFCC features and GMM-HMMs for acoustic modeling \cite{Mesaros2009}. The models are trained on the CMU ARCTIC speech corpus \footnote{\url{http://festvox.org/cmu arctic/}}. Then, different Maximum Likelihood Linear Regression (MLLR) techniques for adapting the models to singing voices are tested \cite{mllr}.\\
The adaptation and test corpus consists of 49 voice-only fragments from 12 pop songs with durations between 20 and 30 seconds. The best results are achieved when both the means and variances of the Gaussians are transformed with MLLR. The results improved slightly when not just a single transform was used for all phonemes, but when they were grouped into base classes beforehand, each receiving individual transformation parameters. The best result is around $.79$ Phoneme Error Rate on the test set.\\
In \cite{Mesaros2010} and \cite{Mesaros2011}, language modeling is added to the presented approach. Phoneme-level language models are trained on the CMU ARCTIC corpus as unigrams, bigrams, and trigrams, while word-level bigram and trigram models are trained on actual song lyrics in order to match the application case. The output from the acoustic models is then refined using these language models. The approach is tested on the clean singing corpus mentioned above, and on 100 manually selected fragments of 17 polyphonic pop songs. To facilitate recognition on polyphonic music, a vocal separation algorithm is introduced \cite{virtanen_separation}.\\
Using phoneme-level language modeling, the phoneme error rate on clean singing is reduced to $.7$. On polyphonic music, it is $.81$. For the word recognition approach, the word error rate is $.88$ on clean singing, and $.94$ on the polyphonic tracks.\\
A more detailed voice adaptation strategy is tested in \cite{Mesaros2010a}. Instead of adapting the acoustic models with mixed singing data, they are adapted gender-wise, or to specific singers. With the gender-specific adaptations, the average phoneme error rate on clean singing is lowered to $.81$ without language modeling, and $.67$ with language modeling. Singer-specific adaptation does not improve the results, probably because of the very small amount of adaptation data in this case.\\
%mvcivar, hosoya
In \cite{mcvicar_repetition} (2014), McVicar et al. build on a very similar baseline system, but also exploit repetitions of choruses to improve transcription accuracy. This has been done for other MIR tasks, such as chord recognition, beat tracking, and source separation. They propose three different strategies for combining individual results: Feature averaging, selection of the chorus instance with the highest likelihood, and combination using the Recogniser Output Voting Error Reduction (ROVER) algorithm \cite{rover}. They also employ three different language models, two of which were matched to the test songs (and therefore not representative for general application). 20 unaccompanied, English-language songs from the RWC database \cite{rwc} were used for testing; chorus sections were selected manually. The best-instance selection and the ROVER strategies improve results significantly; with the ROVER approach and a general-purpose language model, the Phoneme Error Rate is at $.74$ (versus $.76$ in the baseline experiment), while the Word Error Rate is improved from $.97$ to $.9$. Interestingly, cases with a low baseline result benefit the most from exploiting repetition information.\\
\medskip
The final system was proposed by Hansen in 2012 \cite{jens}. It also employs a classical approach consisting of a feature extraction step and a model training step. Extracted features are MFCCs and TRAP (TempoRAl Pattern) features. Then, Multilayer Perceptrons (MLPs) are trained separately on both feature sets. The assumption is that each feature models different properties of the considered phonemes: Short-term MFCCs are good at modeling the pitch-independent properties of stationary sounds, such as sonorants and fricatives. On the flip-side, TRAP features are able to model temporal developments in the spectrum, forming better representations for sounds like plosives or affricates.\\
The results of both MLP classifiers are combined via a fusion classifier, also an MLP. Then, Viterbi decoding is performed on its output.\\
The approach is trained and tested on a data set of 12 vocal tracks of pop songs, which were manually annotated with a set of 27 phonemes. The combined system achieves a recall of $.48$, compared to $.45$ and $.42$ for the individual MFCC and TRAP classifiers respectively. This confirms the assumption that the two features complement each other. The phoneme-wise results further corroborate this.

\section{Forced alignment and retrieval}
%\subsection{Forced alignment in speech}
%\subsection{Forced alignment in singing}
%mesaros
%Three techniques for im- proving automatic synchronization between music and lyrics: Fricative detection, filler model, and novel feature vectors for vocal activity detection
%RETRIEVAL: Mesaros!!!

\section{Language identification}
\subsection{Language identification in speech}
Language identification has been extensively researched in the field of Automatic Speech Recognition since the 1980's. A number of successful algorithms have been developed over the years. An overview over the fundamental techniques is given by Zissman in \cite{zissman}.\\
Fundamentally, four properties of languages can be used to discriminate between them:
\begin{description}
 \item[Phonetics] The unique sounds that are used in a given language.
 \item[Phonotactics] The probabilities of certain phonemes and phoneme sequences.
 \item[Prosody] The ``melody'' of the spoken language.
 \item[Vocabulary] The possible words made up by the phonemes and the probabilities of certain combinations of words.
\end{description}
Even modern system mostly focus on phonetics and phonotactics as the distinguishing factors between languages. Vocabulary is sometimes exploited in the shape of language models.\\
Zissman mentions Parallel Phone Recognition followed by Language Modeling (PPRLM) as one of the basic techniques. It requires audio data, language annotations, and phoneme annotations for each utterance. In order to make use of vocabulary characteristics, full sentence annotations and word-to-phoneme dictionaries are also necessary.\\
Using the audio and phoneme data, acoustic models are trained. They describe the probabilities of certain sound and sound sequences occurring. This is done separately for each considered language. Similarly, language models are generated using the sentence annotations and the dictionary. These models describe the probabilities of certain words and phrases. Again, this is done for each language.\\
New audio examples are then run through all pairs of acoustic and language models, and the likelihoods produced by each model are retained. The highest acoustic likelihood, the highest language likelihood, or the highest combined likelihood are then considered to determine the language. This approach achieves up to $79\%$ accuracy for ten languages \cite{muthusamy}.\\
Another approach uses the idea to train Gaussian Mixture Models for each language. This technique can be considered a ``bag of frames'' approach, i.e. the single data frames are considered to be statistically independent of each other. The generated GMMs then describe probability densities for certain characteristics of each language. Using these, the language of new audio examples can be easily determined.\\
GMM approaches used to perform worse than their PPRLM counterparts, but the development of new features has made the difference negligible \cite{singer}. They are in general easier to implement since only audio examples and their language annotations are required. Allen et al. \cite{allen} report results of up to $76.4\%$ accuracy for ten languages. Different backend classifiers, such as Multi-Layer Perceptrons (MLPs) and Support Vector Machines (SVMs) \cite{campbell} have also been used successfully instead of GMMs.


\subsection{Language identification in singing}
A first approach for language identification in singing was proposed by Tsai and Wang in 2004 \cite{tsai_wang}. At its core, the algorithm is similar to Parallel Phoneme Recognition (PPR). However, instead of full phoneme modeling, they employ an unsupervised clustering algorithm to the input feature data and tokenize the results to form language-specific codebooks (plus one for background music). Following this, the results from each codebook are run through a matching language models to determine the likelihood that the segment was performed in this language. Prior to the whole process, vocal/non-vocal segmentation is performed. This is done by training GMMs on segments of each language, and on non-vocal segments. MFCCs are used as features.\\
The approach is tested on 112 English- and Mandarin-language polyphonic songs each, with 32 songs performed in both languages. A classification accuracy of $.8$ is achieved on the non-overlapping songs. On the overlapping songs, it is only at $.7$, suggesting some influence of the musical material (as opposed to the actual language characteristics). Misclassifications occur more frequently on the English-language songs, possibly because of accents of Chinese singers performing in English, and because of louder background music.\\
\medskip
A second, simpler approach was presented by Schwenninger et al. in 2006 \cite{schwenninger}. They also extract MFCC features, and then use these to directly train statistical models for each language. Three different pre-processing strategies are also tested: Automatic vocal/non-vocal segmentation, distortion reduction, and azimuth discrimination. Vocal/non-vocal segmentation is performed by thresholding the energy in high-frequency bands as an indicator for voice presence over 1 second windows. This leaves a relatively small amount of material per song. Distortion reduction is employed to discard strong drum and bass frames where the vocal spectrum is masked by using a mel-based approach. Finally, azimuth discrimination attempts to detect and isolate singing voice panned to the center of the stereo scene.\\
The approach is tested on three small data sets of speech, unaccompanied singing, and polyphonic music. Without pre-processing steps, the accuracy is $.84$, $.68$, and $.64$ respectively, highlighting the increased difficulty of language identification on singing versus speech, and on polyphonic music versus pure vocals. On the polyphonic corpus, the pre-processing steps do not improve the result.\\
\medskip
In 2011, Mehrabani and Hansen presented a full Parallel Phoneme Recognition followed by Language Modeling (PPRLM) approach for singing language identification. MFCC features are run through phoneme recognizers for Hindi, German, and Mandarin; then, the results are scored by individual language models for each considered language. In addition, a second system is employed which uses prosodic instead of phonetic tokenization. This is done by modeling pitch contours with Legendre polynomials, and then quantizing these vectors with previously trained GMMs. The results are then again used as inputs to language models.\\
The approach is trained and tested on a corpus containing 12 hours of unaccompanied singing and speech in Mandarin, Hindi, and Farsi. The average accuracy for singing is $.78$ and $.43$ for the phoneme- and prosody-based systems respectively, and $.83$ for a combination of both.\\
\medskip
Also in 2011, Chandrasekhar et al. presented a very interesting approach for language identification on music videos, taking into account both audio and video features \cite {chandrasekhar}. On the audio side, the spectrogram, volume, MFCCs, and perceptually motivated Stabilized Auditory Images (SAI) are used as inputs. One-vs-all SVMs are trained for each language. The approach is trained and tested on 25,000 music videos, taking 25 languages into consideration. Using audio features only, the accuracy is $.45$; combined with video features, it rises to $.48$. It is interesting to note that European languages seem to achieve much lower accuracies than Asian and Arabic ones. English, French, German, Spanish and Italian rank below $.4$, while languages like Nepali, Arabic, and Pashto achieve accuracies above $.6$. It is possible that the language characteristics of European languages make them harder to discriminate (especially against each other) than others.

\section{Keyword spotting}
%\subsection{Keyword spotting in speech}
%\subsection{Keyword spotting in singing}
%SPEECH HERE
%pedro!!
Keyword spotting in singing was first attempted in 2008 by Fujihara et al. \cite{hyperlinking_lyrics}. Their approach employs a phoneme recognition step first, which is again based on the vocal re-synthesis method first described in \cite{fujihara_identification}. MFCCs and power features are extracted from the re-synthesized singing and used as inputs to a phoneme model, similar to Gruhne's phoneme recognition approach mentioned above in \ref{sec:sota_phone}.  Three phoneme models are compared: One trained on pure speech and adapted with a small set of singing recordings, one adapted with all recordings, and one trained directly on singing. Viterbi decoding is then performed using keyword-filler HMMs (see \ref{sec:tech_kwfhmm}) to detect candidate segments where keywords may occur. These segments are then re-scored through the filler HMM to verify the occurrence.\\
The method is tested on 79 unaccompanied Japanese-language songs from the RWC database \cite{rwc}. The Phoneme Error Rate is $.73$ for the acoustic models trained on speech, $.67$ for the adapted models, and $.49$ for the models trained on singing (it should be mentioned that the same songs were used for training and testing, although a cross-validation experiment appears to show that the effect is negligible). The (link success rate???) for detecting the keywords is $.3$. The authors show that the ??? depends highly on the number of phonemes in the considered keyword, with longer keywords being easier to detect.



