%\addtocontents{toc}{\protect\clearpage}
\chapter{Data sets} \label{chap:datasets}
This chapter contains descriptions of all the data sets (or corpora) used over the course of this thesis. They are grouped into speech-only data sets, data sets of unaccompanied (=a-capella) singing, and data sets of full musical pieces with singing (``real-world'' data sets).

\section{Speech data sets}
\subsection{TIMIT}
\textit{TIMIT} is, presumably, the most widely used corpus in speech recognition research \cite{timit}. It was developed in 1993 and consists of 6300 English-language audio recordings of 630 native speakers with annotations on the phoneme, word, and sentence levels. The corpus is split into a training and a test section, with the training section containing 4620 utterances, and the test section containing 1680. Each of those utterances has a duration of a few seconds. The recordings are sampled at $16,000 Hz$ and are mono channel.\\
The phoneme annotations follow a model similar to ARPABET and contain 61 different phonemes \cite{Zue1990}.
%wo benutzt?

\subsection{NIST Language identification corpora}
The National Institute for Standards and Technology (NIST) regularly runs various speech recognition challenges, one of them being the Language Recognition Evaluation (LRE) task, which is offered every two to four years \footnote{\url{https://www.nist.gov/itl/iad/mig/language-recognition}}. To this end, they publish training and evaluation corpora of speech in several languages. They consist of short segments (up to 35 seconds) of free telephone speech by many different speakers. The recordings are mono channel with a sampling rate of $8000 Hz$.\\
The corpus for the 2003 challenge was used in this work for comparison of language identification algorithms against speech data \cite{nist2003lre}. Only the English-, German-, and Spanish-language subsets were chosen, since those languages are covered by the corresponding singing dataset. To balance out the languages, 240 recordings were used for each of them, making up around 1 hour of material.

\subsection{OGI Multi-Language Telephone Speech Corpus}
%aufräumen!!
In 1992, the Oregon Institute for for Science and Technology (OGI) also published a multilingual corpus of telephone recordings, called the OGI Multi-Language Telephone Speech Corpus, to facilitate multi-language ASR research. Just like the NIST corpora, it has become widely used for speech recognition tasks. Again, the English-, German-, and Spanish-language subsets were used in this work. They each consist of more than 1000 recordings per language of up to $50$ seconds duration, making up a total of about three to five hours. In contrast to the NIST corpus, the recordings are somewhat less ``clean'' with regards to accents and background noise; the sampling rate is also $8000 Hz$ on mono channel.  For experiments on longer recordings, results on these individual utterances were aggregated for each speaker, producing 118 documents per language (354 in sum).\\




%Tabelle nennen!
\begin{table}[htp]
  \caption{{Amounts of data in the three used data sets: Sum duration on top, number of utterances in italics.}}
  \begin{center}
    \begin{tabular}{|c||c|c|c|}\hline
    \textbf{hh:mm:ss} & \multirow{2}{*}{\textbf{NIST2003LRE}} & \multirow{2}{*}{\textbf{OGIMultilang}} & \multirow{2}{*}{\textbf{YTAcap}}\\
    \textbf{\textit{\#Utterances}} & & & \\ \hline \hline 
    \multirow{2}{*}{English} & 00:59:08 & 05:13:17 &  08:04:25 \\
    & \textit{240} & \textit{1912} & \textit{1975} \\ \hline 
    \multirow{2}{*}{German} & 00:59:35 & 02:52:27 & 04:18:57 \\
    & \textit{240} & \textit{1059} & \textit{1052} \\ \hline 
    \multirow{2}{*}{Spanish} & 00:59:44 & 03:05:45 & 07:21:55 \\
    & \textit{240} & \textit{1151} & \textit{1810} \\ \hline 
    \end{tabular}
  \end{center}
  \label{tab:datasets}
\end{table}

\section{Singing data sets}
\subsection{YouTube data set}
As opposed to the speech case, there are no standardized corpora for sung language identification. For the sung language identification experiments, files of unaccompanied singing were therefore extracted from \textit{YouTube}\footnote{\url{http://www.youtube.com}, Last checked: 05/16/13} videos. This was done for three languages: English, German, and Spanish. The corpus consists of between 116 (258min) and 196 (480min) examples per language. These were mostly videos of amateur singers freely performing songs without accompaniment. Therefore, they are of highly varying quality and often contain background noise. The audio was downloaded in the natively provided quality, but then downsampled to $8000 Hz$ and averaged to a mono channel for uniformity and for compatibility with the speech corpora for language identification. Most of the performers contributed only a single song, with just a few providing up to three. This was done to avoid effects where the classifier recognizes the singer's voice instead of the language.\\
Special attention was paid to musical style. Rap, opera singing, and other specific singing styles were excluded. All the songs performed in these videos were pop songs. Different musical styles can have a high impact on language classification results. The author tried to limit this influence as much as possible by choosing recordings of pop music instead of language-specific genres (such as Latin American music).\\
For some experiments, they were split up into segments of 10-20 seconds at silent points (3,156 ``utterances'' in sum).\\
An overview of the amounts of data in the three corpora for language identification is given in Table \ref{tab:datasets}.

\subsection{Hansen's vocal track data set}
This is one of the data sets used for keyword spotting and phoneme recognition. It was first presented in \cite{jens}. It consists of the vocal tracks of 19 commercial English-language pop songs. They are studio quality with some post-processing applied (EQ, compression, reverb). Some of them contain choir singing. These 19 songs are split up into 920 clips that roughly represent lines in the song lyrics. The original audio quality is $44,100 Hz$ on mono channels; for compatibility with models trained on \textit{TIMIT}, they were downsampled to $16,000 Hz$. \\
Twelve of the songs were annotated with time-aligned phonemes. The phoneme set is the one used in CMU Sphinx\footnote{\url{http://cmusphinx.sourceforge.net/}} and TIMIT \cite{timit} and contains 39 phonemes. All of the songs were annotated with word-level transcriptions. This is the only one of the singing data sets that has full manual annotations, which are assumed to be reliable and can be used as ground truth.\\
For comparison, recordings of spoken recitations of all song lyrics were also made. These were all performed by the same speaker (the author).\\
This data set will be referred to as \textit{ACAP}.

\subsection{DAMP data set}
As described, Hansen's data set is very small and therefore not suited to training phoneme models for singing. As a much larger source of unaccompanied singing, the \textit{DAMP} data set, which is freely available from Stanford University\footnote{\url{https://ccrma.stanford.edu/damp/}}\cite{phdthesis:jeffreysmith}, was employed. This data set contains more than 34,000 recordings of amateur singing of full songs with no background music, which were obtained from the \textit{Smule Sing!} karaoke app. Each performance is labeled with metadata such as the gender of the singer, the region of origin, the song title, etc. The singers performed 301 English-language pop songs. The recordings have good sound quality with little background noise, but come from a lot of different recording conditions. They were originally provided in OGG format at a sampling rate of $22,050 Hz$ (mono channel), but were also converted to wave format and downsampled to $16,000 Hz$ for compatibility.\\
No lyrics annotations are available for this data set, but the textual lyrics can be obtained from the \textit{Smule Sing!} website\footnote{\url{http://www.smule.com/songs}}. These are, however, not aligned in any way. Such an alignment was performed automatically on the word and phoneme levels (see section \ref{sec:phonerec_acap}).\\
The selection of songs is not balanced, with performances ranging between 21 and 2038 instances. Female performances are also much more frequent than male ones, and gender often plays a role when training and evaluating models. For these reasons, several subsets of the dataset were composed by hand:
\begin{description}
\item[DampB] {Contains 20 full recordings per song (6000 in sum), both male and female.}
\item[DampBB] {Same as before, but phoneme instances were discarded until they were balanced and a maximum of 250,000 frames per phoneme where left, where possible. This data set is about 4\% the size of \textit{DampB}.}
\item[DampBB\_small] {Same as before, but phoneme instances were discarded until they were balanced and 60,000 frames per phoneme were left (a bit fewer than the amount contained in \textit{TIMIT}). This data set is about half the size of \textit{DampBB}.}
\item[DampFB and DampMB] {Using 20 full recordings per song and gender (6000 each), these data sets were then reduced in the same way as \textit{DampBB}. \textit{DampFB} is roughly the same size, \textit{DampMB} is a bit smaller because there are fewer male recordings.}
\item[DampTestF and DampTestM] {Contains one full recording per song and gender (300 each). These data sets were used for testing. There is no overlap with any of the training data sets.}
\end{description}
%Matt's data set??

\subsection{Structure of the final corpus}
%TODO: INTEGRATE!!!
\begin{description}

\item[Training data sets] In order to generate training data sets, we decided to balance the data by songs so that certain songs (and their phoneme distributions) would not be overrepresented. The least represented song in the original data set has 20 recordings. We therefore chose 20 to 23 recordings per song at random to generate a training data set, which we named \textbf{DampB}. For each song, as many different singers as possible were selected. Additionally, recordings with more ``Loves'' (user approval) were more likely to be selected (however, a large percentage of the original data set did not have such ratings). This resulted in a data set of 6,902 recordings.\\
 We then repeated this process, but only took recordings by singers of one gender into account. In this way, we created data sets of recordings by female and male singers only, which we named \textbf{DampF} and \textbf{DampM} respectively. Due to the gender split, there were fewer recordings of some of the songs available. Male singers in particular are underrepresented in the original data set. Therefore, \textbf{DampF} contains 6,564 recordings, while \textbf{DampM} contains 4,534. These sizes are roughly in the same range as for the mixed-gender data set, which enables the comparison of models trained on all three.
 \item[Test data sets] For testing new algorithms, we also created two small test data sets from the original \textit{DAMP} data set. These are, again, split by gender, and contain one recording per song, resulting in 301 recordings for the female data set and 300 for the male one (since there was one song with not enough available male recordings). We call them \textbf{DampTestF} and \textbf{DampTestM} respectively.\\
 For mixed-gender training, we suggest simply performing the testing on both test data sets.
\end{description}
A structured overview is given in table \ref{tab:corpus}.

\begin{table}
 \begin{center}
 \begin{tabular}{|c||c|c|}
  \hline
   \textbf{Gender} & \textbf{Training} & \textbf{Test} \\
  \hline
  \hline
  Both & DampB & \\
  & (6,902) & \\
  \hline
  Female & DampF & DampTestF \\
  & (6,654) & (301) \\
  \hline
  Male & DampM & DampTestM \\
  & (4,534) & (300) \\
  \hline
 \end{tabular}
\end{center}
\vspace{-10pt}
 \caption{Overview of the structure of the \textit{DAMP}-based phonetically annotated corpora, number of recordings in brackets.}
 \label{tab:corpus}
\vspace{10pt}
\end{table}



%\subsection{Aji's synthesized singing data set} \label{subsec:data_synth}
%Since it was not feasible to hand-annotate a large data set over the course of this work, another approach was the automatic generation of sung audio. The advantage of this approach is that the results can be assumed to be perfectly aligned to the given phonemes.\\
%For the generation of this data set, recordings from the previously described \textit{DAMP} data set were used. Their phonemes were automatically aligned, and an automatic transcription of the melody was performed. These two sources of data were then aligned to each other. This alignment did not need to be perfect, it just needed to produce a plausible combination of melody line and phonemes. For some songs in the dataset, these steps did not produce acceptable results and they had to be discarded, resulting in a corpus of 5163 recordings with phoneme and melody transcriptions.\\
%The result of this step was then fed into the \textit{Sinsy}\cite{sinsy}\footnote{\url{http://sinsy.jp/)}} singing synthesizer to generate new singing recordings. This synthesizer uses HMMs trained on MFCCs, $F_0$, and vibrato parameters for syllable-level synthesis, and provides one female singing voice. The resulting recordings are good in quality and relatively natural sounding, but appear to have a slight accent.\\
%The whole generation process of this data set was performed in collaboration with Adam Aji. The data set will be referred to as \textit{SYNTH}.

%\subsection{Choosing keywords}


%\section{``Real-world'' data sets}
\subsection{QMUL Expletive data set}
This data set consists of 80 popular full songs which were collected at Queen Mary University, most of them Hip Hop. 711 instances of 48 expletives were annotated on these songs. In addition, the matching textual, unaligned lyrics were retrieved from the internet. The audio is provided at $44,100 kHz$ sampling rate in stereo, but was also resampled to $16,000 Hz$ and a mono track for compatibility with some models.

%\subsection{``69 Love Songs" data set}
%``69 Love Songs'' is a 3-CD album by the band ``The Magnetic Fields'', which was released in 1999 and named one of the \textit{Rolling Stone}'s 500 Greatest Albums of All Time in 2012 \cite{rollingstone}. It contains 69 songs in various musical styles and instrumentations, performed by a variety of musicians, including five vocalists. The total duration is 2 hours and 52 minutes. The data set is interesting for the purposes of this work because the songs' lyrics all cover a similar theme - namely, love. A word count on the lyrics shows, for example, that the word ``love'' itself occurs \~225 times in these songs.\\
%Unaligned lyrics were retrieved from \url{http://stephinsongs.wiw.org}. A thorough semantic analysis can be found in \cite{book:beghtol}.
